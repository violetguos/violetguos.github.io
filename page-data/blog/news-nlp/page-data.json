{"componentChunkName":"component---src-templates-blog-post-js","path":"/blog/news-nlp/","result":{"data":{"avatar":null,"site":{"siteMetadata":{"title":"Violet Guo","siteUrl":"https://violetguos.github.io","author":{"name":"Violet Guo","info":"machine learning, swe"}}},"markdownRemark":{"id":"29a8367c-a15c-5943-92e3-d033d8e7f81a","excerpt":"Author’s Note I wanted to reflect on this past project. My team decided to join an existing machine learning competition where the source data and a base line…","html":"<h1>Author’s Note</h1>\n<p>I wanted to reflect on this past project. My team decided to join an existing machine learning competition where the source data and a base line model was provided to us. The following was our original report. I have omitted the citations and some diagrams, but they are available upon request.</p>\n<h1>Introduction</h1>\n<p>For better or for worse, social media has had a significant impact on content and information distribution.\nSince anyone has the ability to post anything at anytime, there comes the cost of an increase in the spread of fake news articles and claims.\nDetecting whether an article is promoting fake news or not is difficult since the claim’s goal is often to mislead readers to some false conclusion.\nTherefore, being able to fact check claims by analyzing relevant articles can be beneficial as it would allow systems to promote true and relevant news.\nIdeally, this would reduce public exposure to fake news and deter its spread.\nIn this project, we examine this challenge and evaluate how natural language processing (NLP) models can be used for the task of fake news classification.\nOur goal is to train a variety of linear, ensemble, and neural network models and use pre-processing techniques that we’ve seen in class to achieve high classification accuracy.\nBy training and evaluating models on a news dataset provided by the 2019 online competition `Leaders Prize: Fact or Fake News’, we demonstrate several NLP models’ effectiveness at classifying false, partly true, and true news.\nWe show that our methods can successfully detect fake news but would require more work in true news identification.</p>\n<h1>Data Description</h1>\n<p>The source of our labelled dataset is from the 2019 online competition called `Leaders Prize: Fact or Fake News?’ \\cite{DataCup}.\nThe competition involves the task of classifying news claim within its provided dataset as either: False, Partly True, or True.\nThe dataset is comprised of labelled claims (i.e. statements), claimants, and related article content. Claimants are related to claims, however not all claims have a claimant.\nThe data contains a total of 15,555 claims, and 64,974 related articles.\nTherefore, there is a total of 15,555 examples to train and evaluate our models with.\nThe competition has a hidden test set that can only be evaluated on when making a submission.\nWe did not make use of this test set because the competition submission closed before we had the chance to make any.</p>\n<h2>Data Imbalance</h2>\n<p>Upon further analysis of our data, we noticed a severe class imbalance as illustrated in Figure \\ref{fig:label-hist}.\nThe majority of our data is of False and Partly True claims.\nMore precisely our examples are labelled: 48% False, 41% Partly True and 11% True.\nWe believe if this imbalance is not addressed, we expect to see a significantly low recall within our results.\nWe discuss a method to circumvent this imbalance in Section 4.7.</p>\n<h1>4 Methodology</h1>\n<p>Here we explain the different pre-processing techniques, feature extraction methods and models that we used and evaluated.\nFor this project, we are using the Python 3 programming language and several external Python packages: Pandas, SciPy, Matplotlib, Tqdm, Scikit-Learn, Tensorflow, Keras, PyTorch and XGBoost.</p>\n<h2>4.1 Data Split</h2>\n<p>Since the competition’s test set is hidden from us, we made use of splitting the training data that was provided.\nThe split is a ratio of 70%, 15% and 15% for the training, development and test sets respectively.</p>\n<h2>4.2 Data Pre-processing</h2>\n<p>Next we describe the techniques we’ve used for pre-processing our data. The methods used are the following: square bracket removal, non-ASCII character removal, punctuation removal, character lowercasing, numerical character replacement, stemming and lemmatization.</p>\n<p>These techniques are applied on claims and related article content. The majority of claimants are proper nouns, so we didn’t apply pre-processing on claimants. We applied non-ASCII character removal to clean our corpus from unordinary characters. Character lowercasing is applied because we don’t want our models to differentiate between words of the same spelling but different capitalization. Stemming and lemmatization are both performed to make our feature space dimensionally lower, with the intent of making our models faster to train and more generalizable.</p>\n<h2>4.3 Training and Evaluation</h2>\n<p>We tuned our models using the training and development sets to determine optimal hyperparameter configurations.\nThen we trained the optimally parameterized model on the training and validation sets, and finally evaluated it on the held-out test set.\nBy doing so, we can empirically determine optimal hyperparameter configurations by detecting overfitting and use the test set to have an unbias evaluation of our models.</p>\n<h2>4.4 Most Frequent Label Classifier Baseline</h2>\n<p>For comparison purposes, we have tested a Most Frequent Label Classifier baseline.\nGiven a set of test examples, the predictor classifies each example to that of the most frequently occurring label.\nThis was implemented using SciKit-Learn’s DummyClassifier application programming interface (API) .\nThis is not meant to be used as an effective predictor, yet a critical baseline for an unbalanced dataset.</p>\n<h2>4.5 Linear Model Baselines</h2>\n<p>We trained and evaluated three linear models as baselines to compare with more complex models that we eventually train and evaluate.</p>\n<p>We trained and evaluated three different multi-class linear models: Logistic Regression, Support Vector Machine (SVM) and Naive Bayes.\nLogistic Regression and SVM are discriminative models whereas Naive Bayes is a generative model.\nGenerative models estimate the joint distribution of features and labels whereas discriminative models estimate the posterior probability of some label given its features.\nThe difference between Logistic Regression and SVM is the different objective functions used at training.\nThey both estimate an hyperplane that can separate the training labels’ categories.\nThe subtle difference is that SVMs attempt to maximize the margin (i.e. distance) between its estimated hyperplane and the observations from each class \\cite{svm} whereas Logistic Regression performs Maximum Likelihood Estimation (MLE) to maximize the likelihood that a random point gets classified correctly.\nThe models are trained on Term Frequency-Inverse Document Frequency (TF-IDF) representations of the concatenation of the claim, claimant, and related article content.</p>\n<p>We fine tuned each linear models’ parameters with Grid Cross Validation of Fold 3.\nThe model configurations, validation F1 scores, and test F1 scores are available within Table \\ref{table:fine-tune-grid-cv}.\nWe trained, fine-tuned and evaluated these models using SciKit-Learn’s relevant predictor, Grid Cross Validation and metric APIs.</p>\n<h2>4.6 Ensemble Models: Random Forest and Extreme Gradient Boosting (XGBoost)</h2>\n<p>Ensemble models are popularly used in these sorts of classification challenges and are often considered the state-of-the-art solution.</p>\n<p>The main premise is that by combining multiple models, the errors of a single base-learner will likely be compensated by other base-learners, as a result the overall prediction performance would be better than that of a single model.</p>\n<p>A Random Forest is decision-tree-based ensemble model.\nThere are two main components to this ensemble.\nThe first is that each tree is trained on a random sample from the training data.\nThe second is that a random subset of features are selected to generate a split for each node in a tree.</p>\n<p>Similarly to a Random Forest, XGBoost is a decision-tree-based ensemble model, however it uses a gradient boosting framework.\nXGBoost trains an ensemble of base-learners and boosts weak learners using gradient descent.\nIt also uses some system optimizations and algorithmic enhancements to make it faster to train compared to its sister model: Gradient Boosting Machine (GBM).</p>\n<p>Similarly to the linear models, we fine tuned the ensemble models using a Grid Cross Validation of Fold 3.\nTheir results are available within Table 3. The models are trained on TF-IDF representations of the concatenation of the claim, claimant, and related article content.\nWe used SciKit-Learn’s and XGBoost’s relevant predictor APIs.</p>\n<h2>4.7 Fully Connected MLP Neural Network Baseline</h2>\n<p>We re-implement the simple fully connected neural network, or multi-layer perceptron (MLP) using PyTorch, in order to verify whether their method would transfer to our fake news dataset.</p>\n<p>The input layer samples from our customized PyTorch <code>Dataset</code> class, which concatenates a 5,000 dimensional TF vector from claim, and another 5,000 dimensional TF vector from related texts, and a cosine similarity score of the TF-IDF vectors of the two, according to \\texAtcite{DBLP:journals/corr/RiedelSR17}‘s hyperparameters. The paper’s architecture only yields around 35% validation accuracy.</p>\n<p>Upon examining the TF and TF-IDF vectors, only around 500 to 1,000 are non-zero. We then decide to limit the dimensions to 500 in the TF and TF-IDF functions. Then, we apply principal component analysis (PCA) to the 500 dimensional TF vectors, and obtain 64 principle components from claim and text.</p>\n<p>In order to address the data imbalance problem, we further modify a plain PyTorch <code>Dataset</code> using a <code>WeightedRandomSampler</code>. This ensures that each batch samples the rare class with a higher probability, and each batch contains approximately equal number of samples from each class.</p>\n<p>We also experiment with various loss metrics utilized by computer vision experiments in handling data imbalance, such as dice loss \\cite{Wong_2018} and focal loss \\cite{lin2017focal}. However, the metrics that are suitable for vision tasks do not transfer to NLP applications. Binary cross entropy loss is the best out of all metrics we tried. Weighted batch sampling and binary cross entropy loss improve our validation set unweighted F1 score from 35% to 45%.</p>\n<h2>4.7 Sequential Neural Network Model: Bidirectional Long Short-Term Memory (Bi-LSTM)</h2>\n<p>The final model we test is a custom neural network model inspired from the Bi-LSTM architecture.\nThe idea behind this model is to leverage simpler model for metadata features and use more complex model such as LSTM to extract relevant information from more complex features like claim and related articles.</p>\n<p>Metadata features such as the claimant, the number of articles supporting the claim are features already in a format that can be passed to the model and are passed directly to the model via a standard MLP. For the claim and related articles, we first preprocess the text like described in section data-preprocessing, then convert the preprocessed text to a fixed length vector using the text to sequence and padding implementations of Keras.\nThen, we pass both the claim resulting vector and the related article resulting vector to two embedding layers. The goal of using embedding layers is convert sparse vector to dense vector. Finally, we concatenate the two dense vectors and pass the resulting concatenated vector to a bi-lstm model.\nTo obtain a final prediction, we concatenate the output of the bi-lstm and of the MLP and pass everything through a softmax layer. Model architecture is available upon request.</p>\n<p>We expect that this model, if trained properly, should be the most performing one due to the sequential nature of the model. To train the model, we used the Nadam optimizer. This optimizer is a mix between two well known optimizers: Nesterov and Adam.</p>\n<h1>Results</h1>\n<p>To evaluate our models’ performance, we are concerned with Weighted Average F1 and Recall for the `True’ class. We are particularly interested by the Recall of the True category since it tells us how much of the truthful news our system was able to detect within the dataset.\nIf some system like this were to be deployed, we would want it to be able to detect ‘True’ news from ‘False’ ones with a high success rate.<br>\nWe would not simply censor all news because it was incorrectly deemed as ‘False.’\nA more detailed look at precision, recall and F1 scores are available in the results table.\nIt is also the measure that highlights a lack of performance in handling an unbalanced dataset.</p>\n<p>Next, we compare our different methods and discuss potential reasons behind their performance.</p>\n<table>\n<thead>\n<tr>\n<th>Model</th>\n<th>Weighted Average F1 score (%)</th>\n<th>Recall of True Class (%)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Most Frequent Classifier</td>\n<td>31.0</td>\n<td>0.0</td>\n</tr>\n<tr>\n<td>Logistic Regression</td>\n<td>58.0</td>\n<td>1.0</td>\n</tr>\n<tr>\n<td>Linear SVM</td>\n<td>57.0</td>\n<td>0.0</td>\n</tr>\n<tr>\n<td>Naive Bayes</td>\n<td>53.0</td>\n<td>0.0</td>\n</tr>\n<tr>\n<td>Random Forest</td>\n<td>60.0</td>\n<td>2.0</td>\n</tr>\n<tr>\n<td>XGBoost</td>\n<td>56.0</td>\n<td>1.0</td>\n</tr>\n<tr>\n<td>MLP</td>\n<td>40.1</td>\n<td>17.0</td>\n</tr>\n<tr>\n<td>Bi-LSTM Custom</td>\n<td>60.0</td>\n<td>0.0</td>\n</tr>\n<tr>\n<td>Bi-LSTM Rebalanced</td>\n<td>57.0</td>\n<td>5.0</td>\n</tr>\n</tbody>\n</table>\n<h1>Author’s Reflection</h1>\n<p>This project was graded by 2 TAs and our professor. They had very different interpretations of this project.\nI want to pause here and jot down some of my own reflections.</p>\n<h2>Writing</h2>\n<p>In my opinion, we spent too much time explaining how the various machine learning models work. The explanations should be reduced by 30%.</p>\n<h2>Lack of NLP domain expertise</h2>\n<p>The project should spend more time in the data preprocessing and exploration stage. For NLP, there are many old-school NLP algorithms that would extract word level, sentence leve, paragraph level, and corpus level features.</p>\n<p>The data contained the news article, as well as the source. It is highly likely that the source itself is a more influential indicator of the reliability of the claim.</p>\n<h2>Attempted too many ML models</h2>\n<p>For someone with more computational linguistics backgorund, this project could sound like a bookkeeping record of different machine learning models we tried. Some models, such as Bi-LSTM Rebalanced, does not provide much more advantage over other methods we have tried. More isn’t always better.</p>\n<style class=\"grvsc-styles\">\n  .grvsc-container {\n    overflow: auto;\n    position: relative;\n    -webkit-overflow-scrolling: touch;\n    padding-top: 1rem;\n    padding-top: var(--grvsc-padding-top, var(--grvsc-padding-v, 1rem));\n    padding-bottom: 1rem;\n    padding-bottom: var(--grvsc-padding-bottom, var(--grvsc-padding-v, 1rem));\n    border-radius: 8px;\n    border-radius: var(--grvsc-border-radius, 8px);\n    font-feature-settings: normal;\n    line-height: 1.4;\n  }\n  \n  .grvsc-code {\n    display: table;\n  }\n  \n  .grvsc-line {\n    display: table-row;\n    box-sizing: border-box;\n    width: 100%;\n    position: relative;\n  }\n  \n  .grvsc-line > * {\n    position: relative;\n  }\n  \n  .grvsc-gutter-pad {\n    display: table-cell;\n    padding-left: 0.75rem;\n    padding-left: calc(var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem)) / 2);\n  }\n  \n  .grvsc-gutter {\n    display: table-cell;\n    -webkit-user-select: none;\n    -moz-user-select: none;\n    user-select: none;\n  }\n  \n  .grvsc-gutter::before {\n    content: attr(data-content);\n  }\n  \n  .grvsc-source {\n    display: table-cell;\n    padding-left: 1.5rem;\n    padding-left: var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem));\n    padding-right: 1.5rem;\n    padding-right: var(--grvsc-padding-right, var(--grvsc-padding-h, 1.5rem));\n  }\n  \n  .grvsc-source:empty::after {\n    content: ' ';\n    -webkit-user-select: none;\n    -moz-user-select: none;\n    user-select: none;\n  }\n  \n  .grvsc-gutter + .grvsc-source {\n    padding-left: 0.75rem;\n    padding-left: calc(var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem)) / 2);\n  }\n  \n  /* Line transformer styles */\n  \n  .grvsc-has-line-highlighting > .grvsc-code > .grvsc-line::before {\n    content: ' ';\n    position: absolute;\n    width: 100%;\n  }\n  \n  .grvsc-line-diff-add::before {\n    background-color: var(--grvsc-line-diff-add-background-color, rgba(0, 255, 60, 0.2));\n  }\n  \n  .grvsc-line-diff-del::before {\n    background-color: var(--grvsc-line-diff-del-background-color, rgba(255, 0, 20, 0.2));\n  }\n  \n  .grvsc-line-number {\n    padding: 0 2px;\n    text-align: right;\n    opacity: 0.7;\n  }\n  \n</style>","frontmatter":{"title":"Fact or Fake News: Applying NLP in an unbalanced dataset","category":["machine learning"],"date":"01 May 2024","coverImage":{"publicURL":"/static/1006654eb0ef46710600df11917acc25/mrbean.jpg","childImageSharp":{"sizes":{"base64":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAALABQDASIAAhEBAxEB/8QAFwAAAwEAAAAAAAAAAAAAAAAAAAMEAv/EABYBAQEBAAAAAAAAAAAAAAAAAAEAAv/aAAwDAQACEAMQAAABIssFJUWv/8QAGxAAAgIDAQAAAAAAAAAAAAAAAQIDEwARITH/2gAIAQEAAQUCjYUSId1Nkfh4H4f/xAAWEQEBAQAAAAAAAAAAAAAAAAAAARH/2gAIAQMBAT8BkY//xAAVEQEBAAAAAAAAAAAAAAAAAAAAEf/aAAgBAgEBPwGq/8QAGRAAAgMBAAAAAAAAAAAAAAAAASEAERIg/9oACAEBAAY/As2zAuf/xAAZEAEAAwEBAAAAAAAAAAAAAAABABEhMVH/2gAIAQEAAT8hLd0FQAElHI27FqewmOZP/9oADAMBAAIAAwAAABAfz//EABcRAQEBAQAAAAAAAAAAAAAAAAEAEXH/2gAIAQMBAT8QwOwW/8QAFhEAAwAAAAAAAAAAAAAAAAAAAAER/9oACAECAQE/EHTKP//EABgQAQEBAQEAAAAAAAAAAAAAAAERACFR/9oACAEBAAE/EBgOUXtVmnJh1JM0IEfM5D0GOREjHdM2ELF3/9k=","aspectRatio":1.7753623188405796,"src":"/static/1006654eb0ef46710600df11917acc25/f72c7/mrbean.jpg","srcSet":"/static/1006654eb0ef46710600df11917acc25/c3816/mrbean.jpg 245w,\n/static/1006654eb0ef46710600df11917acc25/4dc99/mrbean.jpg 490w,\n/static/1006654eb0ef46710600df11917acc25/f72c7/mrbean.jpg 980w,\n/static/1006654eb0ef46710600df11917acc25/68386/mrbean.jpg 1280w","sizes":"(max-width: 980px) 100vw, 980px"}}}},"fields":{"slug":"/news-nlp/","readingTime":{"text":"11 min read"}}},"previous":{"fields":{"slug":"/street-number/"},"frontmatter":{"title":"Identify Street Numbers in Realistic Environments"}},"next":null,"webmention":{"nodes":[]}},"pageContext":{"slug":"/news-nlp/","previousPostId":"2c1a3621-abb9-5a86-a0dc-163f1b2f8927","nextPostId":null,"permalink":"https://violetguos.github.ioblog/news-nlp/"}},"staticQueryHashes":["2652830850","2841359383","287504094","3255793931"]}