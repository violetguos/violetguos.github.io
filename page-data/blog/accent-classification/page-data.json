{"componentChunkName":"component---src-templates-blog-post-js","path":"/blog/accent-classification/","result":{"data":{"avatar":null,"site":{"siteMetadata":{"title":"Violet Guo","siteUrl":"https://violetguos.github.io","author":{"name":"Violet Guo","info":"machine learning, swe"}}},"markdownRemark":{"id":"ceccdeec-09ce-5d95-9228-953f145a95c8","excerpt":"Abstract The task of classifying the accent of recorded speech has generally been approached with traditional SVM or UBM-GMM methods (Omar and Pelecanos, 201…","html":"<h1>Abstract</h1>\n<p>The task of classifying the accent of recorded speech has generally been approached with traditional SVM or UBM-GMM methods (Omar and Pelecanos, 2010; Ge, 2015). However, modern deep learning methods yield the potential to dramatically increase performance. In our report, we train several varieties of Recurrent and Convolutional Neural Networks on three types of features (MFCC, formant, and raw spectrogram) extracted from North American and British English speech recordings in order to predict the accent of the speaker. All deep learning methods examined surpass non-deep baselines, and the approach yielding the best performance was the MFCC-RNN, shortly followed by the Spec-CNN.</p>\n<h1>Background</h1>\n<p>Treating speech audio using automatic techniques has long been a staple of the machine learning field. These techniques culminate in the construction of Automatic Speech Recognition (ASR) systems, which have a wide variety of applications including speech-to-text utilities, electronic personal assistants, and automatic translation systems. While ability to recognize speech automatically has increased dramatically over the past decade due to the advent of deep neural networks, ASR systems still are at the mercy of their input data: human speech is highly variable, influenced by recording noise and by the age, sex, accent, and other characteristics of the speaker.\nIn particular, two individuals speaking the same sentence in the same language might receive different results from the ASR pipeline if they have two different regional accents. A simple solution might be to train two ASR models, one per expected accent, and to perform accent classification on input audio to determine which model to use for downstream tasks. To this end, we explore several predictive models from a variety of machine learning algorithms to classify English speech audio according to the accent of the speaker. Traditionally, accent classification has been approached with SVM or UBM-GMM methods (Omar and Pelecanos, 2010; Ge, 2015), but we are motivated to take advantage of recent deep learning techniques to potentially improve upon this performance.\nConvolutional Neural Networks (CNNs) have been shown to be useful for audio classification tasks. Hershey et al. (2016) explored the performance of several CNN based models, including a fully- connected CNN model and several preexisting architectures such as VGG, Inception, and Resnet, on the classification of 70 million YouTube videos, each tagged with a content label such as sports or singing, using log-mel spectrograms to represent the audio features of the videos. They achieve an AUC of 92.6% using Resnet-50 and 85.1% using their fully-connected model. Chen, Shen, and Tang (2018) performed a binary classification task (native vs. non-native) using the CMU ARCTIC corpus, which contains recordings of US English and other varieties of accented English. They trained a CNN model with 10 convolutional layers and one fully-connected layer on log-amplitude spectrograms representing speech audio segments, reaching a classification accuracy of 97.8%.\nFlavours of Recurrent Neural Networks (RNNs) have also been used for the task of accent classifica- tion. Chu, Lai, and Le (2017) used a Long Short-Term Memory (LSTM) network to classify speech audio segments represented by its Mel-frequency cepstral coefficients (MFCCs) and delta features\ninto 5 accents, yielding only a 39.8% classification accuracy. Since their accuracy on the test set was getting better with more training inputs, the authors did mention that a bigger dataset would improve their performance.\nIn this work, we perform a comparative study of techniques used in accent classification, varying both feature extraction methods and learning algorithms. We aim to classify recordings of English speech into British vs. North American accents, comparing the performance of an RNN trained on MFCC features, an RNN trained on formant features, and a CNN trained on raw spectrogram features. Our effort is to find which combination of feature extraction and deep learning approaches yields most accurate classification results.</p>\n<h1>Methodology</h1>\n<h2>Datasets and preprocessing</h2>\n<p>The audio for the North-American English comes from the “test” subset of the Librispeech Corpus, which is assembled from clean single-speaker recordings of North American audiobook narrators from the open-source public-domain website Librivox (Panayotov et al., 2015). Since no equivalent- quality British English spoken corpus could be found, we assembled the Librit Corpus, a corpus of recordings of British audiobook narrators also sourced from Librivox. Our versions of Librispeech and Librit contain about 5.5 and 7 hours of audio from 40 and 27 speakers respectively, roughly balanced male and female.\nAudio from both corpora was downsampled to 16 kHz mono .wav files, and then split into 20-second clips. While silence is often removed in speech processing tasks, we elected not to do so, because it is possible that natural pauses in speech could be a cue for accent learnable by a classifier.</p>\n<h2>Feature extraction</h2>\n<p>We extract three types of features from our audio in order to explore which is most informative for this task. The first are Mel-frequency cepstral coefficients (MFCCs), a standard in speech processing that is loosely modelled on the processing of the human ear. For each frame of the audio, we calculate its power spectrum, take the logarithm of the summed energies after applying the Mel filterbank, and keep the first 13 coefficients from the discrete cosine transform of these log filterbank energies.\nThe second type of feature we extract are the formant features. Previous research suggests that formants, the vocal tract resonant frequencies measured when a vowel or similar voiced sound is produced, are a salient cue for accent detection (Hansen and Arslan, 1995). Hence, we extract the first three formants from every voiced frame in the audio, following the heuristic of Deshpande, Chikkerur, and Govindaraju (2005): we consider a frame voiced if its log energy is greater than or equal to -9.0, and if its number of zero-crossings is between 1 and 45.\nThirdly, we apply a Fourier transform to the time signal of the audio, generating a spectrogram. We then save the raw spectrogram features, i.e., the frequency, amplitude, and time values needed to visually represent the spectrogram for that audio file.</p>\n<h2>Proposed networks</h2>\n<p>Since the MFCC and formant features are inherently sequential, being extracted from a series of time frames, they are suitable for use with a Recurrent Neural Network (RNN). We propose a Long Short-Term Memory Network (LSTM) for each of these features, since these are especially suited for “remembering” information from many steps into the past, which applies well given the large number of frames in our 20-second examples. We call these the MFCC-RNN and the Formant-RNN respectively.\nTraditionally, Convolutional Neural Networks (CNNs) are used for computer vision tasks to recognize colours, edges, and contours of objects. We follow others in applying this approach to audio data, where the CNN treats the spectrogram representation roughly as an image that contains “objects” reflective of speech characteristics (Chen, Shen, and Tang, 2018; Hershey et al., 2016). Rather than pre-selecting features using speech-specific processing derived by linguists, the CNN learns\n2\nlow-level audio features present in the raw spectrogram, potentially capturing speech characteristics not quantified by MFCCs or formant measurements. We call this network the Spec-CNN.</p>\n<h1>Experiments</h1>\n<p>We split our data into 90% training and 10% testing, or 80% training, 10% validation, 10% testing where appropriate.\nFor each feature, in addition to the relevant trained network, we also test a Most Frequent (MF) baseline that always chooses the class label most frequent in the training data (since our examples do not contain the exact same amount of data from each class, and sometimes variations in preprocessing cause slight differences in the amount of data for which features can be extracted), and an (rbf) SVM baseline. In order to be considered useful, the network must outperform the MF baseline, and it is desirable that it also outperform the SVM baseline. For details on the hyperparameter tuning for all networks, and for detailed diagrams on network architectures, see Appendices B and C.\nFor the MF and SVM baselines, “flattened” versions of the MFCC and formant features were used, which collapsed the features across the time dimension, since these classifiers do not take sequential data as input.</p>\n<h2>MFCC features (MFCC-RNN)</h2>\n<p>For the MFCC-RNN, we train an LSTM with one hidden layer and 350 hidden units using an Adam optimizer and cross-entropy loss over 10 epochs. The classification accuracies and the learning curves of the MFCC-RNN can be found in Table 1 and Figure 1 respectively.</p>\n<p>Project done in collaboration with\nJonathan Bhimani-Burrows\nKhalil Bibi\nArlie Coles\nAkila Jeeson Daniel\nLouis-François Préville-Ratelle\nAuthors listed in alphabetical order</p>\n<style class=\"grvsc-styles\">\n  .grvsc-container {\n    overflow: auto;\n    position: relative;\n    -webkit-overflow-scrolling: touch;\n    padding-top: 1rem;\n    padding-top: var(--grvsc-padding-top, var(--grvsc-padding-v, 1rem));\n    padding-bottom: 1rem;\n    padding-bottom: var(--grvsc-padding-bottom, var(--grvsc-padding-v, 1rem));\n    border-radius: 8px;\n    border-radius: var(--grvsc-border-radius, 8px);\n    font-feature-settings: normal;\n    line-height: 1.4;\n  }\n  \n  .grvsc-code {\n    display: table;\n  }\n  \n  .grvsc-line {\n    display: table-row;\n    box-sizing: border-box;\n    width: 100%;\n    position: relative;\n  }\n  \n  .grvsc-line > * {\n    position: relative;\n  }\n  \n  .grvsc-gutter-pad {\n    display: table-cell;\n    padding-left: 0.75rem;\n    padding-left: calc(var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem)) / 2);\n  }\n  \n  .grvsc-gutter {\n    display: table-cell;\n    -webkit-user-select: none;\n    -moz-user-select: none;\n    user-select: none;\n  }\n  \n  .grvsc-gutter::before {\n    content: attr(data-content);\n  }\n  \n  .grvsc-source {\n    display: table-cell;\n    padding-left: 1.5rem;\n    padding-left: var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem));\n    padding-right: 1.5rem;\n    padding-right: var(--grvsc-padding-right, var(--grvsc-padding-h, 1.5rem));\n  }\n  \n  .grvsc-source:empty::after {\n    content: ' ';\n    -webkit-user-select: none;\n    -moz-user-select: none;\n    user-select: none;\n  }\n  \n  .grvsc-gutter + .grvsc-source {\n    padding-left: 0.75rem;\n    padding-left: calc(var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem)) / 2);\n  }\n  \n  /* Line transformer styles */\n  \n  .grvsc-has-line-highlighting > .grvsc-code > .grvsc-line::before {\n    content: ' ';\n    position: absolute;\n    width: 100%;\n  }\n  \n  .grvsc-line-diff-add::before {\n    background-color: var(--grvsc-line-diff-add-background-color, rgba(0, 255, 60, 0.2));\n  }\n  \n  .grvsc-line-diff-del::before {\n    background-color: var(--grvsc-line-diff-del-background-color, rgba(255, 0, 20, 0.2));\n  }\n  \n  .grvsc-line-number {\n    padding: 0 2px;\n    text-align: right;\n    opacity: 0.7;\n  }\n  \n</style>","frontmatter":{"title":"Speaker Accent Classification with Deep Learning","category":["machine learning"],"date":"26 January 2023","coverImage":{"publicURL":"/static/8ce55349f0a1fda85e615f4ad876728e/image1.png","childImageSharp":{"sizes":{"base64":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAYAAAAvxDzwAAAACXBIWXMAAA7EAAAOxAGVKw4bAAACUUlEQVQ4y3VTCW7bQAz0/x9WtAUKuI5l3cdKeymJD51IbCdTklKaIkAXGFOWlrPkDHcDWvf7Ha+vrxjHEcMwSpznAdM44HIZF3TTgn4WnLsZp8sKeZ7w8vKCzQfhMAxomgbGNFC1QZxaBAeHIHI4MB5qHH5XCLclwu+hIP4ZImL8oOdfIU7HIzbv7+/MSdVM8N6iqFoEO4MiVNBRgjYOFyTRgiyB1zV8o5YoULC1wvl8xuZ0OlF7M/qhh0oVom8RfEyJaQxXVbSxIdQw1sI4v8LR/w9YidqYhbBtW9LogtvtijQooStONtCUpGXzCkowWhMMrNEijSMi7z0c7WW5uDgh5JcjGVBUTiqwtMESyVcwsXMGFclS1x32+wdst1skSQKlFPq+x+bx8XGp8H5FGeYo9jmcKmEavVTFcO6zSoJXBr07kmkH7HY7IeOipGUul0nZZUemZGGDOinRFhlsksKlhKKApSSpkva3usHFWeyDAMF+jziO5RsXJoQ8d9frFZo00sYiSjy1RG06I5oZNifLYOIEhgxibVWjxQjJIdT0XjTkHx5q7p9PcWtrSeJQFKwZoXULyAifZ3BE7vICrixh2SQhXV3moebFo7NoxAYYQZ5bpKmTmGeEwqFuyDBVw9UVaV3RASl8lsKSRELIZLfbDV3XfZqwgkmbhtpTViSoKoOMblCaOUFWeGQ5xdzTgRrn00r49vYmGrIxrOdyp4c19jJSHKeJv/Ukz0Cg+3vs0K13/fl4li7/Xr3/Lb7wPAVPT88iOs/tPE/r139zl+c/5jwSgGM/+3sAAAAASUVORK5CYII=","aspectRatio":1.432748538011696,"src":"/static/8ce55349f0a1fda85e615f4ad876728e/0c87d/image1.png","srcSet":"/static/8ce55349f0a1fda85e615f4ad876728e/316fe/image1.png 245w,\n/static/8ce55349f0a1fda85e615f4ad876728e/0c87d/image1.png 463w","sizes":"(max-width: 463px) 100vw, 463px"}}}},"fields":{"slug":"/accent-classification/","readingTime":{"text":"8 min read"}}},"previous":{"fields":{"slug":"/books-2021/"},"frontmatter":{"title":"The Books I've Read in 2021"}},"next":null,"webmention":{"nodes":[]}},"pageContext":{"slug":"/accent-classification/","previousPostId":"80acfc5d-12e1-5c5b-ae03-48665c9ba98e","nextPostId":null,"permalink":"https://violetguos.github.ioblog/accent-classification/"}},"staticQueryHashes":["2652830850","2841359383","287504094","3255793931"]}