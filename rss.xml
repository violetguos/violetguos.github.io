<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[Violet Guo]]></title><description><![CDATA[Violet Guo]]></description><link>https://violetguos.github.io</link><generator>GatsbyJS</generator><lastBuildDate>Wed, 01 May 2024 21:53:42 GMT</lastBuildDate><item><title><![CDATA[Speaker Accent Classification with Deep Learning]]></title><description><![CDATA[Abstract The task of classifying the accent of recorded speech has generally been approached with traditional SVM or UBM-GMM methods (Omar…]]></description><link>https://violetguos.github.io/accent-classification/</link><guid isPermaLink="false">https://violetguos.github.io/accent-classification/</guid><pubDate>Thu, 26 Jan 2023 00:00:00 GMT</pubDate><content:encoded>&lt;h1&gt;Abstract&lt;/h1&gt;
&lt;p&gt;The task of classifying the accent of recorded speech has generally been approached with traditional SVM or UBM-GMM methods (Omar and Pelecanos, 2010; Ge, 2015). However, modern deep learning methods yield the potential to dramatically increase performance. In our report, we train several varieties of Recurrent and Convolutional Neural Networks on three types of features (MFCC, formant, and raw spectrogram) extracted from North American and British English speech recordings in order to predict the accent of the speaker. All deep learning methods examined surpass non-deep baselines, and the approach yielding the best performance was the MFCC-RNN, shortly followed by the Spec-CNN.&lt;/p&gt;
&lt;h1&gt;Background&lt;/h1&gt;
&lt;p&gt;Treating speech audio using automatic techniques has long been a staple of the machine learning field. These techniques culminate in the construction of Automatic Speech Recognition (ASR) systems, which have a wide variety of applications including speech-to-text utilities, electronic personal assistants, and automatic translation systems. While ability to recognize speech automatically has increased dramatically over the past decade due to the advent of deep neural networks, ASR systems still are at the mercy of their input data: human speech is highly variable, influenced by recording noise and by the age, sex, accent, and other characteristics of the speaker.&lt;/p&gt;
&lt;p&gt;In particular, two individuals speaking the same sentence in the same language might receive different results from the ASR pipeline if they have two different regional accents. A simple solution might be to train two ASR models, one per expected accent, and to perform accent classification on input audio to determine which model to use for downstream tasks. To this end, we explore several predictive models from a variety of machine learning algorithms to classify English speech audio according to the accent of the speaker. Traditionally, accent classification has been approached with SVM or UBM-GMM methods (Omar and Pelecanos, 2010; Ge, 2015), but we are motivated to take advantage of recent deep learning techniques to potentially improve upon this performance.&lt;/p&gt;
&lt;p&gt;Convolutional Neural Networks (CNNs) have been shown to be useful for audio classification tasks. Hershey et al. (2016) explored the performance of several CNN based models, including a fully- connected CNN model and several preexisting architectures such as VGG, Inception, and Resnet, on the classification of 70 million YouTube videos, each tagged with a content label such as sports or singing, using log-mel spectrograms to represent the audio features of the videos. They achieve an AUC of 92.6% using Resnet-50 and 85.1% using their fully-connected model. Chen, Shen, and Tang (2018) performed a binary classification task (native vs. non-native) using the CMU ARCTIC corpus, which contains recordings of US English and other varieties of accented English. They trained a CNN model with 10 convolutional layers and one fully-connected layer on log-amplitude spectrograms representing speech audio segments, reaching a classification accuracy of 97.8%.&lt;/p&gt;
&lt;p&gt;Flavours of Recurrent Neural Networks (RNNs) have also been used for the task of accent classifica- tion. Chu, Lai, and Le (2017) used a Long Short-Term Memory (LSTM) network to classify speech audio segments represented by its Mel-frequency cepstral coefficients (MFCCs) and delta features into 5 accents, yielding only a 39.8% classification accuracy. Since their accuracy on the test set was getting better with more training inputs, the authors did mention that a bigger dataset would improve their performance.&lt;/p&gt;
&lt;p&gt;In this work, we perform a comparative study of techniques used in accent classification, varying both feature extraction methods and learning algorithms. We aim to classify recordings of English speech into British vs. North American accents, comparing the performance of an RNN trained on MFCC features, an RNN trained on formant features, and a CNN trained on raw spectrogram features. Our effort is to find which combination of feature extraction and deep learning approaches yields most accurate classification results.&lt;/p&gt;
&lt;h1&gt;Methodology&lt;/h1&gt;
&lt;h2&gt;Datasets and preprocessing&lt;/h2&gt;
&lt;p&gt;The audio for the North-American English comes from the “test” subset of the Librispeech Corpus, which is assembled from clean single-speaker recordings of North American audiobook narrators from the open-source public-domain website Librivox (Panayotov et al., 2015). Since no equivalent- quality British English spoken corpus could be found, we assembled the Librit Corpus, a corpus of recordings of British audiobook narrators also sourced from Librivox. Our versions of Librispeech and Librit contain about 5.5 and 7 hours of audio from 40 and 27 speakers respectively, roughly balanced male and female.&lt;/p&gt;
&lt;p&gt;Audio from both corpora was downsampled to 16 kHz mono .wav files, and then split into 20-second clips. While silence is often removed in speech processing tasks, we elected not to do so, because it is possible that natural pauses in speech could be a cue for accent learnable by a classifier.&lt;/p&gt;
&lt;h2&gt;Feature extraction&lt;/h2&gt;
&lt;p&gt;We extract three types of features from our audio in order to explore which is most informative for this task. The first are Mel-frequency cepstral coefficients (MFCCs), a standard in speech processing that is loosely modelled on the processing of the human ear. For each frame of the audio, we calculate its power spectrum, take the logarithm of the summed energies after applying the Mel filterbank, and keep the first 13 coefficients from the discrete cosine transform of these log filterbank energies.&lt;/p&gt;
&lt;p&gt;The second type of feature we extract are the formant features. Previous research suggests that formants, the vocal tract resonant frequencies measured when a vowel or similar voiced sound is produced, are a salient cue for accent detection (Hansen and Arslan, 1995). Hence, we extract the first three formants from every voiced frame in the audio, following the heuristic of Deshpande, Chikkerur, and Govindaraju (2005): we consider a frame voiced if its log energy is greater than or equal to -9.0, and if its number of zero-crossings is between 1 and 45.&lt;/p&gt;
&lt;p&gt;Thirdly, we apply a Fourier transform to the time signal of the audio, generating a spectrogram. We then save the raw spectrogram features, i.e., the frequency, amplitude, and time values needed to visually represent the spectrogram for that audio file.&lt;/p&gt;
&lt;h2&gt;Proposed networks&lt;/h2&gt;
&lt;p&gt;Since the MFCC and formant features are inherently sequential, being extracted from a series of time frames, they are suitable for use with a Recurrent Neural Network (RNN). We propose a Long Short-Term Memory Network (LSTM) for each of these features, since these are especially suited for “remembering” information from many steps into the past, which applies well given the large number of frames in our 20-second examples. We call these the MFCC-RNN and the Formant-RNN respectively.&lt;/p&gt;
&lt;p&gt;Traditionally, Convolutional Neural Networks (CNNs) are used for computer vision tasks to recognize colours, edges, and contours of objects. We follow others in applying this approach to audio data, where the CNN treats the spectrogram representation roughly as an image that contains “objects” reflective of speech characteristics (Chen, Shen, and Tang, 2018; Hershey et al., 2016). Rather than pre-selecting features using speech-specific processing derived by linguists, the CNN learns 2 low-level audio features present in the raw spectrogram, potentially capturing speech characteristics not quantified by MFCCs or formant measurements. We call this network the Spec-CNN.&lt;/p&gt;
&lt;h1&gt;Experiments&lt;/h1&gt;
&lt;p&gt;We split our data into 90% training and 10% testing, or 80% training, 10% validation, 10% testing where appropriate.
For each feature, in addition to the relevant trained network, we also test a Most Frequent (MF) baseline that always chooses the class label most frequent in the training data (since our examples do not contain the exact same amount of data from each class, and sometimes variations in preprocessing cause slight differences in the amount of data for which features can be extracted), and an (rbf) SVM baseline. In order to be considered useful, the network must outperform the MF baseline, and it is desirable that it also outperform the SVM baseline. For details on the hyperparameter tuning for all networks, and for detailed diagrams on network architectures, see Appendices B and C.&lt;/p&gt;
&lt;p&gt;For the MF and SVM baselines, “flattened” versions of the MFCC and formant features were used, which collapsed the features across the time dimension, since these classifiers do not take sequential data as input.&lt;/p&gt;
&lt;h2&gt;MFCC features (MFCC-RNN)&lt;/h2&gt;
&lt;p&gt;For the MFCC-RNN, we train an LSTM with one hidden layer and 350 hidden units using an Adam optimizer and cross-entropy loss over 10 epochs. The classification accuracies and the learning curves of the MFCC-RNN can be found in Table 1 and Figure 1 respectively.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Classifier&lt;/th&gt;
&lt;th&gt;Train&lt;/th&gt;
&lt;th&gt;Valid&lt;/th&gt;
&lt;th&gt;Test&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;(%)&lt;/td&gt;
&lt;td&gt;(%)&lt;/td&gt;
&lt;td&gt;(%)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;MF&lt;/td&gt;
&lt;td&gt;63.33&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;59.23&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SVM&lt;/td&gt;
&lt;td&gt;100.00&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;61.71&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;MFCC-RNN&lt;/td&gt;
&lt;td&gt;100.00&lt;/td&gt;
&lt;td&gt;95.17&lt;/td&gt;
&lt;td&gt;95.32&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;Formant features (Formant-RNN)&lt;/h2&gt;
&lt;p&gt;For the Formant-RNN, we train a bidirectional LSTM with one hidden layer of 200 hidden units, three dense layers with 1024 hidden units, and a fourth dense layer with 2048 hidden units. Dropout with probability of 0.3 was added to all of the previous dense layers. The network was trained over 26 epochs (early stopped) using an Adam optimizer and a cross-entropy loss. The classification accuracies and the learning curves of the Formant-RNN can be found in Table 2 and Figure 2 respectively.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Classifier&lt;/th&gt;
&lt;th&gt;Train&lt;/th&gt;
&lt;th&gt;Valid&lt;/th&gt;
&lt;th&gt;Test&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;(%)&lt;/td&gt;
&lt;td&gt;(%)&lt;/td&gt;
&lt;td&gt;(%)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;MF&lt;/td&gt;
&lt;td&gt;53.35&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;55.07&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SVM&lt;/td&gt;
&lt;td&gt;100.00&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;55.07&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Formant-RNN&lt;/td&gt;
&lt;td&gt;99.06&lt;/td&gt;
&lt;td&gt;90.96&lt;/td&gt;
&lt;td&gt;86.96&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;Raw spectrogram features (Spec-CNN)&lt;/h2&gt;
&lt;p&gt;As outlined in Section 2.1, input data initially consisted of 20 second audio clips, but the spectrogram generated was too large for efficient training of CNNs. Thus, following the practice of Hershey et al. (2016) who also trained on spectrogram data, we further sliced our audio into one-second clips, yielding 19475 North American accent examples and 22541 British accent examples. We then generated spectrograms every 10 ms, using a 25 ms window which gives sufficient time and frequency resolution to resolve features in the spectrogram. The spectrograms thus generated have dimensions of 201 frequency bins between 0-8000 Hz and 66 bins along the one-second time axis.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Classifier&lt;/th&gt;
&lt;th&gt;Train&lt;/th&gt;
&lt;th&gt;Valid&lt;/th&gt;
&lt;th&gt;Test&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;(%)&lt;/td&gt;
&lt;td&gt;(%)&lt;/td&gt;
&lt;td&gt;(%)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;MF&lt;/td&gt;
&lt;td&gt;54.52&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;53.67&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SVM&lt;/td&gt;
&lt;td&gt;99.98&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;53.67&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Spec-CNN&lt;/td&gt;
&lt;td&gt;98.12&lt;/td&gt;
&lt;td&gt;94.57&lt;/td&gt;
&lt;td&gt;92.63&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h1&gt;Discussion&lt;/h1&gt;
&lt;p&gt;All three of our deep-learning classifiers (MFCC-RNN, Formant-RNN, and Spec-CNN) are able to outperform both the MF and SVM baselines. The classifier performing the best was the MFCC-RNN, yielding a test accuracy of 95.32%. This combination of feature and learning algorithm seems to strike the right balance for the characterization of speech: MFCCs make use of a large amount of frequency information, and the RNN approach allows previous contextual information to be modelled, which helps account for many progressive phonetic processes that are probably cues for accent. The MFCC features also use information from the entire signal, and not just from heuristically-chosen speech segments, permitting more nuanced modelling.&lt;/p&gt;
&lt;p&gt;The Spec-CNN earns a close second with test accuracy 92.63%. The raw spectrogram features contain nearly three times more information than the MFCC- or Formant-RNNs, comprising frequency and amplitude information for each frame; this large amount of information is likely responsible for much of the classification accuracy. However, since splitting the audio further into one-second clips was necessary, contextual information that the other networks have is lost to Spec-CNN. Moreover, the large size of the spectrogram information, which likely includes significant amounts of noise, means that a more complex model is needed to learn the important features within it, and thus a larger amount of data is needed to yield significant improvement in accuracy. We speculate that larger training corpora might give higher performance, and that timescales longer than one second might also boost accuracy due to a higher number of contextual clues.&lt;/p&gt;
&lt;p&gt;The Formant-RNN, while far sparser in terms of frequency information than the MFCC-RNN, still performs relatively well with 86.96% test accuracy. Its ability to still recover most of the accent cues that the MFCC-RNN detects is likely bolstered by the bidirectional nature of the RNN, which may help model the regressive as well as the progressive phonetic processes present in the audio. However, since formant features are only extracted from heuristically-chosen voiced frames, this classifier is limited not only by the accuracy of the heuristic but also by the number of accent cues present in only the voiced frames. For example, many North American speakers pronounce &lt;em&gt;s&lt;strong&gt;ch&lt;/strong&gt;edule&lt;/em&gt; with a [k], while British speakers use [S], but since neither of these sounds are voiced, this cue cannot be learned by the Formant-RNN.&lt;/p&gt;
&lt;p&gt;There is a practical tradeoff regarding dataset size, computational time, and accuracy: while the Spec-CNN is potentially capable of leveraging a detailed signal into high accuracy, it requires lots of data and computational power (including navigation of GPU and RAM limits). The MFCC-RNN offers similar performance in less time, and is thus suited for smaller datasets. The Formant-RNN, while the easiest to train of the three, yields the lowest performance due to the under-salient cues it learns.&lt;/p&gt;
&lt;h2&gt;Learned features&lt;/h2&gt;
&lt;p&gt;As expected, the networks do learn to “pay attention” to different features. This is visualized in Figure 4, which shows the influence of each subsection of audio on the final classification outcome for one training example. In the RNN cases, we perturb the time steps one at a time by a small value and calculate the difference in classification probabilities between the original sequence and the sequence with the perturbed time step. The larger the difference between these probabilities, the more influential we consider the time step (represented by a yellower colour). In the CNN case, we use the keras-vis API to visualize the learned weights of the convolution filters, where higher weights yield brighter colours and correspond to more important features.&lt;/p&gt;
&lt;p&gt;Qualitative examination of the (British) audio1alongside Figure 4a suggests that the MFCC-RNN sometimes learns something somewhat compatible with human intuition. From left to right, the red boxes roughly correspond to instances of the vowels [6], [O], and [œ] (the sounds in &lt;em&gt;F&lt;strong&gt;eu&lt;/strong&gt;erbach&lt;/em&gt;, &lt;strong&gt;*or&lt;/strong&gt;g&lt;em&gt;, and another rendering of *F&lt;strong&gt;eu&lt;/strong&gt;erbach\&lt;/em&gt;), respectively, which are vowels more common to British dialects than American ones. There are many other yellow bands such as these, suggesting that the MFCC-RNN also learns less transparently interpretable features (i.e., without a perfect correspondence to a human notion such as a particular vowel) used as accent cues.&lt;/p&gt;
&lt;p&gt;In Figure 4b, we observe a much starker difference between more- and less-influential segments (bluer and yellower, respectively). This is due to the lower dimensionality of the input features. In red are boxed the same vowel instances as are found by the MFCC-RNN, and on the whole there seems to be some degree of correspondence between what these two RNNs learn. This is perhaps surprising, since the MFCC data is much more complex and incorporates much more of the signal information than the formant data, which is simply three frequency measurements per voiced frame.&lt;/p&gt;
&lt;p&gt;Figure 4c shows what is treated as important by the final layer of the Spec-CNN. The network pays attention to different features at different convolutional layers (with layer-by-layer diagrams available in Appendix D). In the second convolutional layer, the Spec-CNN weights still resemble a spectrum and it focuses on the lower frequencies, where most salient speech information is aggregated. However, as we go to the third convolutional layer, the weights are abstracted from the notion of a frequency spectrum, spanning more space in the y direction. Research in deep learning is still being done on interpretability of the learned representations; for this reason, we choose not to label y-axis of the CNN (Olah et al., 2018). The red-boxed intervals on the x-axis in Figure 4c highlighted by Spec-CNN correspond to where the speaker pronounces iconically British vowels, as observed in the RNNs. Therefore, we empirically conjecture that Spec-CNN abstracts away the frequency axis &lt;em&gt;per se&lt;/em&gt; and pays attention to the important time intervals. We remark that our two general methods (RNN and CNN) roughly “agree” with each other regarding which segments are important.&lt;/p&gt;
&lt;h1&gt;Conclusion and future work&lt;/h1&gt;
&lt;p&gt;The MFCC-RNN performs best on our accent classification task, likely due to its sufficiently complex and salient features and its ability to take contextual information into account, and its performance is shortly followed by that of the the Spec-CNN (which may outperform the MFCC-RNN given more data and more computational power). The Formant-RNN performs worst of the three, likely due to the excessively low amount of information present in its features.&lt;/p&gt;
&lt;p&gt;A straightforward next step in evaluating these network architectures and feature choices would be to extend them beyond binary classification, including data from many varieties of English. This would allow a more direct comparison with works such as Chu, Lai, and Le (2017), which was unable to classify multiple varieties of accented English well using an MFCC approach.&lt;/p&gt;
&lt;p&gt;Further future work might include expansion on the Spec-CNN, since current research continues to look into new types of convolution to curate filters for audio signals. One successful application of CNNs with raw audio involves using parametrized sinc functions in the convolution layer instead of a traditional convolution, as in SincNet developed by Ravanelli and Bengio (2018). Dilated convolutions have also been shown to be useful for accent correction and audio generation, since they allow the receptive fields to grow longer in a cheaper way than do LSTMs or other RNNs (Oord et al., 2016).&lt;/p&gt;
&lt;p&gt;Though it underperformed in this work, the above-baseline performance of the Formant-RNN also holds potential due to the easy interpretability of its features. Future work might construct a mapping from the features picked out as important by the network back to their original timestamp, which could be the basis of an accent correction system that delivers feedback to a user about which portions of their speech need adjustment in order to imitate a certain accent. This is especially applicable to the Formant-RNN since there is a well-understood relationship between the configuration of the mouth and the first two formants of a vowel. Such a system could provide practical instructions to the user (i.e. to raise or lower the tongue, round the lips, etc.) and could have applications in language learning or speech therapy tools.&lt;/p&gt;
&lt;h1&gt;Appendix&lt;/h1&gt;
&lt;h2&gt;A sample dataset&lt;/h2&gt;
&lt;p&gt;In the project repo, we include 1&lt;em&gt;0&lt;/em&gt;20sec.wav, a 20 second file from Librit (our British audio corpus). In our report, we specify that our neural networks identify the following:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&quot;center&quot;&gt;Phoneme&lt;/th&gt;
&lt;th align=&quot;center&quot;&gt;Word of the phoneme&lt;/th&gt;
&lt;th align=&quot;center&quot;&gt;Time Window in Audio (seconds)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&quot;center&quot;&gt;&lt;em&gt;eu&lt;/em&gt;&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;&lt;em&gt;F&lt;strong&gt;eu&lt;/strong&gt;erbach&lt;/em&gt;&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;1:49 to 2:19&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&quot;center&quot;&gt;&lt;strong&gt;or&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;&lt;strong&gt;or&lt;/strong&gt;g&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;10:86 to 11:39&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&quot;center&quot;&gt;&lt;strong&gt;eu&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;&lt;em&gt;F&lt;strong&gt;eu&lt;/strong&gt;erbach&lt;/em&gt;&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;17:65 to 18:41&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;You can play the audio using any open source or commercial audio player to skip to the following timestamps and verify our results. Additionally, the image files showing the visualization of the weights learned by the neural network are available here in higher resolution.&lt;/p&gt;
&lt;h2&gt;B Hyperparameter tuning on validation set&lt;/h2&gt;
&lt;p&gt;Hyperparameter values for the RNNs were tuned in a heuristic manner, as we lack the computational resources to systematically check each possible combination. For the CNN, a grid search method was used. For all cases, we choose the hyperparameter values that give the highest classification accuracy on the validation set. This ensures we are not overfitting to the training data.&lt;/p&gt;
&lt;h3&gt;B.1 MFCC-RNN&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Number of epochs&lt;/th&gt;
&lt;th&gt;Number of hidden units&lt;/th&gt;
&lt;th&gt;Train&lt;/th&gt;
&lt;th&gt;Valid&lt;/th&gt;
&lt;th&gt;Test&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;100&lt;/td&gt;
&lt;td&gt;99.92&lt;/td&gt;
&lt;td&gt;83.45&lt;/td&gt;
&lt;td&gt;80.17&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;200&lt;/td&gt;
&lt;td&gt;99.85&lt;/td&gt;
&lt;td&gt;90.10&lt;/td&gt;
&lt;td&gt;92.01&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;300&lt;/td&gt;
&lt;td&gt;99.46&lt;/td&gt;
&lt;td&gt;87.57&lt;/td&gt;
&lt;td&gt;92.56&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;325&lt;/td&gt;
&lt;td&gt;98.54&lt;/td&gt;
&lt;td&gt;88.97&lt;/td&gt;
&lt;td&gt;88.71&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;350&lt;/td&gt;
&lt;td&gt;100.00&lt;/td&gt;
&lt;td&gt;95.17&lt;/td&gt;
&lt;td&gt;95.32&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;375&lt;/td&gt;
&lt;td&gt;96.16&lt;/td&gt;
&lt;td&gt;93.10&lt;/td&gt;
&lt;td&gt;96.14&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;400&lt;/td&gt;
&lt;td&gt;99.23&lt;/td&gt;
&lt;td&gt;88.28&lt;/td&gt;
&lt;td&gt;90.63&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Table 5: Percent classification accuracies using various hyperparameter values on the MFCC-RNN&lt;/p&gt;
&lt;h3&gt;B.2 Formant-RNN&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Number of hidden units&lt;/th&gt;
&lt;th&gt;Valid&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;0.58&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;20&lt;/td&gt;
&lt;td&gt;0.56&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;50&lt;/td&gt;
&lt;td&gt;0.62&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;100&lt;/td&gt;
&lt;td&gt;0.63&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;200&lt;/td&gt;
&lt;td&gt;0.74&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Table 6: accuracy using various numbers of hidden units on the Formant-RNN (tuned over 20 epochs)&lt;/p&gt;
&lt;h3&gt;B.3 Spec-CNN&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Acitvation function&lt;/th&gt;
&lt;th&gt;Valid&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Softmax&lt;/td&gt;
&lt;td&gt;0.630513&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Softplus&lt;/td&gt;
&lt;td&gt;0.791683&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Softsign&lt;/td&gt;
&lt;td&gt;0.879486&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Relu&lt;/td&gt;
&lt;td&gt;0.815786&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Tanh&lt;/td&gt;
&lt;td&gt;0.895775&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Sigmoid&lt;/td&gt;
&lt;td&gt;0.699378&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Hard_sigmoid&lt;/td&gt;
&lt;td&gt;0.693815&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Linear&lt;/td&gt;
&lt;td&gt;0.705999&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;learning rate&lt;/th&gt;
&lt;th&gt;Valid&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;0.00001&lt;/td&gt;
&lt;td&gt;0.551715&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0.0001&lt;/td&gt;
&lt;td&gt;0.686002&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0.001&lt;/td&gt;
&lt;td&gt;0.823864&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0.01&lt;/td&gt;
&lt;td&gt;0.544828&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0.1&lt;/td&gt;
&lt;td&gt;0.481128&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Convolution kernel size&lt;/th&gt;
&lt;th&gt;valid&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;0.815786&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;0.796318&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;0.003469&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Initialization mode for kernel&lt;/th&gt;
&lt;th&gt;Valid&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;uniform&lt;/td&gt;
&lt;td&gt;0.888756&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;lecun_uniform&lt;/td&gt;
&lt;td&gt;0.872997&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;normal&lt;/td&gt;
&lt;td&gt;0.890081&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;zero&lt;/td&gt;
&lt;td&gt;0.545226&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;glorot_normal&lt;/td&gt;
&lt;td&gt;0.890081&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;glorot_uniform&lt;/td&gt;
&lt;td&gt;0.883989&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;he_normal&lt;/td&gt;
&lt;td&gt;0.831148&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;he_uniform&lt;/td&gt;
&lt;td&gt;0.843729&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Optimizer&lt;/th&gt;
&lt;th&gt;Valid&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;SGD&lt;/td&gt;
&lt;td&gt;0.679115&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;RMSprop&lt;/td&gt;
&lt;td&gt;0.818037&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Adagrad&lt;/td&gt;
&lt;td&gt;0.624421&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Adadelta&lt;/td&gt;
&lt;td&gt;0.527480&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Adam&lt;/td&gt;
&lt;td&gt;0.808767&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Adamax&lt;/td&gt;
&lt;td&gt;0.723348&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Nadam&lt;/td&gt;
&lt;td&gt;0.790094&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h1&gt;Authors (alphabetical order)&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Jonathan Bhimani-Burrows&lt;/li&gt;
&lt;li&gt;Khalil Bibi&lt;/li&gt;
&lt;li&gt;Arlie Coles&lt;/li&gt;
&lt;li&gt;Akila Jeeson Daniel&lt;/li&gt;
&lt;li&gt;Violet Guo&lt;/li&gt;
&lt;li&gt;Louis-François Préville-Ratelle&lt;/li&gt;
&lt;/ul&gt;
&lt;style class=&quot;grvsc-styles&quot;&gt;
  .grvsc-container {
    overflow: auto;
    position: relative;
    -webkit-overflow-scrolling: touch;
    padding-top: 1rem;
    padding-top: var(--grvsc-padding-top, var(--grvsc-padding-v, 1rem));
    padding-bottom: 1rem;
    padding-bottom: var(--grvsc-padding-bottom, var(--grvsc-padding-v, 1rem));
    border-radius: 8px;
    border-radius: var(--grvsc-border-radius, 8px);
    font-feature-settings: normal;
    line-height: 1.4;
  }
  
  .grvsc-code {
    display: table;
  }
  
  .grvsc-line {
    display: table-row;
    box-sizing: border-box;
    width: 100%;
    position: relative;
  }
  
  .grvsc-line &gt; * {
    position: relative;
  }
  
  .grvsc-gutter-pad {
    display: table-cell;
    padding-left: 0.75rem;
    padding-left: calc(var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem)) / 2);
  }
  
  .grvsc-gutter {
    display: table-cell;
    -webkit-user-select: none;
    -moz-user-select: none;
    user-select: none;
  }
  
  .grvsc-gutter::before {
    content: attr(data-content);
  }
  
  .grvsc-source {
    display: table-cell;
    padding-left: 1.5rem;
    padding-left: var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem));
    padding-right: 1.5rem;
    padding-right: var(--grvsc-padding-right, var(--grvsc-padding-h, 1.5rem));
  }
  
  .grvsc-source:empty::after {
    content: &apos; &apos;;
    -webkit-user-select: none;
    -moz-user-select: none;
    user-select: none;
  }
  
  .grvsc-gutter + .grvsc-source {
    padding-left: 0.75rem;
    padding-left: calc(var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem)) / 2);
  }
  
  /* Line transformer styles */
  
  .grvsc-has-line-highlighting &gt; .grvsc-code &gt; .grvsc-line::before {
    content: &apos; &apos;;
    position: absolute;
    width: 100%;
  }
  
  .grvsc-line-diff-add::before {
    background-color: var(--grvsc-line-diff-add-background-color, rgba(0, 255, 60, 0.2));
  }
  
  .grvsc-line-diff-del::before {
    background-color: var(--grvsc-line-diff-del-background-color, rgba(255, 0, 20, 0.2));
  }
  
  .grvsc-line-number {
    padding: 0 2px;
    text-align: right;
    opacity: 0.7;
  }
  
&lt;/style&gt;</content:encoded></item><item><title><![CDATA[The Books I've Read in 2021]]></title><description><![CDATA[Book Finished? Our Kind of Traitor 1 Tinker, Tailor, Soldier, Spy 1 Brave New World 1 Street Freak 1 How to Not Get F-ed by Technical…]]></description><link>https://violetguos.github.io/books-2021/</link><guid isPermaLink="false">https://violetguos.github.io/books-2021/</guid><pubDate>Wed, 26 Jan 2022 00:00:00 GMT</pubDate><content:encoded>&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Book&lt;/th&gt;
&lt;th&gt;Finished?&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Our Kind of Traitor&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Tinker, Tailor, Soldier, Spy&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Brave New World&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Street Freak&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;How to Not Get F-ed by Technical Recruiters&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;The Catcher in the Rye&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;To Kill a Mockingbird&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Franny and Zooey&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Maggie by My Side&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;The Black Swan: The Impact of the Highly Improbable&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;The Expendables: How the Middle Class Got Screwed by Globalization&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;One Hundred Years of Solitude&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;12 Rules for Life&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Beyond Order&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Unveiled&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Wuthering Heights&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;The Hormone Balance Bible by Shawn Tassone&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Fix my Period by Nicole Jardim&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Women Code&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;The Adrenaline Reset Diet&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Viktor Frankl - Say Yes to Life&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Man’s search for meaning&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;The Rule of Law - Tom Bingham (did not finish)&lt;/td&gt;
&lt;td&gt;0.5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Pride and Prejudice (did not fin)&lt;/td&gt;
&lt;td&gt;0.5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;total&lt;/td&gt;
&lt;td&gt;23&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;style class=&quot;grvsc-styles&quot;&gt;
  .grvsc-container {
    overflow: auto;
    position: relative;
    -webkit-overflow-scrolling: touch;
    padding-top: 1rem;
    padding-top: var(--grvsc-padding-top, var(--grvsc-padding-v, 1rem));
    padding-bottom: 1rem;
    padding-bottom: var(--grvsc-padding-bottom, var(--grvsc-padding-v, 1rem));
    border-radius: 8px;
    border-radius: var(--grvsc-border-radius, 8px);
    font-feature-settings: normal;
    line-height: 1.4;
  }
  
  .grvsc-code {
    display: table;
  }
  
  .grvsc-line {
    display: table-row;
    box-sizing: border-box;
    width: 100%;
    position: relative;
  }
  
  .grvsc-line &gt; * {
    position: relative;
  }
  
  .grvsc-gutter-pad {
    display: table-cell;
    padding-left: 0.75rem;
    padding-left: calc(var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem)) / 2);
  }
  
  .grvsc-gutter {
    display: table-cell;
    -webkit-user-select: none;
    -moz-user-select: none;
    user-select: none;
  }
  
  .grvsc-gutter::before {
    content: attr(data-content);
  }
  
  .grvsc-source {
    display: table-cell;
    padding-left: 1.5rem;
    padding-left: var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem));
    padding-right: 1.5rem;
    padding-right: var(--grvsc-padding-right, var(--grvsc-padding-h, 1.5rem));
  }
  
  .grvsc-source:empty::after {
    content: &apos; &apos;;
    -webkit-user-select: none;
    -moz-user-select: none;
    user-select: none;
  }
  
  .grvsc-gutter + .grvsc-source {
    padding-left: 0.75rem;
    padding-left: calc(var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem)) / 2);
  }
  
  /* Line transformer styles */
  
  .grvsc-has-line-highlighting &gt; .grvsc-code &gt; .grvsc-line::before {
    content: &apos; &apos;;
    position: absolute;
    width: 100%;
  }
  
  .grvsc-line-diff-add::before {
    background-color: var(--grvsc-line-diff-add-background-color, rgba(0, 255, 60, 0.2));
  }
  
  .grvsc-line-diff-del::before {
    background-color: var(--grvsc-line-diff-del-background-color, rgba(255, 0, 20, 0.2));
  }
  
  .grvsc-line-number {
    padding: 0 2px;
    text-align: right;
    opacity: 0.7;
  }
  
&lt;/style&gt;</content:encoded></item><item><title><![CDATA[Dotfiles to Notfiles]]></title><description><![CDATA[I accidentally nuked all of my dotfiles. Let me explain. I finally decided to back up my dotfiles properly. I found a seemingly…]]></description><link>https://violetguos.github.io/lost-dotfile/</link><guid isPermaLink="false">https://violetguos.github.io/lost-dotfile/</guid><pubDate>Mon, 24 Jan 2022 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;I accidentally nuked all of my dotfiles.&lt;/h2&gt;
&lt;blockquote class=&quot;twitter-tweet&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;I decided to organize my dot files. I ran some rando’s script on Stackoverflow and it ended up overwriting my files…. The symlink was buggy 🙃📉&lt;/p&gt;&amp;mdash; Violet Guo (@YVioletGuo) &lt;a href=&quot;https://twitter.com/YVioletGuo/status/1466527787339104264?ref_src=twsrc%5Etfw&quot;&gt;December 2, 2021&lt;/a&gt;&lt;/blockquote&gt; &lt;script async src=&quot;https://platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;
&lt;p&gt;Let me explain. I finally decided to back up my dotfiles properly. I found a seemingly straightforward Bash script that&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;moves your current dotfiles from your root directory to a designated directory, aka your dotfile repo. Now this repo will become the source of truth. It is also tracked by Git.&lt;/li&gt;
&lt;li&gt;creates symlink at your root directory, which links to your dotfile repo.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;What could possibly go wrong? This random script I found wasn’t &lt;em&gt;idempotent&lt;/em&gt;. It failed but did not stop processing the rest of my files in the background. When I tried to run it again, it overwrote the links that were already linked with the link itself, and the originl dotfiles were lost.&lt;/p&gt;
&lt;p&gt;Thankfully, I did stash my dotfiles somewhere. I proceeded with caution and there’re so many ways to go about dotfile mgmt.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;write your own shell script. I don’t trust myself at this point…&lt;/li&gt;
&lt;li&gt;use a git &lt;a href=&quot;https://www.ackama.com/what-we-think/the-best-way-to-store-your-dotfiles-a-bare-git-repository-explained/&quot;&gt;bare&lt;/a&gt; repo. TBH I really didn’t understand the whole rationale behind it.&lt;/li&gt;
&lt;li&gt;install someone’s pkg via a pkg mgmer. but what if the author abandons the project?&lt;/li&gt;
&lt;li&gt;finally, I found the dotbot&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Dotbot&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;it lives with your dotfile repo as a submodule. worst case scenario, the authoer abandons the project, and you just keep one working copy of it&lt;/li&gt;
&lt;li&gt;it’s deisgned to be idempotent. I value this design especially after my dotfile disaster.&lt;/li&gt;
&lt;li&gt;if you want a more detailed tutorial, try &lt;a href=&quot;https://github.com/anishathalye/dotbot&quot;&gt;the official guide&lt;/a&gt; or &lt;a href=&quot;https://www.elliotdenolf.com/posts/bootstrap-your-dotfiles-with-dotbot&quot;&gt;this blog post&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I finally have a pretty basic dotfile setup. It’s been years and I can’t believe &lt;a href=&quot;https://missing.csail.mit.edu/2020/command-line/&quot;&gt;no one taught this in university&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Speedround&lt;/h2&gt;
&lt;p&gt;Taken from one of the dotfile &lt;a href=&quot;https://github.com/webpro/dotfiles/tree/master/config/thefuck&quot;&gt;repos&lt;/a&gt; I found.&lt;/p&gt;
&lt;!-- ![the frustration is real]() --&gt;
&lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 686px; &quot;&gt;
      &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom: 13.46938775510204%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAADCAYAAACTWi8uAAAACXBIWXMAABYlAAAWJQFJUiTwAAAApklEQVQI14WOTQuCQBiE/f8/JugW0aEsKyIKuxhEFkY3+9rXXVtX98k6dOnQwDA8AwMTmKfFVo668bi6oXL1h12b/+S9/+mCZHdgf8yYzBaE0Zz0eOJyV5TWMkw03ZXQWQph8iA9X1lnltFWs8kMcRzT6w+IpjPCccQ+PRBIYVCFJr/cyK83HlIg2rQvK1TZfH3XDl1aiqdv3ZCrCmParRJE5JNvfgFywOMRXXVU4QAAAABJRU5ErkJggg==&apos;); background-size: cover; display: block;&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;frustration&quot; title=&quot;frustration&quot; src=&quot;/static/2d19aa80d31b864337edb2040754e9d8/f6386/frustration.png&quot; srcset=&quot;/static/2d19aa80d31b864337edb2040754e9d8/86a2e/frustration.png 245w,
/static/2d19aa80d31b864337edb2040754e9d8/41d3c/frustration.png 490w,
/static/2d19aa80d31b864337edb2040754e9d8/f6386/frustration.png 686w&quot; sizes=&quot;(max-width: 686px) 100vw, 686px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot; loading=&quot;lazy&quot;&gt;
    &lt;/span&gt;
The frustration is real. End.
&lt;style class=&quot;grvsc-styles&quot;&gt;
  .grvsc-container {
    overflow: auto;
    position: relative;
    -webkit-overflow-scrolling: touch;
    padding-top: 1rem;
    padding-top: var(--grvsc-padding-top, var(--grvsc-padding-v, 1rem));
    padding-bottom: 1rem;
    padding-bottom: var(--grvsc-padding-bottom, var(--grvsc-padding-v, 1rem));
    border-radius: 8px;
    border-radius: var(--grvsc-border-radius, 8px);
    font-feature-settings: normal;
    line-height: 1.4;
  }
  
  .grvsc-code {
    display: table;
  }
  
  .grvsc-line {
    display: table-row;
    box-sizing: border-box;
    width: 100%;
    position: relative;
  }
  
  .grvsc-line &gt; * {
    position: relative;
  }
  
  .grvsc-gutter-pad {
    display: table-cell;
    padding-left: 0.75rem;
    padding-left: calc(var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem)) / 2);
  }
  
  .grvsc-gutter {
    display: table-cell;
    -webkit-user-select: none;
    -moz-user-select: none;
    user-select: none;
  }
  
  .grvsc-gutter::before {
    content: attr(data-content);
  }
  
  .grvsc-source {
    display: table-cell;
    padding-left: 1.5rem;
    padding-left: var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem));
    padding-right: 1.5rem;
    padding-right: var(--grvsc-padding-right, var(--grvsc-padding-h, 1.5rem));
  }
  
  .grvsc-source:empty::after {
    content: &apos; &apos;;
    -webkit-user-select: none;
    -moz-user-select: none;
    user-select: none;
  }
  
  .grvsc-gutter + .grvsc-source {
    padding-left: 0.75rem;
    padding-left: calc(var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem)) / 2);
  }
  
  /* Line transformer styles */
  
  .grvsc-has-line-highlighting &gt; .grvsc-code &gt; .grvsc-line::before {
    content: &apos; &apos;;
    position: absolute;
    width: 100%;
  }
  
  .grvsc-line-diff-add::before {
    background-color: var(--grvsc-line-diff-add-background-color, rgba(0, 255, 60, 0.2));
  }
  
  .grvsc-line-diff-del::before {
    background-color: var(--grvsc-line-diff-del-background-color, rgba(255, 0, 20, 0.2));
  }
  
  .grvsc-line-number {
    padding: 0 2px;
    text-align: right;
    opacity: 0.7;
  }
  
&lt;/style&gt;</content:encoded></item><item><title><![CDATA[Recipe Normalization]]></title><description><![CDATA[In deep learning, we use normalization to solve gradient vanishing or explosion. It kind of inspired me to apply normalization to a recipe…]]></description><link>https://violetguos.github.io/recipe-norm/</link><guid isPermaLink="false">https://violetguos.github.io/recipe-norm/</guid><pubDate>Tue, 07 Sep 2021 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;In deep learning, we use normalization to solve gradient vanishing or explosion. It kind of inspired me to apply normalization to a recipe.&lt;/p&gt;
&lt;p&gt;Let’s dive in.&lt;/p&gt;
&lt;p&gt;This is a gluten free, diary free, processed sugar free cake recipe.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;1/2 cup coconut flour, or 8 tbs&lt;/li&gt;
&lt;li&gt;1/2 cup cacao&lt;/li&gt;
&lt;li&gt;1/2 tsp salt&lt;/li&gt;
&lt;li&gt;1/2 tsp baking soda&lt;/li&gt;
&lt;li&gt;1/4 cup coconut oil&lt;/li&gt;
&lt;li&gt;1 cup raw honey&lt;/li&gt;
&lt;li&gt;6 eggs
(&lt;a href=&quot;https://www.youtube.com/watch?v=Hlh1RJ1Tmig&quot;&gt;Link&lt;/a&gt; to recipe &amp;#x26; tutorial)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you are making this for yourself, it’s a pretty big cake. You could freeze it, but the texture becomes suboptimal.&lt;/p&gt;
&lt;p&gt;I decided to apply a similar normalization idea. I’ll normalize the recipe wrt the eggs.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;1 egg&lt;/strong&gt;, and divide everything by 6.&lt;/li&gt;
&lt;li&gt;1.3 tbs coconut flour (8 tbs/6 = 1.3 tbs approx.)&lt;/li&gt;
&lt;li&gt;1.3 tbs caco&lt;/li&gt;
&lt;li&gt;1/12 tsp salt&lt;/li&gt;
&lt;li&gt;1/12 tsp baking soda&lt;/li&gt;
&lt;li&gt;4 tsp coconut oil&lt;/li&gt;
&lt;li&gt;8 tsp raw honey&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now, it yields one serving. You could alternatively normalize wrt the coconut oil or raw honey, just like how we have &lt;a href=&quot;https://arxiv.org/abs/1502.03167&quot;&gt;batch norm&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1607.06450&quot;&gt;layer norm&lt;/a&gt;, etc in deep learning.&lt;/p&gt;
&lt;p&gt;Happy cooking.&lt;/p&gt;
&lt;style class=&quot;grvsc-styles&quot;&gt;
  .grvsc-container {
    overflow: auto;
    position: relative;
    -webkit-overflow-scrolling: touch;
    padding-top: 1rem;
    padding-top: var(--grvsc-padding-top, var(--grvsc-padding-v, 1rem));
    padding-bottom: 1rem;
    padding-bottom: var(--grvsc-padding-bottom, var(--grvsc-padding-v, 1rem));
    border-radius: 8px;
    border-radius: var(--grvsc-border-radius, 8px);
    font-feature-settings: normal;
    line-height: 1.4;
  }
  
  .grvsc-code {
    display: table;
  }
  
  .grvsc-line {
    display: table-row;
    box-sizing: border-box;
    width: 100%;
    position: relative;
  }
  
  .grvsc-line &gt; * {
    position: relative;
  }
  
  .grvsc-gutter-pad {
    display: table-cell;
    padding-left: 0.75rem;
    padding-left: calc(var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem)) / 2);
  }
  
  .grvsc-gutter {
    display: table-cell;
    -webkit-user-select: none;
    -moz-user-select: none;
    user-select: none;
  }
  
  .grvsc-gutter::before {
    content: attr(data-content);
  }
  
  .grvsc-source {
    display: table-cell;
    padding-left: 1.5rem;
    padding-left: var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem));
    padding-right: 1.5rem;
    padding-right: var(--grvsc-padding-right, var(--grvsc-padding-h, 1.5rem));
  }
  
  .grvsc-source:empty::after {
    content: &apos; &apos;;
    -webkit-user-select: none;
    -moz-user-select: none;
    user-select: none;
  }
  
  .grvsc-gutter + .grvsc-source {
    padding-left: 0.75rem;
    padding-left: calc(var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem)) / 2);
  }
  
  /* Line transformer styles */
  
  .grvsc-has-line-highlighting &gt; .grvsc-code &gt; .grvsc-line::before {
    content: &apos; &apos;;
    position: absolute;
    width: 100%;
  }
  
  .grvsc-line-diff-add::before {
    background-color: var(--grvsc-line-diff-add-background-color, rgba(0, 255, 60, 0.2));
  }
  
  .grvsc-line-diff-del::before {
    background-color: var(--grvsc-line-diff-del-background-color, rgba(255, 0, 20, 0.2));
  }
  
  .grvsc-line-number {
    padding: 0 2px;
    text-align: right;
    opacity: 0.7;
  }
  
&lt;/style&gt;</content:encoded></item><item><title><![CDATA[Tips for techical interviews]]></title><description><![CDATA[After going through technical interviews for small, medium, large companies in the post corona world, I’ve noticed enough differences that I…]]></description><link>https://violetguos.github.io/interview-tips/</link><guid isPermaLink="false">https://violetguos.github.io/interview-tips/</guid><pubDate>Wed, 21 Apr 2021 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;After going through technical interviews for small, medium, large companies in the post corona world, I’ve noticed enough differences that I felt I ought to inform my fellow leetcoders. I also wanted to follow up on my last post and provide some tips.&lt;/p&gt;
&lt;h2&gt;Choose your adventure&lt;/h2&gt;
&lt;p&gt;I categorize the companies in tech into the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Small startups. They have less than 50 employees.&lt;/li&gt;
&lt;li&gt;Medium companies. They just IPOed or are about to IPO.&lt;/li&gt;
&lt;li&gt;Big companies. Think FAANG et al.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Small startups&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Work on the specific stacks for their business&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;e.g. web: Ruby on Rails or PhP, (almost) never a mix&lt;/li&gt;
&lt;li&gt;e.g. ML: pytorch or tensorflow&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;They will ask you to work on a take home assignment with their specific stack&lt;/li&gt;
&lt;li&gt;They don’t care about fluff. Can you do what they do? That’s all they want to know.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Medium companies&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;They quiz you with coding problems like Leetcode easy, but much longer.&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;They expect you to read a very long prompt and apply your data structures.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;They almost never ask candidates to program a data structure from scratch.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You need to know the common data structures, but with little to no emphasis on algorithm&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;using the language of your choice, be well versed in the language’s following libraries&lt;/li&gt;
&lt;li&gt;heap&lt;/li&gt;
&lt;li&gt;priority queue&lt;/li&gt;
&lt;li&gt;stack&lt;/li&gt;
&lt;li&gt;queue&lt;/li&gt;
&lt;li&gt;set&lt;/li&gt;
&lt;li&gt;hash map&lt;/li&gt;
&lt;li&gt;array&lt;/li&gt;
&lt;li&gt;basic string manipulation methods&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The coding problem emphasizes application&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You need to know basic SWE principles&lt;/li&gt;
&lt;li&gt;Write classes, not just a global function&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The coding challenge is usually done via an online coding platform, whether proctored or not. Some popular platforms are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;coderpad&lt;/li&gt;
&lt;li&gt;hackerrank pairing&lt;/li&gt;
&lt;li&gt;hackerrank solo challenges&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Big companies&lt;/h2&gt;
&lt;p&gt;I had relatively less experience with typical FAANG interviews described in popular books such as CTCI. Here’s my takeaway.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You need to grind Leetcode easy, medium, and hard.&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Brush up on obscure data structures&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;trie&lt;/li&gt;
&lt;li&gt;set&lt;/li&gt;
&lt;li&gt;graph&lt;/li&gt;
&lt;li&gt;linked list&lt;/li&gt;
&lt;li&gt;tree&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Emphasis on the algorithm part&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;dynamic programming&lt;/li&gt;
&lt;li&gt;graph, tree traversal&lt;/li&gt;
&lt;li&gt;recursion&lt;/li&gt;
&lt;li&gt;backtracking&lt;/li&gt;
&lt;li&gt;matrix manipulation&lt;/li&gt;
&lt;li&gt;Tip:&lt;/li&gt;
&lt;li&gt;Watch youtube tutorials, then proceed to code.&lt;/li&gt;
&lt;li&gt;Don’t stare at leetcode for hours on end&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Best Youtube channel I found while deep diving in Leetcode’s comment section&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://www.youtube.com/channel/UCmJz2DV1a3yfgrR7GqRtUUA&quot;&gt;https://www.youtube.com/channel/UCmJz2DV1a3yfgrR7GqRtUUA&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Less emphasis on your syntax&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Caution: this has changed due to remote hiring. See disclaimer/FAQ below.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;System design&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;if you are a new grad, you gotta power through it&lt;/li&gt;
&lt;li&gt;I personally think it’s not reasonable to test new grads on this. Assuming that new grads had internships during their studies, I can tell you with great confidence that no company would ever let an intern touch production stuff. If they do, they’re in trouble.&lt;/li&gt;
&lt;li&gt;Watch these youtube channels&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.youtube.com/channel/UCRPMAqdtSgd0Ipeef7iFsKw&quot;&gt;https://www.youtube.com/channel/UCRPMAqdtSgd0Ipeef7iFsKw&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.youtube.com/channel/UCZLJf_R2sWyUtXSKiKlyvAw&quot;&gt;https://www.youtube.com/channel/UCZLJf_R2sWyUtXSKiKlyvAw&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;if you don’t have time, here’s a must watch&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=UzLMhqg3_Wc&quot;&gt;https://www.youtube.com/watch?v=UzLMhqg3_Wc&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Disclaimer/FAQ&lt;/h2&gt;
&lt;h3&gt;Is syntax important?&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;It’s not supposed to be the point.&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In the past, with whiteboard interviewing, as long as you have retained around 90% of the syntax, it’s a pass&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;e.g. in Python, to test whether a character is a number, is it &lt;code&gt;char.isnumber()&lt;/code&gt;? or is it &lt;code&gt;char.isnumeric()&lt;/code&gt;? doesn’t matter. You get the gist of it.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Unfortunately, with remote hiring, it’s no longer the case.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Some companies started using interview platforms with built in compilers.&lt;/li&gt;
&lt;li&gt;The platforms are sometimes not crafted with the flexibility we need.&lt;/li&gt;
&lt;li&gt;You have to prepare for that. If you can’t beat them, join them. Welcome to the leetcode grind.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Which language do I choose?&lt;/h3&gt;
&lt;p&gt;Ideally,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the one you’re familiar with&lt;/li&gt;
&lt;li&gt;the one that’s popular in general&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;in that order&lt;/strong&gt;.&lt;/p&gt;
&lt;h3&gt;Has not knowing a particular programming language backfired?&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Of course, based on my personal experience below.&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Small companies&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;They don’t have the time to onboard you.&lt;/li&gt;
&lt;li&gt;They don’t care for CS generalists. They need someone here to work with their tech stack ASAP.&lt;/li&gt;
&lt;li&gt;They usually advertise with a specific tech stack and fully expect candidates knowing their stack.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Big companies&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;During my interview with one of the As of FAANG, the online coding challenge only allowed C, C++, or Java.&lt;/li&gt;
&lt;li&gt;I know C and C++, but I used them so long ago that I couldn’t possibly complete 5 coding exercises in 30 minutes.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;With remote hiring, it’s more common for companies to automate testing and they care more about your syntactical accuracy. Smaller companies have a different hiring routine. Make sure that you use your time wisely and prepare for the job you want.&lt;/p&gt;
&lt;style class=&quot;grvsc-styles&quot;&gt;
  .grvsc-container {
    overflow: auto;
    position: relative;
    -webkit-overflow-scrolling: touch;
    padding-top: 1rem;
    padding-top: var(--grvsc-padding-top, var(--grvsc-padding-v, 1rem));
    padding-bottom: 1rem;
    padding-bottom: var(--grvsc-padding-bottom, var(--grvsc-padding-v, 1rem));
    border-radius: 8px;
    border-radius: var(--grvsc-border-radius, 8px);
    font-feature-settings: normal;
    line-height: 1.4;
  }
  
  .grvsc-code {
    display: table;
  }
  
  .grvsc-line {
    display: table-row;
    box-sizing: border-box;
    width: 100%;
    position: relative;
  }
  
  .grvsc-line &gt; * {
    position: relative;
  }
  
  .grvsc-gutter-pad {
    display: table-cell;
    padding-left: 0.75rem;
    padding-left: calc(var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem)) / 2);
  }
  
  .grvsc-gutter {
    display: table-cell;
    -webkit-user-select: none;
    -moz-user-select: none;
    user-select: none;
  }
  
  .grvsc-gutter::before {
    content: attr(data-content);
  }
  
  .grvsc-source {
    display: table-cell;
    padding-left: 1.5rem;
    padding-left: var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem));
    padding-right: 1.5rem;
    padding-right: var(--grvsc-padding-right, var(--grvsc-padding-h, 1.5rem));
  }
  
  .grvsc-source:empty::after {
    content: &apos; &apos;;
    -webkit-user-select: none;
    -moz-user-select: none;
    user-select: none;
  }
  
  .grvsc-gutter + .grvsc-source {
    padding-left: 0.75rem;
    padding-left: calc(var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem)) / 2);
  }
  
  /* Line transformer styles */
  
  .grvsc-has-line-highlighting &gt; .grvsc-code &gt; .grvsc-line::before {
    content: &apos; &apos;;
    position: absolute;
    width: 100%;
  }
  
  .grvsc-line-diff-add::before {
    background-color: var(--grvsc-line-diff-add-background-color, rgba(0, 255, 60, 0.2));
  }
  
  .grvsc-line-diff-del::before {
    background-color: var(--grvsc-line-diff-del-background-color, rgba(255, 0, 20, 0.2));
  }
  
  .grvsc-line-number {
    padding: 0 2px;
    text-align: right;
    opacity: 0.7;
  }
  
&lt;/style&gt;</content:encoded></item></channel></rss>