<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[Violet Guo]]></title><description><![CDATA[Violet Guo]]></description><link>https://violetguos.github.io</link><generator>GatsbyJS</generator><lastBuildDate>Fri, 17 May 2024 22:36:23 GMT</lastBuildDate><item><title><![CDATA[What MLEs can learn from the LLaMA Paper About Efficient LLM Models]]></title><description><![CDATA[I recently read the LLAMA paper from an industry pracitioner’s point of view.
The quoted sections are exerepts from the original paper. The…]]></description><link>https://violetguos.github.io/margin-note-llama/</link><guid isPermaLink="false">https://violetguos.github.io/margin-note-llama/</guid><pubDate>Fri, 17 May 2024 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;I recently read the LLAMA paper from an industry pracitioner’s point of view.
The quoted sections are exerepts from the original paper. The following comments, questions, and observations are based on my margin notes.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;However, recent work from Hoffmann et al. (2022) shows that, for a given compute budget, the best performances are not achieved by the largest mod- els, but by smaller models trained on more data.
The objective of the scaling laws from Hoff- mann et al. (2022) is to determine how to best scale the dataset and model sizes for a particular training compute budget. However, this objective disregards the inference budget, which becomes critical when serving a language model at scale.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;very insghtful remark with practical implications in model serving for most user facing products.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;2.1 Pretraining Data&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Meta preprocessed text data with basic non DL models, such as&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CCNET pipeline&lt;/li&gt;
&lt;li&gt;fast text linear classifeir&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;The preprocessing of C4 also contains deduplication and language identifi- cation steps: the main difference with CCNet is the quality filtering, which mostly relies on heuris- tics such as presence of punctuation marks or the number of words and sentences in a webpage&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I wonder if they did any hyper parameter tuning, e.g. empirical observation of different heuristics used instead of just presence of puncation.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Github
we filtered low quality files with heuristics based on the line length or proportion of alphanumeric characters, and removed boilerplate, such as headers, with reg- ular expressions.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;How did they decide whcih regex to use?&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;arXiv. We process arXiv Latex files to add scientific data to our dataset. Following Lewkowycz et al. (2022), we removed everything before the first section, as well as the bibliography. We also removed the comments from the .tex files, and inline-expanded definitions and macros written by users to increase consistency across papers.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In Lwekowycz et al, they explianed this preprocessing in more detail. In a typical NeurIps paper, the format is set by the conference. It roughyly follows the following structure.&lt;/p&gt;
&lt;pre class=&quot;grvsc-container quiet-light grvsc-ps-tuw09S&quot; data-language=&quot;&quot; data-index=&quot;0&quot;&gt;&lt;code class=&quot;grvsc-code&quot;&gt;&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;\userpackage{some_package}&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;\some_customized_color_for_fancy_plots&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;\author_list{some names}&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;\title{impressive acronym for the model}&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;% some comment&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;% or a lot of comments&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;\begin{abstract}&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;usually we only read this&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;\end{abstract}&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;\section{One big section}&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;continues until conclusion&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;\include{compiled bibliography}&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;\appendex{some extra info}&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lwekowycz et al removed the bibliography as well as everything before the first &lt;code&gt;\section&lt;/code&gt;. I am curious why they chose to exclude the abstract. Their work intended to train language models to perform on mathemtical text. Would including the abstracts that summarize the overall research and mathematical princples enhance the model’s peroformance on mathematical reasoning?&lt;/p&gt;
&lt;p&gt;No explanation was given by LLAMA authors or Lewkowycz et al.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Pre-normalization [GPT3]. To improve the training stability, we normalize the input of each transformer sub-layer, instead of normalizing the output. We use the RMSNorm normalizing func- tion, introduced by Zhang and Sennrich (2019).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Did they run into numerical stability issues in the input layer even before the activitions? Or was it just done to boost performance? What about other normalization methods?&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;This imple- mentation, available in the xformers library,2 is inspired by Rabe and Staats (2021) and uses the backward from Dao et al. (2022). This is achieved by not storing the attention weights and not com- puting the key/query scores that are masked due to the causal nature of the language modeling task.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;For masked tokens, they simply skipped the computation.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;To further improve training efficiency, we re- duced the amount of activations that are recom- puted during the backward pass with checkpoint- ing. More precisely, we save the activations that are expensive to compute, such as the outputs of linear layers. This is achieved by manually imple- menting the backward function for the transformer layers, instead of relying on the PyTorch autograd.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The authors played with pytorch’s autograd. My team did this when we tried to train a Wasserstein GAN. In any neural network, normally PyTorch builds a graph based on all the matrix transoformations you define, and does all the matrix calculus for you automatically. When you disable autograd, from that layer going backward, you are on your own. You need to&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;derive the matrix calculus needed for backpropagation&lt;/li&gt;
&lt;li&gt;program it&lt;/li&gt;
&lt;li&gt;ensure the matrix dimensions are correct&lt;/li&gt;
&lt;li&gt;enhance the performance, address issues such as OOM and parallelism&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Ideally, the authors shouldn’t skim over this section, as it is extremely hard. Many folks in industry would benefit greatly if they could be more candid and discuss things they’ve tried, and what worked vs what didn’t work.&lt;/p&gt;
&lt;p&gt;Fortunately, Meta researchers were generous and provided a link to their repo. This &lt;a href=&quot;https://github.com/facebookresearch/xformers/blob/6e1718b4af7e80087b9a247a6cd100b0cd2be339/xformers/components/reversible.py#L87&quot;&gt;customzied back propagation function&lt;/a&gt; looks like their customized gradient calculation in the backpropagation.&lt;/p&gt;
&lt;pre class=&quot;grvsc-container quiet-light grvsc-ps-tuw09S&quot; data-language=&quot;python&quot; data-index=&quot;1&quot;&gt;&lt;code class=&quot;grvsc-code&quot;&gt;&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-7 grvsc-tuw09S-11&quot;&gt;def&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-9 grvsc-tZ0ymb-b grvsc-tuw09S-10&quot;&gt;backward_pass&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;(&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;        &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-7 grvsc-tuw09S-3&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-7 grvsc-tuw09S-3&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; torch&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;Tensor&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-7 grvsc-tuw09S-3&quot;&gt;dy&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; torch&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;Tensor&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-7 grvsc-tuw09S-3&quot;&gt;f_args&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-15&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;{},&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-7 grvsc-tuw09S-3&quot;&gt;g_args&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-15&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;{}&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;):&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-3 grvsc-tZ0ymb-i grvsc-tuw09S-4&quot;&gt;# pragma: no cover  # this is covered, but called directly from C++&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;        y1&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; y2 &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-15&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; torch&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;chunk&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-5 grvsc-tuw09S-3&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-7 grvsc-tuw09S-3&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-15&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-5 grvsc-tuw09S-13&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;split_dim&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;        &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-10 grvsc-tuw09S-11&quot;&gt;del&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; y&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;        dy1&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; dy2 &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-15&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; torch&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;chunk&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;dy&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-5 grvsc-tuw09S-3&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-7 grvsc-tuw09S-3&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-15&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-5 grvsc-tuw09S-13&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;split_dim&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;        &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-10 grvsc-tuw09S-11&quot;&gt;del&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; dy&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;        &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-10 grvsc-tuw09S-11&quot;&gt;with&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; torch&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;enable_grad&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;():&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;            y1&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;requires_grad &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-15&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-5 grvsc-tuw09S-7&quot;&gt;True&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;            gy1 &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-15&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-5 grvsc-tuw09S-13&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;y1&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-7 grvsc-tuw09S-3&quot;&gt;set_rng&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-15&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-5 grvsc-tuw09S-7&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-15&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;g_args&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;            torch&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;autograd&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;gy1&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; dy2&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;        &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-10 grvsc-tuw09S-11&quot;&gt;with&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; torch&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;no_grad&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;():&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;            x2 &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-15&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; y2 &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-15&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; gy1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;            &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-10 grvsc-tuw09S-11&quot;&gt;del&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; y2&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; gy1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;            dx1 &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-15&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; dy1 &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-15&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; y1&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;grad&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;            &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-10 grvsc-tuw09S-11&quot;&gt;del&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; dy1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;            y1&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;grad &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-15&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-5 grvsc-tuw09S-7&quot;&gt;None&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;        &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-10 grvsc-tuw09S-11&quot;&gt;with&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; torch&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;enable_grad&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;():&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;            x2&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;requires_grad &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-15&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-5 grvsc-tuw09S-7&quot;&gt;True&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;            fx2 &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-15&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-5 grvsc-tuw09S-13&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;x2&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-7 grvsc-tuw09S-3&quot;&gt;set_rng&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-15&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-5 grvsc-tuw09S-7&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-15&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;f_args&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;            torch&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;autograd&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;fx2&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; dx1&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;        &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-10 grvsc-tuw09S-11&quot;&gt;with&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; torch&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;no_grad&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;():&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;            x1 &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-15&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; y1 &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-15&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; fx2&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;            &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-10 grvsc-tuw09S-11&quot;&gt;del&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; y1&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; fx2&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;            dx2 &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-15&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; dy2 &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-15&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; x2&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;grad&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;            &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-10 grvsc-tuw09S-11&quot;&gt;del&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; dy2&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;            x2&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;grad &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-15&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-5 grvsc-tuw09S-7&quot;&gt;None&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;            x &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-15&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; torch&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;cat&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;x1&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; x2&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;detach&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;()],&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-7 grvsc-tuw09S-3&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-15&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-5 grvsc-tuw09S-13&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;split_dim&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;            dx &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-15&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; torch&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;cat&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;dx1&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; dx2&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-7 grvsc-tuw09S-3&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-15&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-5 grvsc-tuw09S-13&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;split_dim&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;        &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-10 grvsc-tuw09S-11&quot;&gt;return&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; x&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; dx&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The lines such as &lt;code&gt;del y&lt;/code&gt; and &lt;code&gt;del dy&lt;/code&gt; were probably added to address OOM issues. The authors switched between &lt;code&gt;x2.detach()&lt;/code&gt;, &lt;code&gt;with torch.no_grad()&lt;/code&gt; and &lt;code&gt;with torch.enable_grad()&lt;/code&gt;, and combined the results. This was what the authors meant by ”[saving] the activations that are expensive to compute” during the backward pass.&lt;/p&gt;
&lt;p&gt;The rest of the paper focuses on analyzing model peroformance on standard metrics and benchmarks in NLP. It was trained on Meta’s GPU cluster for 21 days. Essentially, in order to improve model performance during training and inference, it requires a combination of mathematical and engineering rigor.&lt;/p&gt;
&lt;p&gt;The quoted text are from LLaMA: Open and Efficient Foundation Language Models by Touvron et al, &lt;a href=&quot;https://arxiv.org/abs/2302.13971&quot;&gt;https://arxiv.org/abs/2302.13971&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code: &lt;a href=&quot;https://github.com/facebookresearch/xformers&quot;&gt;https://github.com/facebookresearch/xformers&lt;/a&gt;&lt;/p&gt;
&lt;style class=&quot;grvsc-styles&quot;&gt;
  .grvsc-container {
    overflow: auto;
    position: relative;
    -webkit-overflow-scrolling: touch;
    padding-top: 1rem;
    padding-top: var(--grvsc-padding-top, var(--grvsc-padding-v, 1rem));
    padding-bottom: 1rem;
    padding-bottom: var(--grvsc-padding-bottom, var(--grvsc-padding-v, 1rem));
    border-radius: 8px;
    border-radius: var(--grvsc-border-radius, 8px);
    font-feature-settings: normal;
    line-height: 1.4;
  }
  
  .grvsc-code {
    display: table;
  }
  
  .grvsc-line {
    display: table-row;
    box-sizing: border-box;
    width: 100%;
    position: relative;
  }
  
  .grvsc-line &gt; * {
    position: relative;
  }
  
  .grvsc-gutter-pad {
    display: table-cell;
    padding-left: 0.75rem;
    padding-left: calc(var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem)) / 2);
  }
  
  .grvsc-gutter {
    display: table-cell;
    -webkit-user-select: none;
    -moz-user-select: none;
    user-select: none;
  }
  
  .grvsc-gutter::before {
    content: attr(data-content);
  }
  
  .grvsc-source {
    display: table-cell;
    padding-left: 1.5rem;
    padding-left: var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem));
    padding-right: 1.5rem;
    padding-right: var(--grvsc-padding-right, var(--grvsc-padding-h, 1.5rem));
  }
  
  .grvsc-source:empty::after {
    content: &apos; &apos;;
    -webkit-user-select: none;
    -moz-user-select: none;
    user-select: none;
  }
  
  .grvsc-gutter + .grvsc-source {
    padding-left: 0.75rem;
    padding-left: calc(var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem)) / 2);
  }
  
  /* Line transformer styles */
  
  .grvsc-has-line-highlighting &gt; .grvsc-code &gt; .grvsc-line::before {
    content: &apos; &apos;;
    position: absolute;
    width: 100%;
  }
  
  .grvsc-line-diff-add::before {
    background-color: var(--grvsc-line-diff-add-background-color, rgba(0, 255, 60, 0.2));
  }
  
  .grvsc-line-diff-del::before {
    background-color: var(--grvsc-line-diff-del-background-color, rgba(255, 0, 20, 0.2));
  }
  
  .grvsc-line-number {
    padding: 0 2px;
    text-align: right;
    opacity: 0.7;
  }
  
  .quiet-light { background-color: #F5F5F5; }
  .quiet-light .grvsc-tZ0ymb-i { font-style: italic; }
  .quiet-light .grvsc-tZ0ymb-b { font-weight: bold; }
  .quiet-light .grvsc-tZ0ymb-1 { color: #333333; }
  .quiet-light .grvsc-tZ0ymb-7 { color: #7A3E9D; }
  .quiet-light .grvsc-tZ0ymb-9 { color: #AA3731; }
  .quiet-light .grvsc-tZ0ymb-6 { color: #777777; }
  .quiet-light .grvsc-tZ0ymb-3 { color: #AAAAAA; }
  .quiet-light .grvsc-tZ0ymb-5 { color: #9C5D27; }
  .quiet-light .grvsc-tZ0ymb-10 { color: #4B69C6; }
  .quiet-light .grvsc-line-highlighted::before {
    background-color: var(--grvsc-line-highlighted-background-color, rgba(0, 0, 0, 0.05));
    box-shadow: inset var(--grvsc-line-highlighted-border-width, 4px) 0 0 0 var(--grvsc-line-highlighted-border-color, rgba(0, 0, 0, 0.2));
  }
  div#dark .grvsc-ps-tuw09S {
    background-color: #1e1e1e;
    color: #c5c8c6;
  }
  div#dark .grvsc-ps-tuw09S .grvsc-tuw09S-1 { color: #C5C8C6FF; }
  div#dark .grvsc-ps-tuw09S .grvsc-tuw09S-11 { color: #9872A2; }
  div#dark .grvsc-ps-tuw09S .grvsc-tuw09S-10 { color: #CE6700; }
  div#dark .grvsc-ps-tuw09S .grvsc-tuw09S-3 { color: #6089B4; }
  div#dark .grvsc-ps-tuw09S .grvsc-tuw09S-15 { color: #676867; }
  div#dark .grvsc-ps-tuw09S .grvsc-tuw09S-4 { color: #9A9B99; }
  div#dark .grvsc-ps-tuw09S .grvsc-tuw09S-13 { color: #C7444A; }
  div#dark .grvsc-ps-tuw09S .grvsc-tuw09S-7 { color: #408080; }
  div#dark .grvsc-ps-tuw09S .grvsc-line-highlighted::before {
    background-color: var(--grvsc-line-highlighted-background-color, rgba(255, 255, 255, 0.1));
    box-shadow: inset var(--grvsc-line-highlighted-border-width, 4px) 0 0 0 var(--grvsc-line-highlighted-border-color, rgba(255, 255, 255, 0.5));
  }
&lt;/style&gt;</content:encoded></item><item><title><![CDATA[A World without Tensorflow or Pytorch]]></title><description><![CDATA[Before neural netowrk frameworks were released, there was no effective way to automatically run gradient descent, defined model layer…]]></description><link>https://violetguos.github.io/neural-net-raw/</link><guid isPermaLink="false">https://violetguos.github.io/neural-net-raw/</guid><pubDate>Fri, 03 May 2024 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;Before neural netowrk frameworks were released, there was no effective way to automatically run gradient descent, defined model layer dimension, or initialize weights. All these features are baked into Tensorflow and Pytorch.&lt;/p&gt;
&lt;p&gt;In a world without any neural network frameworks, we need to understand the fundamentals of a fully connected layer (aka MLP, neural network).&lt;/p&gt;
&lt;p&gt;The following example assumes that we use the MNIST dataset.&lt;/p&gt;
&lt;h1&gt;fully connected layer&lt;/h1&gt;
&lt;p&gt;A matrix represents an image of a digit, and each cell is a pixel value. The image has dimension n by d.&lt;/p&gt;
&lt;p&gt;Before passing through the activation function, we need to multiply the input by the weight and add the bias offset.&lt;/p&gt;
&lt;p&gt;Mathmatically, it is represented like so&lt;/p&gt;
&lt;p&gt;Wx^T + b&lt;/p&gt;
&lt;p&gt;W: dh by d, where dh is the dimension of the hidden layer
x: n by d
b: dh by n&lt;/p&gt;
&lt;p&gt;which yields a dh by n matrix.&lt;/p&gt;
&lt;p&gt;The activation is applied per element of the matrix&lt;/p&gt;
&lt;pre class=&quot;grvsc-container quiet-light grvsc-ps-tuw09S&quot; data-language=&quot;python&quot; data-index=&quot;0&quot;&gt;&lt;code class=&quot;grvsc-code&quot;&gt;&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-7 grvsc-tuw09S-11&quot;&gt;def&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-9 grvsc-tZ0ymb-b grvsc-tuw09S-10&quot;&gt;relu&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-7 grvsc-tuw09S-3&quot;&gt;pre_activation&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;):&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-8&quot;&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-4 grvsc-tuw09S-8&quot;&gt;    Relu using only numpy.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-4 grvsc-tuw09S-8&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-8&quot;&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;    relu_output &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-15&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; np&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;pre_activation&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;    relu_flat &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-15&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; relu_output&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;flatten&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;()&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-10 grvsc-tuw09S-11&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; i&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; neuron &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-15&quot;&gt;in&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-9 grvsc-tZ0ymb-b grvsc-tuw09S-11&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;pre_activation&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;flatten&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;()):&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;        &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-10 grvsc-tuw09S-11&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; neuron &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-15&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-5 grvsc-tuw09S-3&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;:&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;            relu_flat&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-15&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; neuron&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;    relu_output &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-15&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; relu_flat&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;pre_activation&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-10 grvsc-tuw09S-11&quot;&gt;return&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; relu_output&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The output after activation retains the same input matrix dimenion, in this case dh by n&lt;/p&gt;
&lt;h1&gt;weight initialization&lt;/h1&gt;
&lt;p&gt;If we initialized all the weights to 0, backpropogation will always be 0. There is nothing to learn.&lt;/p&gt;
&lt;p&gt;Instead, a simple and effective initialization uses the hidden layer dimensions, and samples a matrix from a normal distribution, with the value bounded by the square root of $\frac{6}{sum of layer dimensions}$&lt;/p&gt;
&lt;pre class=&quot;grvsc-container quiet-light grvsc-ps-tuw09S&quot; data-language=&quot;python&quot; data-index=&quot;1&quot;&gt;&lt;code class=&quot;grvsc-code&quot;&gt;&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-7 grvsc-tuw09S-11&quot;&gt;def&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-9 grvsc-tZ0ymb-b grvsc-tuw09S-10&quot;&gt;glorot_init&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-7 grvsc-tuw09S-3&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-7 grvsc-tuw09S-3&quot;&gt;dh1&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-7 grvsc-tuw09S-3&quot;&gt;dh2&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-7 grvsc-tuw09S-3&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;):&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;    dl_1 &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-15&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; np&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-5 grvsc-tuw09S-3&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-15&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;d &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-15&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; dh1&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;)))&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;    W_1 &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-15&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; np&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;uniform&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-15&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-5 grvsc-tuw09S-3&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-15&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;dl_1&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; dl_1&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;dh1&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; d&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;))&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-10 grvsc-tuw09S-11&quot;&gt;return&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; W_1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The intuition behind this design is the forward propagation Wx^T + b will sum over every row in W, so having the layer’s dimension in the denominator will prevent the result Wx^T from exploding.&lt;/p&gt;
&lt;h1&gt;Output&lt;/h1&gt;
&lt;p&gt;Typically, we use softmax function to essentially &lt;em&gt;cast&lt;/em&gt; a set of values into a probability distribution.
The output will retain the shape of the input to softmax, and each value can be interpreted as a probability.
The index of the maximum probability is the index of the predicted category.&lt;/p&gt;
&lt;pre class=&quot;grvsc-container quiet-light grvsc-ps-tuw09S&quot; data-language=&quot;python&quot; data-index=&quot;2&quot;&gt;&lt;code class=&quot;grvsc-code&quot;&gt;&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-7 grvsc-tuw09S-11&quot;&gt;def&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-9 grvsc-tZ0ymb-b grvsc-tuw09S-10&quot;&gt;softmax&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-7 grvsc-tuw09S-3&quot;&gt;pre_activation&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;):&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-8&quot;&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-4 grvsc-tuw09S-8&quot;&gt;    Numerically stable because subtracting the max value makes bit overflow impossible,&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-4 grvsc-tuw09S-8&quot;&gt;    we will only have non-positive values in the vector&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-4 grvsc-tuw09S-8&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-8&quot;&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;    exps &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-15&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; np&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;pre_activation &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-15&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; np&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;pre_activation&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;))&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-10 grvsc-tuw09S-11&quot;&gt;return&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; exps &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-15&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; np&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;exps&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;Backpropagation&lt;/h1&gt;
&lt;p&gt;The probability of an image of handwritten “1” to be 1 should be 100%.
We want to maximize this probability. Since we have the softmax representation from the forward propagation’s output, we want to maximize the softmax, aka we want softmax of the correct class to equal 1.&lt;/p&gt;
&lt;p&gt;The following will require you thinking like a true mathematician. In order to maximize this probaility, we can minimize negative of the log-likelihood to achieve the same effect.&lt;/p&gt;
&lt;p&gt;We don’t have a magically formula to find the weight values to minimize the negative log likehood. Instead, we modify the values so we move closer to the minimum log likelihood iteration by iteration. This can be done using gradient descent. For every iteration, we update the value of the weights by the value of this gradient.
If you are interested in the derivations, visit &lt;a href=&quot;https://www.highonscience.com/blog/2021/06/18/ml-loss-function-cheat-sheet/#log-loss&quot;&gt;this blog&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&quot;grvsc-container quiet-light grvsc-ps-tuw09S&quot; data-language=&quot;python&quot; data-index=&quot;3&quot;&gt;&lt;code class=&quot;grvsc-code&quot;&gt;&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-7 grvsc-tuw09S-11&quot;&gt;def&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-9 grvsc-tZ0ymb-b grvsc-tuw09S-10&quot;&gt;backprop&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-7 grvsc-tuw09S-3&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-7 grvsc-tuw09S-3&quot;&gt;batch_data&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-7 grvsc-tuw09S-3&quot;&gt;batch_target&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;):&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;        &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-8&quot;&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-4 grvsc-tuw09S-8&quot;&gt;        dimensions:&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-4 grvsc-tuw09S-8&quot;&gt;        o_s: m x1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-4 grvsc-tuw09S-8&quot;&gt;        grad_oa : m x 1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-4 grvsc-tuw09S-8&quot;&gt;        hs: dh x 1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-4 grvsc-tuw09S-8&quot;&gt;        grad_w2: m x dh&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-4 grvsc-tuw09S-8&quot;&gt;        grad_oa: m x n&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-4 grvsc-tuw09S-8&quot;&gt;        grad_b2: m x n&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-4 grvsc-tuw09S-8&quot;&gt;        grad_oa: m x n&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-4 grvsc-tuw09S-8&quot;&gt;        W(2): m x dh&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-4 grvsc-tuw09S-8&quot;&gt;        grad_hs: dh x n&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-4 grvsc-tuw09S-8&quot;&gt;        grad_oa: m x n&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-4 grvsc-tuw09S-8&quot;&gt;        grad_ha: dh x n&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-4 grvsc-tuw09S-8&quot;&gt;        x : n x d&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-4 grvsc-tuw09S-8&quot;&gt;        grad_W1: dh x d&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-4 grvsc-tuw09S-8&quot;&gt;        grad_ha: dh x n&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-4 grvsc-tuw09S-8&quot;&gt;        grad_b1: dh x n&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-4 grvsc-tuw09S-8&quot;&gt;        &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-8&quot;&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;        &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-5 grvsc-tuw09S-13&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;grad_oa &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-15&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-5 grvsc-tuw09S-13&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;o_s &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-15&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; batch_target&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;        &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-3 grvsc-tZ0ymb-i grvsc-tuw09S-4&quot;&gt;# hidden layer 3&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;        &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-5 grvsc-tuw09S-13&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;grad_W3 &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-15&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; np&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;outer&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-5 grvsc-tuw09S-13&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;grad_oa&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-5 grvsc-tuw09S-13&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;h_s2&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;        &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-5 grvsc-tuw09S-13&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;grad_b3 &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-15&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-5 grvsc-tuw09S-13&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;grad_oa&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;        &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-5 grvsc-tuw09S-13&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;grad_hs2 &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-15&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; np&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-5 grvsc-tuw09S-13&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;W_3&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-5 grvsc-tuw09S-13&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;grad_oa&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;        h_a_stack2 &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-15&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; np&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;where&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-5 grvsc-tuw09S-13&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;h_a2 &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-15&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-5 grvsc-tuw09S-3&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-5 grvsc-tuw09S-3&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-5 grvsc-tuw09S-3&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;        &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-5 grvsc-tuw09S-13&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;grad_ha2 &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-15&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; np&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;multiply&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-5 grvsc-tuw09S-13&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;grad_hs2&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; h_a_stack2&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;        &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-5 grvsc-tuw09S-13&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;grad_W2 &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-15&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; np&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;outer&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-5 grvsc-tuw09S-13&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;grad_ha2&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-5 grvsc-tuw09S-13&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;h_s1&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;        &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-5 grvsc-tuw09S-13&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;grad_b2 &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-15&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-5 grvsc-tuw09S-13&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;grad_ha2&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;        &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-5 grvsc-tuw09S-13&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;grad_hs1 &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-15&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; np&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-5 grvsc-tuw09S-13&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;W_2&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;T &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-5 grvsc-tuw09S-13&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;grad_ha2&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;        h_a_stack1 &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-15&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; np&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;where&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-5 grvsc-tuw09S-13&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;h_a1 &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-15&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-5 grvsc-tuw09S-3&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-5 grvsc-tuw09S-3&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-5 grvsc-tuw09S-3&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;        &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-5 grvsc-tuw09S-13&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;grad_ha1 &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-15&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; np&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;multiply&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-5 grvsc-tuw09S-13&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;grad_hs1&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; h_a_stack1&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;        &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-3 grvsc-tZ0ymb-i grvsc-tuw09S-4&quot;&gt;# hidden layer 1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;        &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-5 grvsc-tuw09S-13&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;grad_W1 &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-15&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; np&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;outer&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-5 grvsc-tuw09S-13&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;grad_ha1&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; batch_data&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;        &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-5 grvsc-tuw09S-13&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;grad_b1 &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-15&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-5 grvsc-tuw09S-13&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;grad_ha1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After we obtained the gradients for each layer, we apply the &lt;em&gt;descent&lt;/em&gt; by subtracting it from the weights. The learning rate serves as a control, and can be designed to be adaptive according to the shape of the loss function to accelerate or decelrate the descent.&lt;/p&gt;
&lt;pre class=&quot;grvsc-container quiet-light grvsc-ps-tuw09S&quot; data-language=&quot;python&quot; data-index=&quot;4&quot;&gt;&lt;code class=&quot;grvsc-code&quot;&gt;&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-7 grvsc-tuw09S-11&quot;&gt;def&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-9 grvsc-tZ0ymb-b grvsc-tuw09S-10&quot;&gt;update_weights&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-7 grvsc-tuw09S-3&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;):&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;        &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-5 grvsc-tuw09S-13&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;W_1 &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-15&quot;&gt;-=&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-5 grvsc-tuw09S-13&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;grad_W1 &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-15&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-5 grvsc-tuw09S-13&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;learning_rate&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;        &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-5 grvsc-tuw09S-13&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;W_2 &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-15&quot;&gt;-=&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-5 grvsc-tuw09S-13&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;grad_W2 &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-15&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-5 grvsc-tuw09S-13&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;learning_rate&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;        &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-5 grvsc-tuw09S-13&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;W_3 &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-15&quot;&gt;-=&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-5 grvsc-tuw09S-13&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;grad_W3 &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-15&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-5 grvsc-tuw09S-13&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;learning_rate&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;        &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-5 grvsc-tuw09S-13&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;b_1 &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-15&quot;&gt;-=&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-5 grvsc-tuw09S-13&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;grad_b1 &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-15&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-5 grvsc-tuw09S-13&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;learning_rate&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;        &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-5 grvsc-tuw09S-13&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;b_2 &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-15&quot;&gt;-=&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-5 grvsc-tuw09S-13&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;grad_b2 &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-15&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-5 grvsc-tuw09S-13&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;learning_rate&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;        &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-5 grvsc-tuw09S-13&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;b_3 &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-15&quot;&gt;-=&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-5 grvsc-tuw09S-13&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;grad_b3 &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-15&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt; &lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-5 grvsc-tuw09S-13&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-6 grvsc-tuw09S-1&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;grvsc-tZ0ymb-1 grvsc-tuw09S-1&quot;&gt;learning_rate&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The update is performed for every single image (aka our input data) over a number of times. Each complete set is referred to as an epoch. The number of epoch is usually determined at run time using early stopping. There is no consensus on how many epochs to run, and more isn’t always better.&lt;/p&gt;
&lt;p&gt;There is probably no occasion where we will actually write everything using numpy and pretend we don’t have tensorflow or pytorch. This post aims to provide some visbility into a simple neural network training works, and why the frameworks are appreciated in the ML community.&lt;/p&gt;
&lt;style class=&quot;grvsc-styles&quot;&gt;
  .grvsc-container {
    overflow: auto;
    position: relative;
    -webkit-overflow-scrolling: touch;
    padding-top: 1rem;
    padding-top: var(--grvsc-padding-top, var(--grvsc-padding-v, 1rem));
    padding-bottom: 1rem;
    padding-bottom: var(--grvsc-padding-bottom, var(--grvsc-padding-v, 1rem));
    border-radius: 8px;
    border-radius: var(--grvsc-border-radius, 8px);
    font-feature-settings: normal;
    line-height: 1.4;
  }
  
  .grvsc-code {
    display: table;
  }
  
  .grvsc-line {
    display: table-row;
    box-sizing: border-box;
    width: 100%;
    position: relative;
  }
  
  .grvsc-line &gt; * {
    position: relative;
  }
  
  .grvsc-gutter-pad {
    display: table-cell;
    padding-left: 0.75rem;
    padding-left: calc(var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem)) / 2);
  }
  
  .grvsc-gutter {
    display: table-cell;
    -webkit-user-select: none;
    -moz-user-select: none;
    user-select: none;
  }
  
  .grvsc-gutter::before {
    content: attr(data-content);
  }
  
  .grvsc-source {
    display: table-cell;
    padding-left: 1.5rem;
    padding-left: var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem));
    padding-right: 1.5rem;
    padding-right: var(--grvsc-padding-right, var(--grvsc-padding-h, 1.5rem));
  }
  
  .grvsc-source:empty::after {
    content: &apos; &apos;;
    -webkit-user-select: none;
    -moz-user-select: none;
    user-select: none;
  }
  
  .grvsc-gutter + .grvsc-source {
    padding-left: 0.75rem;
    padding-left: calc(var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem)) / 2);
  }
  
  /* Line transformer styles */
  
  .grvsc-has-line-highlighting &gt; .grvsc-code &gt; .grvsc-line::before {
    content: &apos; &apos;;
    position: absolute;
    width: 100%;
  }
  
  .grvsc-line-diff-add::before {
    background-color: var(--grvsc-line-diff-add-background-color, rgba(0, 255, 60, 0.2));
  }
  
  .grvsc-line-diff-del::before {
    background-color: var(--grvsc-line-diff-del-background-color, rgba(255, 0, 20, 0.2));
  }
  
  .grvsc-line-number {
    padding: 0 2px;
    text-align: right;
    opacity: 0.7;
  }
  
  .quiet-light { background-color: #F5F5F5; }
  .quiet-light .grvsc-tZ0ymb-i { font-style: italic; }
  .quiet-light .grvsc-tZ0ymb-b { font-weight: bold; }
  .quiet-light .grvsc-tZ0ymb-7 { color: #7A3E9D; }
  .quiet-light .grvsc-tZ0ymb-1 { color: #333333; }
  .quiet-light .grvsc-tZ0ymb-9 { color: #AA3731; }
  .quiet-light .grvsc-tZ0ymb-6 { color: #777777; }
  .quiet-light .grvsc-tZ0ymb-4 { color: #448C27; }
  .quiet-light .grvsc-tZ0ymb-10 { color: #4B69C6; }
  .quiet-light .grvsc-tZ0ymb-5 { color: #9C5D27; }
  .quiet-light .grvsc-tZ0ymb-3 { color: #AAAAAA; }
  .quiet-light .grvsc-line-highlighted::before {
    background-color: var(--grvsc-line-highlighted-background-color, rgba(0, 0, 0, 0.05));
    box-shadow: inset var(--grvsc-line-highlighted-border-width, 4px) 0 0 0 var(--grvsc-line-highlighted-border-color, rgba(0, 0, 0, 0.2));
  }
  div#dark .grvsc-ps-tuw09S {
    background-color: #1e1e1e;
    color: #c5c8c6;
  }
  div#dark .grvsc-ps-tuw09S .grvsc-tuw09S-11 { color: #9872A2; }
  div#dark .grvsc-ps-tuw09S .grvsc-tuw09S-1 { color: #C5C8C6FF; }
  div#dark .grvsc-ps-tuw09S .grvsc-tuw09S-10 { color: #CE6700; }
  div#dark .grvsc-ps-tuw09S .grvsc-tuw09S-3 { color: #6089B4; }
  div#dark .grvsc-ps-tuw09S .grvsc-tuw09S-8 { color: #9AA83A; }
  div#dark .grvsc-ps-tuw09S .grvsc-tuw09S-15 { color: #676867; }
  div#dark .grvsc-ps-tuw09S .grvsc-tuw09S-13 { color: #C7444A; }
  div#dark .grvsc-ps-tuw09S .grvsc-tuw09S-4 { color: #9A9B99; }
  div#dark .grvsc-ps-tuw09S .grvsc-line-highlighted::before {
    background-color: var(--grvsc-line-highlighted-background-color, rgba(255, 255, 255, 0.1));
    box-shadow: inset var(--grvsc-line-highlighted-border-width, 4px) 0 0 0 var(--grvsc-line-highlighted-border-color, rgba(255, 255, 255, 0.5));
  }
&lt;/style&gt;</content:encoded></item><item><title><![CDATA[Writing Good Code vs Training More Models]]></title><description><![CDATA[Writing Good Code vs Training More Models In this project, we had relatively poor model performance. I would like to do a little…]]></description><link>https://violetguos.github.io/tree-data/</link><guid isPermaLink="false">https://violetguos.github.io/tree-data/</guid><pubDate>Thu, 02 May 2024 00:00:00 GMT</pubDate><content:encoded>&lt;h1&gt;Writing Good Code vs Training More Models&lt;/h1&gt;
&lt;p&gt;In this project, we had relatively poor model performance. I would like to do a little introspection below.&lt;/p&gt;
&lt;h1&gt;Original Project Report Introduction&lt;/h1&gt;
&lt;p&gt;In many real world settings, having access to a rich labelled data set is challenging. There are several situations in which one has a large corpus of unlabelled data, and a relatively small amount of labelled data, which is often laboriously obtained by manual effort. In order to build scalable machine learning systems, the task of labelling data should be automated. This problem broadly falls under the category of unsupervised or semi-supervised learning.&lt;/p&gt;
&lt;p&gt;In this project, there is a large store of unlabelled images of forest canopies, and a small set of labelled data. The labels indicate the ‘species’, ‘density’ and ‘height’ of the tree. The goal is to predict the species, height, and density from aerial images of forest canopies. Labelling this data set is performed by manual intervention, which proves to be expensive and time consuming. Therefore, the goal is to build an automated system to label the large corpus of unlabelled data using labelled and unlabelled data.&lt;/p&gt;
&lt;p&gt;We aim to build a baseline algorithm to perform this task. Baselines are an often ignored and critical part of a machine learning project; they provide a reference point to compare the performance of more sophisticated models. We use the k-means algorithm to cluster the large corpus of training data, and assign labels to these learned centroids by dividing the labelled data set into non overlapping sets. The high dimensional nature of the data (4096 dimensions per data point) makes this problem challenging. &lt;strong&gt;Therefore, as a pre-processing step, we use PCA (Principal Component Analysis), a dimensionality reduction algorithm, to reduce the redundancy in our data set and get better results using the above mentioned methods.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We obtain a 45.8% validation accuracy with our best model.&lt;/p&gt;
&lt;h1&gt;Analysis&lt;/h1&gt;
&lt;p&gt;In the bolded text from the original project report, you can tell that we did not try a variety of dimensionality reduection algorithms or explore other classic computer vision methods without machine learning.&lt;/p&gt;
&lt;p&gt;We ended up spending a lot of time refactoring our project code base. This was the project’s Github structure&lt;/p&gt;
&lt;pre class=&quot;grvsc-container quiet-light grvsc-ps-tuw09S&quot; data-language=&quot;&quot; data-index=&quot;0&quot;&gt;&lt;code class=&quot;grvsc-code&quot;&gt;&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;.&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;├── data&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;│   └── synthetic&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;├── evaluation&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;├── models&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;│   ├── horoma_classifier&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;│   ├── k_means&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;│   ├── most_freq_model&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;│   └── pca&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;├── notebook&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;│   └── eda&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;├── results&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;│   ├── dataset_stats&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;│   ├── plots&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;│   │   └── pca_before_after&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;│   └── tsne&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;│       ├── no_pca_no_normalization&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;│       ├── pca_100_no_normalization&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;│       ├── pca_100_with_normalization&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;│       └── pca_30_with_normalization&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;├── src&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;│   ├── algorithms&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;│   ├── datasets&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;│   ├── experiments&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;│   │   └── config&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;│   ├── scripts&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;│   ├── transforms&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;│   └── utils&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;└── test&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;    ├── algorithms&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;    ├── data&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;grvsc-line&quot;&gt;&lt;span class=&quot;grvsc-source&quot;&gt;    └── transforms&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is hard to justify how much work went into the software engineering just to run different variations of the PCA algorithm.&lt;/p&gt;
&lt;p&gt;In many cases, it is hard to balance writing high quality code and trying as many methods as possible.&lt;/p&gt;
&lt;p&gt;For more exploratory tasks, it is better to focus on modeling instead of writing production ready code.&lt;/p&gt;
&lt;style class=&quot;grvsc-styles&quot;&gt;
  .grvsc-container {
    overflow: auto;
    position: relative;
    -webkit-overflow-scrolling: touch;
    padding-top: 1rem;
    padding-top: var(--grvsc-padding-top, var(--grvsc-padding-v, 1rem));
    padding-bottom: 1rem;
    padding-bottom: var(--grvsc-padding-bottom, var(--grvsc-padding-v, 1rem));
    border-radius: 8px;
    border-radius: var(--grvsc-border-radius, 8px);
    font-feature-settings: normal;
    line-height: 1.4;
  }
  
  .grvsc-code {
    display: table;
  }
  
  .grvsc-line {
    display: table-row;
    box-sizing: border-box;
    width: 100%;
    position: relative;
  }
  
  .grvsc-line &gt; * {
    position: relative;
  }
  
  .grvsc-gutter-pad {
    display: table-cell;
    padding-left: 0.75rem;
    padding-left: calc(var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem)) / 2);
  }
  
  .grvsc-gutter {
    display: table-cell;
    -webkit-user-select: none;
    -moz-user-select: none;
    user-select: none;
  }
  
  .grvsc-gutter::before {
    content: attr(data-content);
  }
  
  .grvsc-source {
    display: table-cell;
    padding-left: 1.5rem;
    padding-left: var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem));
    padding-right: 1.5rem;
    padding-right: var(--grvsc-padding-right, var(--grvsc-padding-h, 1.5rem));
  }
  
  .grvsc-source:empty::after {
    content: &apos; &apos;;
    -webkit-user-select: none;
    -moz-user-select: none;
    user-select: none;
  }
  
  .grvsc-gutter + .grvsc-source {
    padding-left: 0.75rem;
    padding-left: calc(var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem)) / 2);
  }
  
  /* Line transformer styles */
  
  .grvsc-has-line-highlighting &gt; .grvsc-code &gt; .grvsc-line::before {
    content: &apos; &apos;;
    position: absolute;
    width: 100%;
  }
  
  .grvsc-line-diff-add::before {
    background-color: var(--grvsc-line-diff-add-background-color, rgba(0, 255, 60, 0.2));
  }
  
  .grvsc-line-diff-del::before {
    background-color: var(--grvsc-line-diff-del-background-color, rgba(255, 0, 20, 0.2));
  }
  
  .grvsc-line-number {
    padding: 0 2px;
    text-align: right;
    opacity: 0.7;
  }
  
  .quiet-light { background-color: #F5F5F5; }
  .quiet-light .grvsc-line-highlighted::before {
    background-color: var(--grvsc-line-highlighted-background-color, rgba(0, 0, 0, 0.05));
    box-shadow: inset var(--grvsc-line-highlighted-border-width, 4px) 0 0 0 var(--grvsc-line-highlighted-border-color, rgba(0, 0, 0, 0.2));
  }
  div#dark .grvsc-ps-tuw09S {
    background-color: #1e1e1e;
    color: #c5c8c6;
  }
  div#dark .grvsc-ps-tuw09S .grvsc-line-highlighted::before {
    background-color: var(--grvsc-line-highlighted-background-color, rgba(255, 255, 255, 0.1));
    box-shadow: inset var(--grvsc-line-highlighted-border-width, 4px) 0 0 0 var(--grvsc-line-highlighted-border-color, rgba(255, 255, 255, 0.5));
  }
&lt;/style&gt;</content:encoded></item><item><title><![CDATA[Fact or Fake News: Applying NLP in an unbalanced dataset]]></title><description><![CDATA[Author’s Note I wanted to reflect on this past project. My team decided to join an existing machine learning competition where the source…]]></description><link>https://violetguos.github.io/news-nlp/</link><guid isPermaLink="false">https://violetguos.github.io/news-nlp/</guid><pubDate>Wed, 01 May 2024 00:00:00 GMT</pubDate><content:encoded>&lt;h1&gt;Author’s Note&lt;/h1&gt;
&lt;p&gt;I wanted to reflect on this past project. My team decided to join an existing machine learning competition where the source data and a base line model was provided to us. The following is an excerpt from an original report written by Étienne Girard-Proulx, Violet Guo, and Matthew Lesko-Krleza (in alphabetical order). I have omitted the citations and some diagrams, but they are available upon request.&lt;/p&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;For better or for worse, social media has had a significant impact on content and information distribution.
Since anyone has the ability to post anything at anytime, there comes the cost of an increase in the spread of fake news articles and claims.
Detecting whether an article is promoting fake news or not is difficult since the claim’s goal is often to mislead readers to some false conclusion.
Therefore, being able to fact check claims by analyzing relevant articles can be beneficial as it would allow systems to promote true and relevant news.
Ideally, this would reduce public exposure to fake news and deter its spread.
In this project, we examine this challenge and evaluate how natural language processing (NLP) models can be used for the task of fake news classification.
Our goal is to train a variety of linear, ensemble, and neural network models and use pre-processing techniques that we’ve seen in class to achieve high classification accuracy.
By training and evaluating models on a news dataset provided by the 2019 online competition `Leaders Prize: Fact or Fake News’, we demonstrate several NLP models’ effectiveness at classifying false, partly true, and true news.
We show that our methods can successfully detect fake news but would require more work in true news identification.&lt;/p&gt;
&lt;h1&gt;Data Description&lt;/h1&gt;
&lt;p&gt;The source of our labelled dataset is from the 2019 online competition called `Leaders Prize: Fact or Fake News?’ \cite{DataCup}.
The competition involves the task of classifying news claim within its provided dataset as either: False, Partly True, or True.
The dataset is comprised of labelled claims (i.e. statements), claimants, and related article content. Claimants are related to claims, however not all claims have a claimant.
The data contains a total of 15,555 claims, and 64,974 related articles.
Therefore, there is a total of 15,555 examples to train and evaluate our models with.
The competition has a hidden test set that can only be evaluated on when making a submission.
We did not make use of this test set because the competition submission closed before we had the chance to make any.&lt;/p&gt;
&lt;h2&gt;Data Imbalance&lt;/h2&gt;
&lt;p&gt;Upon further analysis of our data, we noticed a severe class imbalance as illustrated in Figure \ref{fig:label-hist}.
The majority of our data is of False and Partly True claims.
More precisely our examples are labelled: 48% False, 41% Partly True and 11% True.
We believe if this imbalance is not addressed, we expect to see a significantly low recall within our results.
We discuss a method to circumvent this imbalance in Section 4.7.&lt;/p&gt;
&lt;h1&gt;4 Methodology&lt;/h1&gt;
&lt;p&gt;Here we explain the different pre-processing techniques, feature extraction methods and models that we used and evaluated.
For this project, we are using the Python 3 programming language and several external Python packages: Pandas, SciPy, Matplotlib, Tqdm, Scikit-Learn, Tensorflow, Keras, PyTorch and XGBoost.&lt;/p&gt;
&lt;h2&gt;4.1 Data Split&lt;/h2&gt;
&lt;p&gt;Since the competition’s test set is hidden from us, we made use of splitting the training data that was provided.
The split is a ratio of 70%, 15% and 15% for the training, development and test sets respectively.&lt;/p&gt;
&lt;h2&gt;4.2 Data Pre-processing&lt;/h2&gt;
&lt;p&gt;Next we describe the techniques we’ve used for pre-processing our data. The methods used are the following: square bracket removal, non-ASCII character removal, punctuation removal, character lowercasing, numerical character replacement, stemming and lemmatization.&lt;/p&gt;
&lt;p&gt;These techniques are applied on claims and related article content. The majority of claimants are proper nouns, so we didn’t apply pre-processing on claimants. We applied non-ASCII character removal to clean our corpus from unordinary characters. Character lowercasing is applied because we don’t want our models to differentiate between words of the same spelling but different capitalization. Stemming and lemmatization are both performed to make our feature space dimensionally lower, with the intent of making our models faster to train and more generalizable.&lt;/p&gt;
&lt;h2&gt;4.3 Training and Evaluation&lt;/h2&gt;
&lt;p&gt;We tuned our models using the training and development sets to determine optimal hyperparameter configurations.
Then we trained the optimally parameterized model on the training and validation sets, and finally evaluated it on the held-out test set.
By doing so, we can empirically determine optimal hyperparameter configurations by detecting overfitting and use the test set to have an unbias evaluation of our models.&lt;/p&gt;
&lt;h2&gt;4.4 Most Frequent Label Classifier Baseline&lt;/h2&gt;
&lt;p&gt;For comparison purposes, we have tested a Most Frequent Label Classifier baseline.
Given a set of test examples, the predictor classifies each example to that of the most frequently occurring label.
This was implemented using SciKit-Learn’s DummyClassifier application programming interface (API) .
This is not meant to be used as an effective predictor, yet a critical baseline for an unbalanced dataset.&lt;/p&gt;
&lt;h2&gt;4.5 Linear Model Baselines&lt;/h2&gt;
&lt;p&gt;We trained and evaluated three linear models as baselines to compare with more complex models that we eventually train and evaluate.&lt;/p&gt;
&lt;p&gt;We trained and evaluated three different multi-class linear models: Logistic Regression, Support Vector Machine (SVM) and Naive Bayes.
Logistic Regression and SVM are discriminative models whereas Naive Bayes is a generative model.
Generative models estimate the joint distribution of features and labels whereas discriminative models estimate the posterior probability of some label given its features.
The difference between Logistic Regression and SVM is the different objective functions used at training.
They both estimate an hyperplane that can separate the training labels’ categories.
The subtle difference is that SVMs attempt to maximize the margin (i.e. distance) between its estimated hyperplane and the observations from each class \cite{svm} whereas Logistic Regression performs Maximum Likelihood Estimation (MLE) to maximize the likelihood that a random point gets classified correctly.
The models are trained on Term Frequency-Inverse Document Frequency (TF-IDF) representations of the concatenation of the claim, claimant, and related article content.&lt;/p&gt;
&lt;p&gt;We fine tuned each linear models’ parameters with Grid Cross Validation of Fold 3.
The model configurations, validation F1 scores, and test F1 scores are available within Table \ref{table:fine-tune-grid-cv}.
We trained, fine-tuned and evaluated these models using SciKit-Learn’s relevant predictor, Grid Cross Validation and metric APIs.&lt;/p&gt;
&lt;h2&gt;4.6 Ensemble Models: Random Forest and Extreme Gradient Boosting (XGBoost)&lt;/h2&gt;
&lt;p&gt;Ensemble models are popularly used in these sorts of classification challenges and are often considered the state-of-the-art solution.&lt;/p&gt;
&lt;p&gt;The main premise is that by combining multiple models, the errors of a single base-learner will likely be compensated by other base-learners, as a result the overall prediction performance would be better than that of a single model.&lt;/p&gt;
&lt;p&gt;A Random Forest is decision-tree-based ensemble model.
There are two main components to this ensemble.
The first is that each tree is trained on a random sample from the training data.
The second is that a random subset of features are selected to generate a split for each node in a tree.&lt;/p&gt;
&lt;p&gt;Similarly to a Random Forest, XGBoost is a decision-tree-based ensemble model, however it uses a gradient boosting framework.
XGBoost trains an ensemble of base-learners and boosts weak learners using gradient descent.
It also uses some system optimizations and algorithmic enhancements to make it faster to train compared to its sister model: Gradient Boosting Machine (GBM).&lt;/p&gt;
&lt;p&gt;Similarly to the linear models, we fine tuned the ensemble models using a Grid Cross Validation of Fold 3.
Their results are available within Table 3. The models are trained on TF-IDF representations of the concatenation of the claim, claimant, and related article content.
We used SciKit-Learn’s and XGBoost’s relevant predictor APIs.&lt;/p&gt;
&lt;h2&gt;4.7 Fully Connected MLP Neural Network Baseline&lt;/h2&gt;
&lt;p&gt;We re-implement the simple fully connected neural network, or multi-layer perceptron (MLP) using PyTorch, in order to verify whether their method would transfer to our fake news dataset.&lt;/p&gt;
&lt;p&gt;The input layer samples from our customized PyTorch &lt;code&gt;Dataset&lt;/code&gt; class, which concatenates a 5,000 dimensional TF vector from claim, and another 5,000 dimensional TF vector from related texts, and a cosine similarity score of the TF-IDF vectors of the two, according to \texAtcite{DBLP:journals/corr/RiedelSR17}‘s hyperparameters. The paper’s architecture only yields around 35% validation accuracy.&lt;/p&gt;
&lt;p&gt;Upon examining the TF and TF-IDF vectors, only around 500 to 1,000 are non-zero. We then decide to limit the dimensions to 500 in the TF and TF-IDF functions. Then, we apply principal component analysis (PCA) to the 500 dimensional TF vectors, and obtain 64 principle components from claim and text.&lt;/p&gt;
&lt;p&gt;In order to address the data imbalance problem, we further modify a plain PyTorch &lt;code&gt;Dataset&lt;/code&gt; using a &lt;code&gt;WeightedRandomSampler&lt;/code&gt;. This ensures that each batch samples the rare class with a higher probability, and each batch contains approximately equal number of samples from each class.&lt;/p&gt;
&lt;p&gt;We also experiment with various loss metrics utilized by computer vision experiments in handling data imbalance, such as dice loss \cite{Wong_2018} and focal loss \cite{lin2017focal}. However, the metrics that are suitable for vision tasks do not transfer to NLP applications. Binary cross entropy loss is the best out of all metrics we tried. Weighted batch sampling and binary cross entropy loss improve our validation set unweighted F1 score from 35% to 45%.&lt;/p&gt;
&lt;h2&gt;4.7 Sequential Neural Network Model: Bidirectional Long Short-Term Memory (Bi-LSTM)&lt;/h2&gt;
&lt;p&gt;The final model we test is a custom neural network model inspired from the Bi-LSTM architecture.
The idea behind this model is to leverage simpler model for metadata features and use more complex model such as LSTM to extract relevant information from more complex features like claim and related articles.&lt;/p&gt;
&lt;p&gt;Metadata features such as the claimant, the number of articles supporting the claim are features already in a format that can be passed to the model and are passed directly to the model via a standard MLP. For the claim and related articles, we first preprocess the text like described in section data-preprocessing, then convert the preprocessed text to a fixed length vector using the text to sequence and padding implementations of Keras.
Then, we pass both the claim resulting vector and the related article resulting vector to two embedding layers. The goal of using embedding layers is convert sparse vector to dense vector. Finally, we concatenate the two dense vectors and pass the resulting concatenated vector to a bi-lstm model.
To obtain a final prediction, we concatenate the output of the bi-lstm and of the MLP and pass everything through a softmax layer. Model architecture is available upon request.&lt;/p&gt;
&lt;p&gt;We expect that this model, if trained properly, should be the most performing one due to the sequential nature of the model. To train the model, we used the Nadam optimizer. This optimizer is a mix between two well known optimizers: Nesterov and Adam.&lt;/p&gt;
&lt;h1&gt;Results&lt;/h1&gt;
&lt;p&gt;To evaluate our models’ performance, we are concerned with Weighted Average F1 and Recall for the `True’ class. We are particularly interested by the Recall of the True category since it tells us how much of the truthful news our system was able to detect within the dataset.
If some system like this were to be deployed, we would want it to be able to detect ‘True’ news from ‘False’ ones with a high success rate.&lt;br&gt;
We would not simply censor all news because it was incorrectly deemed as ‘False.’
A more detailed look at precision, recall and F1 scores are available in the results table.
It is also the measure that highlights a lack of performance in handling an unbalanced dataset.&lt;/p&gt;
&lt;p&gt;Next, we compare our different methods and discuss potential reasons behind their performance.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th&gt;Weighted Average F1 score (%)&lt;/th&gt;
&lt;th&gt;Recall of True Class (%)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Most Frequent Classifier&lt;/td&gt;
&lt;td&gt;31.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Logistic Regression&lt;/td&gt;
&lt;td&gt;58.0&lt;/td&gt;
&lt;td&gt;1.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Linear SVM&lt;/td&gt;
&lt;td&gt;57.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Naive Bayes&lt;/td&gt;
&lt;td&gt;53.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Random Forest&lt;/td&gt;
&lt;td&gt;60.0&lt;/td&gt;
&lt;td&gt;2.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;XGBoost&lt;/td&gt;
&lt;td&gt;56.0&lt;/td&gt;
&lt;td&gt;1.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;MLP&lt;/td&gt;
&lt;td&gt;40.1&lt;/td&gt;
&lt;td&gt;17.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Bi-LSTM Custom&lt;/td&gt;
&lt;td&gt;60.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Bi-LSTM Rebalanced&lt;/td&gt;
&lt;td&gt;57.0&lt;/td&gt;
&lt;td&gt;5.0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h1&gt;Author’s Reflection&lt;/h1&gt;
&lt;p&gt;This project was graded by 2 TAs and our professor. They had very different interpretations of this project.
I want to pause here and jot down some of my own reflections.&lt;/p&gt;
&lt;h2&gt;Writing&lt;/h2&gt;
&lt;p&gt;In my opinion, we spent too much time explaining how the various machine learning models work. The explanations should be reduced by 30%.&lt;/p&gt;
&lt;h2&gt;Lack of NLP domain expertise&lt;/h2&gt;
&lt;p&gt;The project should spend more time in the data preprocessing and exploration stage. For NLP, there are many old-school NLP algorithms that would extract word level, sentence leve, paragraph level, and corpus level features.&lt;/p&gt;
&lt;p&gt;The data contained the news article, as well as the source. It is highly likely that the source itself is a more influential indicator of the reliability of the claim.&lt;/p&gt;
&lt;h2&gt;Attempted too many ML models&lt;/h2&gt;
&lt;p&gt;For someone with more computational linguistics backgorund, this project could sound like a bookkeeping record of different machine learning models we tried. Some models, such as Bi-LSTM Rebalanced, does not provide much more advantage over other methods we have tried. More isn’t always better.&lt;/p&gt;
&lt;style class=&quot;grvsc-styles&quot;&gt;
  .grvsc-container {
    overflow: auto;
    position: relative;
    -webkit-overflow-scrolling: touch;
    padding-top: 1rem;
    padding-top: var(--grvsc-padding-top, var(--grvsc-padding-v, 1rem));
    padding-bottom: 1rem;
    padding-bottom: var(--grvsc-padding-bottom, var(--grvsc-padding-v, 1rem));
    border-radius: 8px;
    border-radius: var(--grvsc-border-radius, 8px);
    font-feature-settings: normal;
    line-height: 1.4;
  }
  
  .grvsc-code {
    display: table;
  }
  
  .grvsc-line {
    display: table-row;
    box-sizing: border-box;
    width: 100%;
    position: relative;
  }
  
  .grvsc-line &gt; * {
    position: relative;
  }
  
  .grvsc-gutter-pad {
    display: table-cell;
    padding-left: 0.75rem;
    padding-left: calc(var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem)) / 2);
  }
  
  .grvsc-gutter {
    display: table-cell;
    -webkit-user-select: none;
    -moz-user-select: none;
    user-select: none;
  }
  
  .grvsc-gutter::before {
    content: attr(data-content);
  }
  
  .grvsc-source {
    display: table-cell;
    padding-left: 1.5rem;
    padding-left: var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem));
    padding-right: 1.5rem;
    padding-right: var(--grvsc-padding-right, var(--grvsc-padding-h, 1.5rem));
  }
  
  .grvsc-source:empty::after {
    content: &apos; &apos;;
    -webkit-user-select: none;
    -moz-user-select: none;
    user-select: none;
  }
  
  .grvsc-gutter + .grvsc-source {
    padding-left: 0.75rem;
    padding-left: calc(var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem)) / 2);
  }
  
  /* Line transformer styles */
  
  .grvsc-has-line-highlighting &gt; .grvsc-code &gt; .grvsc-line::before {
    content: &apos; &apos;;
    position: absolute;
    width: 100%;
  }
  
  .grvsc-line-diff-add::before {
    background-color: var(--grvsc-line-diff-add-background-color, rgba(0, 255, 60, 0.2));
  }
  
  .grvsc-line-diff-del::before {
    background-color: var(--grvsc-line-diff-del-background-color, rgba(255, 0, 20, 0.2));
  }
  
  .grvsc-line-number {
    padding: 0 2px;
    text-align: right;
    opacity: 0.7;
  }
  
&lt;/style&gt;</content:encoded></item><item><title><![CDATA[Predicting ECG Signal from Unlabelled Data]]></title><description><![CDATA[Introduction The following project is a classic example on why complicated and novel methods aren’t always the best. OMsignal is a company…]]></description><link>https://violetguos.github.io/ecg-signal/</link><guid isPermaLink="false">https://violetguos.github.io/ecg-signal/</guid><pubDate>Tue, 26 Sep 2023 00:00:00 GMT</pubDate><content:encoded>&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;The following project is a classic example on why complicated and novel methods aren’t always the best.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;OMsignal is a company specializing in analytics on electrocardiogram (ECG) signals. The project requires using raw ECG signal to predict the mean PR interval, the mean RT interval, the standard deviation of the RR interval, and to identify the user. This multitask problem was approached as a supervised learning problem in block 1, using only labeled data. Results show that convolutional neural networks performed better than regular multilayers perceptrons or even recurrent neural networks. Block 2 shifts the focus from supervised to semi-supervised learning, with the addition of around 600k unlabeled samples. While the multitask objective remains essentially the same, the new samples contain precious information relative to the representation of the data. The purpose of this block is to extract this information from the unlabeled samples in order to improve the training of predictive models and increase their performances on the test set.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1&gt;Methodology&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;In block 1, data preprocessing such as normalization and noise removal were tried. In addition to data preprocessing, data augmentation such as negation of signal sections, temporal shift of the signal and partial noise addition were also tried. We decided to keep these because block 1 reports show significant improvement of the performances while using those methods. In addition to this, we converted the data into frequency space domain using Fast Fourier Transforms (FFT) to use as an additional/alternative input. Data loading functions from block 1 were adapted in order to process unlabeled samples.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Unsupervised Pretraining&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;As a form of unsupervised pre-training, we implemented an autoencoder model that learns a latent representation of the data. Autoencoders contain an encoder and decoder. Encoders learn an intermediate representation of the dataset through dimensionality reduction, while decoders try to reconstruct the input with minimal error. Where classification models tend to overfit on small labeled datasets, studies show that autoencoders can increase generalization and stability (\cite{DBLP:journals/corr/abs-1807-11407}). We used an autoencoder to learn a latent representation of the unlabeled data which is then passed to a prediction module.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;The first model we have tried with that approach was to use an autoencoder with a single hidden layer neural network for both the encoder and the decoder. For the prediction module, we used the same CNN that was used as the baseline model for the block 1. Hence, the prediction module is a multi-layer CNN with batch-normalization for 1D inputs. Its also a multi-task network that can be trained on any combination of targets.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Semi-supervised Training: Class 1 model&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;In class 1 models, a branch is added to the network to handle the unlabeled data. Similarily to the unsupervised pretraining approach, encoders try to learn a low-dimensional representation that helps generalization and reduce overfitting. The major difference is that, instead of first training the autoencoder and then the prediction module, in this approach we train the two simultaneously by feeding a mix of labeled and unlabeled data. This allows the training of the low-dimensional representation to be not only driven by the unlabeled data. The basic architecture of a class 1 model is shown in Figure \ref{class 1 architecture}&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;Vanilla Class 1 Semi-Supervised Model&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;As our first class 1 model, we trained the same multi-layer CNN as before, but we added a branch reconstructing the input when the sample fed is unlabeled. Our encoder here uses the convolutional layers of the CNN, and depending on if the data is labeled or not, the processed sample is passed either to a two-hidden-layer MLP that gives a prediction or to a decoder that reconstructs the input. In both cases, this is followed by back-propagation to update the weights.
We’ve attempted this with two types of decoder. First, we tried an MLP. We decided to test MLP both to stay closer to the prediction module architecture (also an MLP), and to make sure the hidden representation of our inputs works well with MLP. Second, we built a convolutional autoencoder. Here, the encoder consists of convolutional layers and the decoder of deconvolutional layers. The decoder reconstructs the input with transpose convolution operations. The decoder architecture mirrors the encoder, replacing each &lt;code&gt;nn.Conv1d&lt;/code&gt; with &lt;code&gt;nn.TransposeConv1d&lt;/code&gt; and &lt;code&gt;nn.Maxpool1d&lt;/code&gt; with &lt;code&gt;nn.MaxUnpool1d&lt;/code&gt;. The architecture of the convolutional autoencoder is shown in figure \ref{conv-ae-arch} of the annex.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;Ladder Network&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;The ladder network is a semi-supervised algorithm that couples supervised loss and unsupervised loss to train both the encoder and decoder. The term ladder refers to this coupling mechanism. The ladder network follows a conventional autoencoder principle, except for the following changes:&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Adds a Gaussian noise to the inputs for decoders. Forces the decoder to learn to reconstruct the output from a noisy input.&lt;/li&gt;
&lt;li&gt;Weighs each decoder layer’s reconstruction error. Usually the deeper encoders are weighted higher because they are closer to the final output prediction layer.&lt;/li&gt;
&lt;li&gt;For the gradient path, it combines both classification layer’s cross entropy loss and each decoder layer’s reconstruction loss to back propagate. Ideally, through this combination scheme, a single optimizer should be able to find the optimal weights for both tasks.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;However, one major downside of the ladder network in this specific project is that it has not been proven to work on regression tasks. Neither the original paper (\cite{DBLP:journals/corr/RasmusVHBR15}) nor others (\cite{DBLP:journals/corr/PezeshkiFBCB15}) present any experiment on regression tasks. This limits the network’s scope to the user_id classification only.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;Semi-supervised Training: Class 2 model&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;The “Mean Teacher” model, developed by the Curious AI Company [\cite{DBLP:journals/corr/TarvainenV17}], is a consistency regularization method. This semi-supervised training method consists of using two instances of an architecture: the Student model and the Teacher model. Training through gradient descent is only performed on the Student model. The parameters for the Teacher model are obtained via an exponential moving average (EMA) of the actual and previous weights and biases of the Student model. During training, the sample fed to the models is noised differently in order to create a slightly different version of it for each model. We used all data pre-processing and augmentation methods described in section 2.1. Both the Student and Teacher models make predictions for these samples. The final loss function combines two terms: a classification cost between the Student model prediction and the actual label of the sample (if the sample originally has a label) and a consistency cost between the Student and Teacher model prediction (for both labeled and unlabeled). The structure is described in Figure \ref{mean&lt;em&gt;teacher&lt;/em&gt;struc} in Appendix. For Student and Teacher model we opted for a 2-layer convolutionnal neural network using max pooling and dropout, based on the performances from previous block teams. Further details about the architecture can be found in the repository.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;We decided to implement this model because of the promising results it shown in semi-supervised learning and its novelty. The measure of its performance can be found in the orignal paper by Tarvainen. The EMA weight sharing between the Student and the Teacher model allow a transfer of information that gives the Teacher a better intermediate representation of the sample, and it has been shown that averaging model weights over training steps can produce more accurate results than using final weights directly (Polyak et al.[1992]). Source code for this model was taken from Curious AI’s Github repository (\url{&lt;a href=&quot;https://github.com/CuriousAI/mean-teacher%7D&quot;&gt;https://github.com/CuriousAI/mean-teacher}&lt;/a&gt;). The code can be found in the legacy folder of our project. All modified files and functions were copied into our project first in order to keep the legacy files intact.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1&gt;Model Results&lt;/h1&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th&gt;Overall score&lt;/th&gt;
&lt;th&gt;prMeanTau&lt;/th&gt;
&lt;th&gt;rtMeanTau&lt;/th&gt;
&lt;th&gt;rrStdDevTau&lt;/th&gt;
&lt;th&gt;userIdAcc&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Baseline Block1&lt;/td&gt;
&lt;td&gt;0.403&lt;/td&gt;
&lt;td&gt;0.591&lt;/td&gt;
&lt;td&gt;0.761&lt;/td&gt;
&lt;td&gt;0.328&lt;/td&gt;
&lt;td&gt;0.181&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Unsupervised Pretraining&lt;/td&gt;
&lt;td&gt;0.000&lt;/td&gt;
&lt;td&gt;0.000&lt;/td&gt;
&lt;td&gt;0.219&lt;/td&gt;
&lt;td&gt;0.079&lt;/td&gt;
&lt;td&gt;0.012&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Class 1 MLP autodencoder&lt;/td&gt;
&lt;td&gt;0.410&lt;/td&gt;
&lt;td&gt;0.658&lt;/td&gt;
&lt;td&gt;0.766&lt;/td&gt;
&lt;td&gt;0.321&lt;/td&gt;
&lt;td&gt;0.174&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Mean teacher&lt;/td&gt;
&lt;td&gt;0.228&lt;/td&gt;
&lt;td&gt;0.23&lt;/td&gt;
&lt;td&gt;0.26&lt;/td&gt;
&lt;td&gt;0.25&lt;/td&gt;
&lt;td&gt;0.18&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Ladder Network&lt;/td&gt;
&lt;td&gt;NA&lt;/td&gt;
&lt;td&gt;NA&lt;/td&gt;
&lt;td&gt;NA&lt;/td&gt;
&lt;td&gt;NA&lt;/td&gt;
&lt;td&gt;0.05&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Class 1 CONVae&lt;/td&gt;
&lt;td&gt;NA&lt;/td&gt;
&lt;td&gt;NA&lt;/td&gt;
&lt;td&gt;NA&lt;/td&gt;
&lt;td&gt;NA&lt;/td&gt;
&lt;td&gt;0.131&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h1&gt;Reflection&lt;/h1&gt;
&lt;p&gt;Every team that tried deep learning models got similar results, with the user prediction accuracy around 20%. Only one team was able to obtain nearly 98% accuracy. They did so with a simple model inspired by a medical journal, involving handcrafted features from medical researchers who specialized in ECG signals. Obviously, the feature based model was also much faster to train compared to neural network based models.&lt;/p&gt;
&lt;p&gt;This is a classic example where deep learning is not always superior. As machine learning engineers working in industry, it is extremely important to take domain expertise and constraints into consideration.&lt;/p&gt;
&lt;p&gt;The quotes are excerpts from an original report written by Étienne Girard-Proulx, Violet Guo, and Jean-Philippe Letendre (in alphabetical order).&lt;/p&gt;
&lt;style class=&quot;grvsc-styles&quot;&gt;
  .grvsc-container {
    overflow: auto;
    position: relative;
    -webkit-overflow-scrolling: touch;
    padding-top: 1rem;
    padding-top: var(--grvsc-padding-top, var(--grvsc-padding-v, 1rem));
    padding-bottom: 1rem;
    padding-bottom: var(--grvsc-padding-bottom, var(--grvsc-padding-v, 1rem));
    border-radius: 8px;
    border-radius: var(--grvsc-border-radius, 8px);
    font-feature-settings: normal;
    line-height: 1.4;
  }
  
  .grvsc-code {
    display: table;
  }
  
  .grvsc-line {
    display: table-row;
    box-sizing: border-box;
    width: 100%;
    position: relative;
  }
  
  .grvsc-line &gt; * {
    position: relative;
  }
  
  .grvsc-gutter-pad {
    display: table-cell;
    padding-left: 0.75rem;
    padding-left: calc(var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem)) / 2);
  }
  
  .grvsc-gutter {
    display: table-cell;
    -webkit-user-select: none;
    -moz-user-select: none;
    user-select: none;
  }
  
  .grvsc-gutter::before {
    content: attr(data-content);
  }
  
  .grvsc-source {
    display: table-cell;
    padding-left: 1.5rem;
    padding-left: var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem));
    padding-right: 1.5rem;
    padding-right: var(--grvsc-padding-right, var(--grvsc-padding-h, 1.5rem));
  }
  
  .grvsc-source:empty::after {
    content: &apos; &apos;;
    -webkit-user-select: none;
    -moz-user-select: none;
    user-select: none;
  }
  
  .grvsc-gutter + .grvsc-source {
    padding-left: 0.75rem;
    padding-left: calc(var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem)) / 2);
  }
  
  /* Line transformer styles */
  
  .grvsc-has-line-highlighting &gt; .grvsc-code &gt; .grvsc-line::before {
    content: &apos; &apos;;
    position: absolute;
    width: 100%;
  }
  
  .grvsc-line-diff-add::before {
    background-color: var(--grvsc-line-diff-add-background-color, rgba(0, 255, 60, 0.2));
  }
  
  .grvsc-line-diff-del::before {
    background-color: var(--grvsc-line-diff-del-background-color, rgba(255, 0, 20, 0.2));
  }
  
  .grvsc-line-number {
    padding: 0 2px;
    text-align: right;
    opacity: 0.7;
  }
  
&lt;/style&gt;</content:encoded></item><item><title><![CDATA[Identify Street Numbers in Realistic Environments]]></title><description><![CDATA[Introduction In order to alleviate the hardships of people with vision loss when navigating through a city, the Humanware project aims to…]]></description><link>https://violetguos.github.io/street-number/</link><guid isPermaLink="false">https://violetguos.github.io/street-number/</guid><pubDate>Wed, 26 Apr 2023 00:00:00 GMT</pubDate><content:encoded>&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;In order to alleviate the hardships of people with vision loss when navigating through a city, the Humanware project aims to create an application to identify text and numbers in real time using the camera of a phone. In the previous two blocks, the open source dataset that was used provides a ground truth bounding box of the address number, therefore previous blocks always have access to the digit’s location. However, in order to adapt the application for deployment, we need to consider that street numbers are embedded on many different backgrounds, and the extraction of digit location must be done before digit classification.&lt;/p&gt;
&lt;p&gt;For the third block of the project, we tried to automate the generation of boundary boxes around a zone of interest containing address numbers. To obtain the bounding boxes, we compared two image segmentation models using the implementation of &lt;em&gt;Faster RCNN&lt;/em&gt; [7] algorithms found in the &lt;em&gt;Facebook&lt;/em&gt; repository[4]. To evaluate the usability of our models as bounding box generators, we used an existing digit recognition model (ResNet [2] from Bounding Box) and compared its accuracy when using the true boxes and our predicted boxes.&lt;/p&gt;
&lt;p&gt;Our final model is an end-to-end task that uses Faster-RCNN to find the bounding box of an address in an image and feeds it to a ResNet model from Bounding Box for digit recognition. More details about the previous model are given in Section 3.3. We obtained a mean Average Precision of 89.6% with Faster-RCNN, 70.0% validation accuracy on Bounding Box ResNet [2] model using ground-truth boxes, and 70.3% validation accuracy when combining the generated bounding boxes to the ResNet model.&lt;/p&gt;
&lt;p&gt;The following report is split as follows. Section 2 presents the previous and current datasets and their format. Section 3 includes the methodology and describes the used algorithms and their hyperparameters for detection and sequence recognition. Section 4 presents the final results we obtained with each model and an analysis comparing them to each other and previous results. Section 5 explores new approaches that could enhance the results of our models.&lt;/p&gt;
&lt;h1&gt;Dataset&lt;/h1&gt;
&lt;p&gt;In this section, we briefly introduce the SVHN dataset used by Bounding Box and present the synthetic dataset we use in this block.&lt;/p&gt;
&lt;h2&gt;Street View House Numbers Dataset (SVHN)&lt;/h2&gt;
&lt;p&gt;The SVHN [5] Dataset contains over 600k images of house numbers obtained from Google Street View. Each image is cropped to focus on the house numbers and individual bounding boxes are given for each digit. This dataset is limited by the lack of negative examples (e.g. no digits present, blocked digits) and lack of background (images are too zoomed in).&lt;/p&gt;
&lt;h2&gt;Element AI Synthetic dataset&lt;/h2&gt;
&lt;p&gt;We aim to improve the robustness of Bounding Box models with the synthetic dataset and eliminate the limitations of SVHN. The dataset we used to train the segmentation task is composed of 6000 synthetic images containing a rendering of houses and a house number. The dataset is split in a 5:1 &lt;em&gt;train to validation&lt;/em&gt; ratio and contains different angles and viewpoints that are not present in SVHN, allowing for a better representation of real-world applications. However, this dataset has new flaws that were not seen in SVHN:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The renders are not photorealistic.&lt;/li&gt;
&lt;li&gt;The dataset is orders of magnitude smaller than SVHN&lt;/li&gt;
&lt;li&gt;There are only a few different house models.&lt;/li&gt;
&lt;li&gt;The house numbers are only three or four digit long.&lt;/li&gt;
&lt;li&gt;There is only one font used for the house numbers.&lt;/li&gt;
&lt;li&gt;Some digits are hidden by objects (&lt;em&gt;Figure 1a&lt;/em&gt;) and are impossible to classify.&lt;/li&gt;
&lt;li&gt;Some images contain cropped digits (&lt;em&gt;Figure 1b&lt;/em&gt;) or no address at all (&lt;em&gt;Figure 1c&lt;/em&gt;) and are impossible to classify.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These limitations will affect our models if they were to be used on real world images and the accuracy should drop. However, the dataset is adequate for a proof of concept before translating and adapting the work to real data.&lt;/p&gt;
&lt;h1&gt;Algorithms and Methodology&lt;/h1&gt;
&lt;p&gt;In Bounding Box modoeling, the goal was to predict the length of each sequence as well as transcribing digits correctly while the ground truth bounding boxes of address numbers were provided. In the third block, the bounding boxes are not provided, so our first task is to generate the 2 bounding boxes by an object detection model and then send the generated boxes to the sequence digit recognition model in Bounding Box.&lt;/p&gt;
&lt;p&gt;Object detection is the process of finding and classifying different objects in an image. To do this, we use the architecture named Faster RCNN described in Subsection 3.2.&lt;/p&gt;
&lt;p&gt;After obtaining the location of the digit sequence, Bounding Box model is used to classify digit sequences in the sub-region of an original image cropped with bounding boxes. Figure 2 shows the architecture of combining two models . We have described the Bounding Box (ResNet) model in the subsection following Faster-RCNN.&lt;/p&gt;
&lt;h2&gt;Data Preparation&lt;/h2&gt;
&lt;p&gt;In Bounding Box, since true bounding boxes are given, data preprocessing primarily involves using PyTorch’s transform library to add small perturbations on the fly. The Bounding Box methods include random cropping and border padding. In Block 3, however, the whole Faster-RCNN is essentially a data preprocessing step that produces the bounding box predictions. After obtaining the bounding box predictions, we apply the same random cropping and padding to segmented images. Bounding Box has demonstrated that random cropping reduces overfitting, while border padding helps with convolutions where digits get cut off by bounding boxes.&lt;/p&gt;
&lt;h2&gt;Faster RCNN overview&lt;/h2&gt;
&lt;p&gt;In Block 3, we are instructed to use Faster RCNN and adapt it to our data set. The goal of using Faster RCNN is to identify the digit patch inside a larger image with other background objects.&lt;/p&gt;
&lt;p&gt;Faster RCNN couples a stable convolutional neural network (CNN) model, such as ResNet [7], and the Regional of Interest (ROI) Proposal Net (RPN) to achieve efficient and accurate results in object detection. Figure 3 demonstrates how we adapt Faster-RCNN for ElementAI dataset. The code is implemented in Pytorch by Facebook AI developers and the teaching staff has kindly recommended a stable build for us to use out-of-the-box with Element AI dataset. Faster-RCNN is built on top of C++ backend and due to constraints on the Calcul Quebec cluster, we only retrain the model’s output layers and modify the dataloader for this specific ElementAI dataset. For this project, we tried two different Faster RCNN architectures: R-50-FPN and R-101-FPN.&lt;/p&gt;
&lt;p&gt;The metric used to compare our bounding boxes to the given boxes for each image is the IoU, or Intersection over Union. This metric first takes the ratio between the area contained in both boxes and the area contained in either box. This ratio is then compared to a threshold between 0 and 1. If the IoU is over the threshold, the predicted box is considered a correct box. The metric used to evaluate the Faster-RCNN models is the mean Average Precision (mAP)[4]. The mAP is the average of the precision values for IoU thresholds from 0.5 to 0.95 (with a step of 0.05).&lt;/p&gt;
&lt;p&gt;&lt;em&gt;IoU&lt;/em&gt; =area of overlap / area of union&lt;/p&gt;
&lt;p&gt;$mAP = \frac{1}{10}\sum_{j=1}^{10}AP(IoU = 0.45+0.05 \times j)$&lt;/p&gt;
&lt;h2&gt;Bounding Box Model&lt;/h2&gt;
&lt;p&gt;ResNet is a type of convolutional neural network with skipping layer residual connections that has proven to excel in ImageNet competitions and other various computer vision tasks [2]. ResNet34 and ResNet50 are two varations of the architecture, where 34 and 50 indicate the number of residual connections respectively. In order to prevent overfitting, we apply early stopping during training with patience ranging from 20 to 1/3 of total epochs (refer to Section 3.3 for details).&lt;/p&gt;
&lt;p&gt;The bounding box model (see Figure 4) implements feature extraction with ResNet, and after feature extraction, we would obtain a fixed length vector. Then, in order to predict both digit sequence length and digit classification, Bounding Box implements multi-task learning, which connects the feature vector to 6 different branches, where each branch contains a fully connected (FC) layer. Branch 1 is used to predict the sequence length, and the other 5 branches predict the content in each position of the sequence. If we obtain a sequence length less than 5, we simply truncate the final output [1]. This truncation approach ensures that we can apply the same model for sequences with varying lengths.&lt;/p&gt;
&lt;h2&gt;Faster RCNN and Bounding Box model pipeline&lt;/h2&gt;
&lt;p&gt;The last and the main part of this block is to make an end-to-end pipeline between the trained model in Bounding Box and Faster-RCNN model. We train Faster-RCNN model and Bounding Box models independently. For evaluation part, we input full sized images to Faster-RCNN model and obtain the predicted bounding boxes. Then, using a dataloader consists of data transformation such as cropping and down scaling (see previous Section 3.1), we preprocess bounding box segmented test data further, and then use Bounding Box ResNet model to identify and predict digits. The pipeline is illustrated in Figure 2.&lt;/p&gt;
&lt;p&gt;As pointed out by teaching staff, the pipeline is more software-engineering centric rather than a machine learning problem. The pipeline itself does not have any parameter or hyperparamter, and therefore all discussions and results are empirical.&lt;/p&gt;
&lt;h2&gt;Hyperparameter optimization&lt;/h2&gt;
&lt;p&gt;Tables 1 and 2 contain the value of hyperparameters we used.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;For Faster-RCNN model, we used the default hyperparameters as instructed by the teaching staff. Instead of exploring a range of hyperparameter search space, we simply pick the values from a given set shown below. The &lt;strong&gt;{}&lt;/strong&gt; symbol indicates a set.&lt;/li&gt;
&lt;li&gt;For hyperparameter tuning of Bounding Box ResNet model, we used Bayesian Optimization in Scikit-learn. We tuned the learning rate, momentum, and decay parameters for SGD, and only the initial learning rate for Adam since it implements auto-correcting of the rest of its parameters. Table 1 indicates the range of hyperparameter space we choose.&lt;/li&gt;
&lt;li&gt;The final pipeline is using bash script to connect the output of Faster-RCNN to Bounding Box ResNet. As explained in Section 3.4, no hyperparameters are shown.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Table 1: Hyperparameters for Bounding Box model&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Hyperparameter&lt;/th&gt;
&lt;th&gt;Range&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Learning Rate (LR)&lt;/td&gt;
&lt;td&gt;[0.0001,0.01]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Momentum&lt;/td&gt;
&lt;td&gt;[0.90,0.91]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Weight Decay&lt;/td&gt;
&lt;td&gt;[0.0001, 0.001]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Decay steps&lt;/td&gt;
&lt;td&gt;[10000, 10001]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Decay Rate&lt;/td&gt;
&lt;td&gt;[0.89, 0.90]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Features-O-size&lt;/td&gt;
&lt;td&gt;[3000,6000]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Batch size&lt;/td&gt;
&lt;td&gt;32&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Epochs&lt;/td&gt;
&lt;td&gt;[50, 400]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Patience&lt;/td&gt;
&lt;td&gt;[20, 1/3 total epoch]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;LR milestones&lt;/td&gt;
&lt;td&gt;[20, 40, 80]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;LR Gamma&lt;/td&gt;
&lt;td&gt;0.1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Table 2: Hyperparameters for Faster RCNN&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Hyperparameter&lt;/th&gt;
&lt;th&gt;Range&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Base Learning Rate(LR)&lt;/td&gt;
&lt;td&gt;{0.0025, 0.002}&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Weight Decay&lt;/td&gt;
&lt;td&gt;0.0001&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Steps&lt;/td&gt;
&lt;td&gt;(2400, 3200)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Batch size-train&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Batch size-test&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Max-Iterations&lt;/td&gt;
&lt;td&gt;{3600, 7200, 14400}&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h1&gt;Results and Analysis&lt;/h1&gt;
&lt;p&gt;This section presents our results obtained we the different models we trained.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;4.1&lt;/strong&gt; &lt;strong&gt;Faster RCNN Bounding Box Segmentation results&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The results of the best object detection models are presented in table 3. The different values for the mAP are presented in table 3. We notice that the R-101-FPN model has a higher mAP score than the R-50-FPN model.&lt;/p&gt;
&lt;p&gt;Table 3: The mean Average Precision (mAP) when using the validation dataset for the two Faster RCNN models we trained for 14400 iterations.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th&gt;mAP&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;R-50-FPN&lt;/td&gt;
&lt;td&gt;86.7%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;R-101-FPN&lt;/td&gt;
&lt;td&gt;89.6%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We notice that the training loss for R-101-FPN is generally slightly lower than the loss for R-50-FPN. Both these losses reach a plateau around 0.01 after 10000 iterations, as shown in figure 5.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Original Block 2 model vs modified Block 2 model&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The experimented models in Block2 are VGG19, ResNet18, ResNet34, Resnet50, and we decided to compare ResNet34 and ResNet50 in our project because Block 2 team concludes that they are the best performing models. The model architectures remain the same, while the input is altered larger/deeper models have higher capacity that works well with SVHN (over 600, 000 digit images), but less suitable with the setup of Bounding Box models.&lt;/p&gt;
&lt;p&gt;Attempted transfer learning&lt;/p&gt;
&lt;p&gt;We attempted loading checkpoint weights as an initialization. The results show no significant improvement, largely due to particularities of the ElementAI dataset described in Section 2.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Faster RCNN and Block 2 Pipeline Results: Impacts of Bounding Box&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The sequence transcription accuracy is expected to drop when using predicted bounding box from Faster-RCNN versus using ground truth bounding box for Block 2 ResNet. After inspecting the vali-dation accuracy, the difference is merely 0.3%, therefore the pipeline does not hinder the performance significantly.&lt;/p&gt;
&lt;h1&gt;Conclusion and further directions&lt;/h1&gt;
&lt;p&gt;In previous block the task was to predict the sequence of digits on the SVHN [5] dataset. In this block the task is to segment the images in the Element AI dataset and predict house number sequences. To solve this task, we first apply an object detection model (Faster-RCCN) [7] to obtain the bounding boxes. These bounding boxes are then used as inputs for the classification model (ResNet34 [2]) to predict the house number.&lt;/p&gt;
&lt;p&gt;Our model has 70.3% accuracy on Element AI validation set. We experiment with two versions of Faster-RCNN [7] and the one with R-101-FPN outperforms the one with R-50-FPN. We also observe that ResNet34 [2] works better than ResNet50 [2] for the sequence prediction task.&lt;/p&gt;
&lt;p&gt;We notice that the performance of same architecture on ElementAI dataset drops by 20% compared to performance on SVHN. The presence of negative samples and background noise in ElementAI dataset contributes to the drop. If granted another dataset with similar variety, such as angle, noise, and background, this model could serve as a baseline. Apart from performance issues, we also noticed that the whole model takes a lot of time to make predictions.&lt;/p&gt;
&lt;p&gt;To increase performance and robustness of our model, semi-supervised representation learning techniques such as the ones proposed by Salimans et al. [8] can be used. To reduce the prediction time, real-time models such as YOLO [6] and SSD [3] can be used.&lt;/p&gt;
&lt;h1&gt;Authors (alphabetical order)&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Saber Benchalel&lt;/li&gt;
&lt;li&gt;Violet Guo&lt;/li&gt;
&lt;li&gt;Marzieh Mehdlzadeh&lt;/li&gt;
&lt;li&gt;Ishaan Kumar&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Citations and figures are available upon request.&lt;/p&gt;
&lt;style class=&quot;grvsc-styles&quot;&gt;
  .grvsc-container {
    overflow: auto;
    position: relative;
    -webkit-overflow-scrolling: touch;
    padding-top: 1rem;
    padding-top: var(--grvsc-padding-top, var(--grvsc-padding-v, 1rem));
    padding-bottom: 1rem;
    padding-bottom: var(--grvsc-padding-bottom, var(--grvsc-padding-v, 1rem));
    border-radius: 8px;
    border-radius: var(--grvsc-border-radius, 8px);
    font-feature-settings: normal;
    line-height: 1.4;
  }
  
  .grvsc-code {
    display: table;
  }
  
  .grvsc-line {
    display: table-row;
    box-sizing: border-box;
    width: 100%;
    position: relative;
  }
  
  .grvsc-line &gt; * {
    position: relative;
  }
  
  .grvsc-gutter-pad {
    display: table-cell;
    padding-left: 0.75rem;
    padding-left: calc(var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem)) / 2);
  }
  
  .grvsc-gutter {
    display: table-cell;
    -webkit-user-select: none;
    -moz-user-select: none;
    user-select: none;
  }
  
  .grvsc-gutter::before {
    content: attr(data-content);
  }
  
  .grvsc-source {
    display: table-cell;
    padding-left: 1.5rem;
    padding-left: var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem));
    padding-right: 1.5rem;
    padding-right: var(--grvsc-padding-right, var(--grvsc-padding-h, 1.5rem));
  }
  
  .grvsc-source:empty::after {
    content: &apos; &apos;;
    -webkit-user-select: none;
    -moz-user-select: none;
    user-select: none;
  }
  
  .grvsc-gutter + .grvsc-source {
    padding-left: 0.75rem;
    padding-left: calc(var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem)) / 2);
  }
  
  /* Line transformer styles */
  
  .grvsc-has-line-highlighting &gt; .grvsc-code &gt; .grvsc-line::before {
    content: &apos; &apos;;
    position: absolute;
    width: 100%;
  }
  
  .grvsc-line-diff-add::before {
    background-color: var(--grvsc-line-diff-add-background-color, rgba(0, 255, 60, 0.2));
  }
  
  .grvsc-line-diff-del::before {
    background-color: var(--grvsc-line-diff-del-background-color, rgba(255, 0, 20, 0.2));
  }
  
  .grvsc-line-number {
    padding: 0 2px;
    text-align: right;
    opacity: 0.7;
  }
  
&lt;/style&gt;</content:encoded></item><item><title><![CDATA[Speaker Accent Classification with Deep Learning]]></title><description><![CDATA[Abstract The task of classifying the accent of recorded speech has generally been approached with traditional SVM or UBM-GMM methods (Omar…]]></description><link>https://violetguos.github.io/accent-classification/</link><guid isPermaLink="false">https://violetguos.github.io/accent-classification/</guid><pubDate>Thu, 26 Jan 2023 00:00:00 GMT</pubDate><content:encoded>&lt;h1&gt;Abstract&lt;/h1&gt;
&lt;p&gt;The task of classifying the accent of recorded speech has generally been approached with traditional SVM or UBM-GMM methods (Omar and Pelecanos, 2010; Ge, 2015). However, modern deep learning methods yield the potential to dramatically increase performance. In our report, we train several varieties of Recurrent and Convolutional Neural Networks on three types of features (MFCC, formant, and raw spectrogram) extracted from North American and British English speech recordings in order to predict the accent of the speaker. All deep learning methods examined surpass non-deep baselines, and the approach yielding the best performance was the MFCC-RNN, shortly followed by the Spec-CNN.&lt;/p&gt;
&lt;h1&gt;Background&lt;/h1&gt;
&lt;p&gt;Treating speech audio using automatic techniques has long been a staple of the machine learning field. These techniques culminate in the construction of Automatic Speech Recognition (ASR) systems, which have a wide variety of applications including speech-to-text utilities, electronic personal assistants, and automatic translation systems. While ability to recognize speech automatically has increased dramatically over the past decade due to the advent of deep neural networks, ASR systems still are at the mercy of their input data: human speech is highly variable, influenced by recording noise and by the age, sex, accent, and other characteristics of the speaker.&lt;/p&gt;
&lt;p&gt;In particular, two individuals speaking the same sentence in the same language might receive different results from the ASR pipeline if they have two different regional accents. A simple solution might be to train two ASR models, one per expected accent, and to perform accent classification on input audio to determine which model to use for downstream tasks. To this end, we explore several predictive models from a variety of machine learning algorithms to classify English speech audio according to the accent of the speaker. Traditionally, accent classification has been approached with SVM or UBM-GMM methods (Omar and Pelecanos, 2010; Ge, 2015), but we are motivated to take advantage of recent deep learning techniques to potentially improve upon this performance.&lt;/p&gt;
&lt;p&gt;Convolutional Neural Networks (CNNs) have been shown to be useful for audio classification tasks. Hershey et al. (2016) explored the performance of several CNN based models, including a fully- connected CNN model and several preexisting architectures such as VGG, Inception, and Resnet, on the classification of 70 million YouTube videos, each tagged with a content label such as sports or singing, using log-mel spectrograms to represent the audio features of the videos. They achieve an AUC of 92.6% using Resnet-50 and 85.1% using their fully-connected model. Chen, Shen, and Tang (2018) performed a binary classification task (native vs. non-native) using the CMU ARCTIC corpus, which contains recordings of US English and other varieties of accented English. They trained a CNN model with 10 convolutional layers and one fully-connected layer on log-amplitude spectrograms representing speech audio segments, reaching a classification accuracy of 97.8%.&lt;/p&gt;
&lt;p&gt;Flavours of Recurrent Neural Networks (RNNs) have also been used for the task of accent classifica- tion. Chu, Lai, and Le (2017) used a Long Short-Term Memory (LSTM) network to classify speech audio segments represented by its Mel-frequency cepstral coefficients (MFCCs) and delta features into 5 accents, yielding only a 39.8% classification accuracy. Since their accuracy on the test set was getting better with more training inputs, the authors did mention that a bigger dataset would improve their performance.&lt;/p&gt;
&lt;p&gt;In this work, we perform a comparative study of techniques used in accent classification, varying both feature extraction methods and learning algorithms. We aim to classify recordings of English speech into British vs. North American accents, comparing the performance of an RNN trained on MFCC features, an RNN trained on formant features, and a CNN trained on raw spectrogram features. Our effort is to find which combination of feature extraction and deep learning approaches yields most accurate classification results.&lt;/p&gt;
&lt;h1&gt;Methodology&lt;/h1&gt;
&lt;h2&gt;Datasets and preprocessing&lt;/h2&gt;
&lt;p&gt;The audio for the North-American English comes from the “test” subset of the Librispeech Corpus, which is assembled from clean single-speaker recordings of North American audiobook narrators from the open-source public-domain website Librivox (Panayotov et al., 2015). Since no equivalent- quality British English spoken corpus could be found, we assembled the Librit Corpus, a corpus of recordings of British audiobook narrators also sourced from Librivox. Our versions of Librispeech and Librit contain about 5.5 and 7 hours of audio from 40 and 27 speakers respectively, roughly balanced male and female.&lt;/p&gt;
&lt;p&gt;Audio from both corpora was downsampled to 16 kHz mono .wav files, and then split into 20-second clips. While silence is often removed in speech processing tasks, we elected not to do so, because it is possible that natural pauses in speech could be a cue for accent learnable by a classifier.&lt;/p&gt;
&lt;h2&gt;Feature extraction&lt;/h2&gt;
&lt;p&gt;We extract three types of features from our audio in order to explore which is most informative for this task. The first are Mel-frequency cepstral coefficients (MFCCs), a standard in speech processing that is loosely modelled on the processing of the human ear. For each frame of the audio, we calculate its power spectrum, take the logarithm of the summed energies after applying the Mel filterbank, and keep the first 13 coefficients from the discrete cosine transform of these log filterbank energies.&lt;/p&gt;
&lt;p&gt;The second type of feature we extract are the formant features. Previous research suggests that formants, the vocal tract resonant frequencies measured when a vowel or similar voiced sound is produced, are a salient cue for accent detection (Hansen and Arslan, 1995). Hence, we extract the first three formants from every voiced frame in the audio, following the heuristic of Deshpande, Chikkerur, and Govindaraju (2005): we consider a frame voiced if its log energy is greater than or equal to -9.0, and if its number of zero-crossings is between 1 and 45.&lt;/p&gt;
&lt;p&gt;Thirdly, we apply a Fourier transform to the time signal of the audio, generating a spectrogram. We then save the raw spectrogram features, i.e., the frequency, amplitude, and time values needed to visually represent the spectrogram for that audio file.&lt;/p&gt;
&lt;h2&gt;Proposed networks&lt;/h2&gt;
&lt;p&gt;Since the MFCC and formant features are inherently sequential, being extracted from a series of time frames, they are suitable for use with a Recurrent Neural Network (RNN). We propose a Long Short-Term Memory Network (LSTM) for each of these features, since these are especially suited for “remembering” information from many steps into the past, which applies well given the large number of frames in our 20-second examples. We call these the MFCC-RNN and the Formant-RNN respectively.&lt;/p&gt;
&lt;p&gt;Traditionally, Convolutional Neural Networks (CNNs) are used for computer vision tasks to recognize colours, edges, and contours of objects. We follow others in applying this approach to audio data, where the CNN treats the spectrogram representation roughly as an image that contains “objects” reflective of speech characteristics (Chen, Shen, and Tang, 2018; Hershey et al., 2016). Rather than pre-selecting features using speech-specific processing derived by linguists, the CNN learns 2 low-level audio features present in the raw spectrogram, potentially capturing speech characteristics not quantified by MFCCs or formant measurements. We call this network the Spec-CNN.&lt;/p&gt;
&lt;h1&gt;Experiments&lt;/h1&gt;
&lt;p&gt;We split our data into 90% training and 10% testing, or 80% training, 10% validation, 10% testing where appropriate.
For each feature, in addition to the relevant trained network, we also test a Most Frequent (MF) baseline that always chooses the class label most frequent in the training data (since our examples do not contain the exact same amount of data from each class, and sometimes variations in preprocessing cause slight differences in the amount of data for which features can be extracted), and an (rbf) SVM baseline. In order to be considered useful, the network must outperform the MF baseline, and it is desirable that it also outperform the SVM baseline. For details on the hyperparameter tuning for all networks, and for detailed diagrams on network architectures, see Appendices B and C.&lt;/p&gt;
&lt;p&gt;For the MF and SVM baselines, “flattened” versions of the MFCC and formant features were used, which collapsed the features across the time dimension, since these classifiers do not take sequential data as input.&lt;/p&gt;
&lt;h2&gt;MFCC features (MFCC-RNN)&lt;/h2&gt;
&lt;p&gt;For the MFCC-RNN, we train an LSTM with one hidden layer and 350 hidden units using an Adam optimizer and cross-entropy loss over 10 epochs. The classification accuracies and the learning curves of the MFCC-RNN can be found in Table 1 and Figure 1 respectively.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Classifier&lt;/th&gt;
&lt;th&gt;Train&lt;/th&gt;
&lt;th&gt;Valid&lt;/th&gt;
&lt;th&gt;Test&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;(%)&lt;/td&gt;
&lt;td&gt;(%)&lt;/td&gt;
&lt;td&gt;(%)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;MF&lt;/td&gt;
&lt;td&gt;63.33&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;59.23&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SVM&lt;/td&gt;
&lt;td&gt;100.00&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;61.71&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;MFCC-RNN&lt;/td&gt;
&lt;td&gt;100.00&lt;/td&gt;
&lt;td&gt;95.17&lt;/td&gt;
&lt;td&gt;95.32&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;Formant features (Formant-RNN)&lt;/h2&gt;
&lt;p&gt;For the Formant-RNN, we train a bidirectional LSTM with one hidden layer of 200 hidden units, three dense layers with 1024 hidden units, and a fourth dense layer with 2048 hidden units. Dropout with probability of 0.3 was added to all of the previous dense layers. The network was trained over 26 epochs (early stopped) using an Adam optimizer and a cross-entropy loss. The classification accuracies and the learning curves of the Formant-RNN can be found in Table 2 and Figure 2 respectively.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Classifier&lt;/th&gt;
&lt;th&gt;Train&lt;/th&gt;
&lt;th&gt;Valid&lt;/th&gt;
&lt;th&gt;Test&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;(%)&lt;/td&gt;
&lt;td&gt;(%)&lt;/td&gt;
&lt;td&gt;(%)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;MF&lt;/td&gt;
&lt;td&gt;53.35&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;55.07&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SVM&lt;/td&gt;
&lt;td&gt;100.00&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;55.07&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Formant-RNN&lt;/td&gt;
&lt;td&gt;99.06&lt;/td&gt;
&lt;td&gt;90.96&lt;/td&gt;
&lt;td&gt;86.96&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;Raw spectrogram features (Spec-CNN)&lt;/h2&gt;
&lt;p&gt;As outlined in Section 2.1, input data initially consisted of 20 second audio clips, but the spectrogram generated was too large for efficient training of CNNs. Thus, following the practice of Hershey et al. (2016) who also trained on spectrogram data, we further sliced our audio into one-second clips, yielding 19475 North American accent examples and 22541 British accent examples. We then generated spectrograms every 10 ms, using a 25 ms window which gives sufficient time and frequency resolution to resolve features in the spectrogram. The spectrograms thus generated have dimensions of 201 frequency bins between 0-8000 Hz and 66 bins along the one-second time axis.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Classifier&lt;/th&gt;
&lt;th&gt;Train&lt;/th&gt;
&lt;th&gt;Valid&lt;/th&gt;
&lt;th&gt;Test&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;(%)&lt;/td&gt;
&lt;td&gt;(%)&lt;/td&gt;
&lt;td&gt;(%)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;MF&lt;/td&gt;
&lt;td&gt;54.52&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;53.67&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SVM&lt;/td&gt;
&lt;td&gt;99.98&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;53.67&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Spec-CNN&lt;/td&gt;
&lt;td&gt;98.12&lt;/td&gt;
&lt;td&gt;94.57&lt;/td&gt;
&lt;td&gt;92.63&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h1&gt;Discussion&lt;/h1&gt;
&lt;p&gt;All three of our deep-learning classifiers (MFCC-RNN, Formant-RNN, and Spec-CNN) are able to outperform both the MF and SVM baselines. The classifier performing the best was the MFCC-RNN, yielding a test accuracy of 95.32%. This combination of feature and learning algorithm seems to strike the right balance for the characterization of speech: MFCCs make use of a large amount of frequency information, and the RNN approach allows previous contextual information to be modelled, which helps account for many progressive phonetic processes that are probably cues for accent. The MFCC features also use information from the entire signal, and not just from heuristically-chosen speech segments, permitting more nuanced modelling.&lt;/p&gt;
&lt;p&gt;The Spec-CNN earns a close second with test accuracy 92.63%. The raw spectrogram features contain nearly three times more information than the MFCC- or Formant-RNNs, comprising frequency and amplitude information for each frame; this large amount of information is likely responsible for much of the classification accuracy. However, since splitting the audio further into one-second clips was necessary, contextual information that the other networks have is lost to Spec-CNN. Moreover, the large size of the spectrogram information, which likely includes significant amounts of noise, means that a more complex model is needed to learn the important features within it, and thus a larger amount of data is needed to yield significant improvement in accuracy. We speculate that larger training corpora might give higher performance, and that timescales longer than one second might also boost accuracy due to a higher number of contextual clues.&lt;/p&gt;
&lt;p&gt;The Formant-RNN, while far sparser in terms of frequency information than the MFCC-RNN, still performs relatively well with 86.96% test accuracy. Its ability to still recover most of the accent cues that the MFCC-RNN detects is likely bolstered by the bidirectional nature of the RNN, which may help model the regressive as well as the progressive phonetic processes present in the audio. However, since formant features are only extracted from heuristically-chosen voiced frames, this classifier is limited not only by the accuracy of the heuristic but also by the number of accent cues present in only the voiced frames. For example, many North American speakers pronounce &lt;em&gt;s&lt;strong&gt;ch&lt;/strong&gt;edule&lt;/em&gt; with a [k], while British speakers use [S], but since neither of these sounds are voiced, this cue cannot be learned by the Formant-RNN.&lt;/p&gt;
&lt;p&gt;There is a practical tradeoff regarding dataset size, computational time, and accuracy: while the Spec-CNN is potentially capable of leveraging a detailed signal into high accuracy, it requires lots of data and computational power (including navigation of GPU and RAM limits). The MFCC-RNN offers similar performance in less time, and is thus suited for smaller datasets. The Formant-RNN, while the easiest to train of the three, yields the lowest performance due to the under-salient cues it learns.&lt;/p&gt;
&lt;h2&gt;Learned features&lt;/h2&gt;
&lt;p&gt;As expected, the networks do learn to “pay attention” to different features. This is visualized in Figure 4, which shows the influence of each subsection of audio on the final classification outcome for one training example. In the RNN cases, we perturb the time steps one at a time by a small value and calculate the difference in classification probabilities between the original sequence and the sequence with the perturbed time step. The larger the difference between these probabilities, the more influential we consider the time step (represented by a yellower colour). In the CNN case, we use the keras-vis API to visualize the learned weights of the convolution filters, where higher weights yield brighter colours and correspond to more important features.&lt;/p&gt;
&lt;p&gt;Qualitative examination of the (British) audio1alongside Figure 4a suggests that the MFCC-RNN sometimes learns something somewhat compatible with human intuition. From left to right, the red boxes roughly correspond to instances of the vowels [6], [O], and [œ] (the sounds in &lt;em&gt;F&lt;strong&gt;eu&lt;/strong&gt;erbach&lt;/em&gt;, &lt;strong&gt;*or&lt;/strong&gt;g&lt;em&gt;, and another rendering of *F&lt;strong&gt;eu&lt;/strong&gt;erbach\&lt;/em&gt;), respectively, which are vowels more common to British dialects than American ones. There are many other yellow bands such as these, suggesting that the MFCC-RNN also learns less transparently interpretable features (i.e., without a perfect correspondence to a human notion such as a particular vowel) used as accent cues.&lt;/p&gt;
&lt;p&gt;In Figure 4b, we observe a much starker difference between more- and less-influential segments (bluer and yellower, respectively). This is due to the lower dimensionality of the input features. In red are boxed the same vowel instances as are found by the MFCC-RNN, and on the whole there seems to be some degree of correspondence between what these two RNNs learn. This is perhaps surprising, since the MFCC data is much more complex and incorporates much more of the signal information than the formant data, which is simply three frequency measurements per voiced frame.&lt;/p&gt;
&lt;p&gt;Figure 4c shows what is treated as important by the final layer of the Spec-CNN. The network pays attention to different features at different convolutional layers (with layer-by-layer diagrams available in Appendix D). In the second convolutional layer, the Spec-CNN weights still resemble a spectrum and it focuses on the lower frequencies, where most salient speech information is aggregated. However, as we go to the third convolutional layer, the weights are abstracted from the notion of a frequency spectrum, spanning more space in the y direction. Research in deep learning is still being done on interpretability of the learned representations; for this reason, we choose not to label y-axis of the CNN (Olah et al., 2018). The red-boxed intervals on the x-axis in Figure 4c highlighted by Spec-CNN correspond to where the speaker pronounces iconically British vowels, as observed in the RNNs. Therefore, we empirically conjecture that Spec-CNN abstracts away the frequency axis &lt;em&gt;per se&lt;/em&gt; and pays attention to the important time intervals. We remark that our two general methods (RNN and CNN) roughly “agree” with each other regarding which segments are important.&lt;/p&gt;
&lt;h1&gt;Conclusion and future work&lt;/h1&gt;
&lt;p&gt;The MFCC-RNN performs best on our accent classification task, likely due to its sufficiently complex and salient features and its ability to take contextual information into account, and its performance is shortly followed by that of the the Spec-CNN (which may outperform the MFCC-RNN given more data and more computational power). The Formant-RNN performs worst of the three, likely due to the excessively low amount of information present in its features.&lt;/p&gt;
&lt;p&gt;A straightforward next step in evaluating these network architectures and feature choices would be to extend them beyond binary classification, including data from many varieties of English. This would allow a more direct comparison with works such as Chu, Lai, and Le (2017), which was unable to classify multiple varieties of accented English well using an MFCC approach.&lt;/p&gt;
&lt;p&gt;Further future work might include expansion on the Spec-CNN, since current research continues to look into new types of convolution to curate filters for audio signals. One successful application of CNNs with raw audio involves using parametrized sinc functions in the convolution layer instead of a traditional convolution, as in SincNet developed by Ravanelli and Bengio (2018). Dilated convolutions have also been shown to be useful for accent correction and audio generation, since they allow the receptive fields to grow longer in a cheaper way than do LSTMs or other RNNs (Oord et al., 2016).&lt;/p&gt;
&lt;p&gt;Though it underperformed in this work, the above-baseline performance of the Formant-RNN also holds potential due to the easy interpretability of its features. Future work might construct a mapping from the features picked out as important by the network back to their original timestamp, which could be the basis of an accent correction system that delivers feedback to a user about which portions of their speech need adjustment in order to imitate a certain accent. This is especially applicable to the Formant-RNN since there is a well-understood relationship between the configuration of the mouth and the first two formants of a vowel. Such a system could provide practical instructions to the user (i.e. to raise or lower the tongue, round the lips, etc.) and could have applications in language learning or speech therapy tools.&lt;/p&gt;
&lt;h1&gt;Appendix&lt;/h1&gt;
&lt;h2&gt;A sample dataset&lt;/h2&gt;
&lt;p&gt;In the project repo, we include 1&lt;em&gt;0&lt;/em&gt;20sec.wav, a 20 second file from Librit (our British audio corpus). In our report, we specify that our neural networks identify the following:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&quot;center&quot;&gt;Phoneme&lt;/th&gt;
&lt;th align=&quot;center&quot;&gt;Word of the phoneme&lt;/th&gt;
&lt;th align=&quot;center&quot;&gt;Time Window in Audio (seconds)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&quot;center&quot;&gt;&lt;em&gt;eu&lt;/em&gt;&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;&lt;em&gt;F&lt;strong&gt;eu&lt;/strong&gt;erbach&lt;/em&gt;&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;1:49 to 2:19&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&quot;center&quot;&gt;&lt;strong&gt;or&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;&lt;strong&gt;or&lt;/strong&gt;g&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;10:86 to 11:39&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&quot;center&quot;&gt;&lt;strong&gt;eu&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;&lt;em&gt;F&lt;strong&gt;eu&lt;/strong&gt;erbach&lt;/em&gt;&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;17:65 to 18:41&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;You can play the audio using any open source or commercial audio player to skip to the following timestamps and verify our results. Additionally, the image files showing the visualization of the weights learned by the neural network are available here in higher resolution.&lt;/p&gt;
&lt;h2&gt;B Hyperparameter tuning on validation set&lt;/h2&gt;
&lt;p&gt;Hyperparameter values for the RNNs were tuned in a heuristic manner, as we lack the computational resources to systematically check each possible combination. For the CNN, a grid search method was used. For all cases, we choose the hyperparameter values that give the highest classification accuracy on the validation set. This ensures we are not overfitting to the training data.&lt;/p&gt;
&lt;h3&gt;B.1 MFCC-RNN&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Number of epochs&lt;/th&gt;
&lt;th&gt;Number of hidden units&lt;/th&gt;
&lt;th&gt;Train&lt;/th&gt;
&lt;th&gt;Valid&lt;/th&gt;
&lt;th&gt;Test&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;100&lt;/td&gt;
&lt;td&gt;99.92&lt;/td&gt;
&lt;td&gt;83.45&lt;/td&gt;
&lt;td&gt;80.17&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;200&lt;/td&gt;
&lt;td&gt;99.85&lt;/td&gt;
&lt;td&gt;90.10&lt;/td&gt;
&lt;td&gt;92.01&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;300&lt;/td&gt;
&lt;td&gt;99.46&lt;/td&gt;
&lt;td&gt;87.57&lt;/td&gt;
&lt;td&gt;92.56&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;325&lt;/td&gt;
&lt;td&gt;98.54&lt;/td&gt;
&lt;td&gt;88.97&lt;/td&gt;
&lt;td&gt;88.71&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;350&lt;/td&gt;
&lt;td&gt;100.00&lt;/td&gt;
&lt;td&gt;95.17&lt;/td&gt;
&lt;td&gt;95.32&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;375&lt;/td&gt;
&lt;td&gt;96.16&lt;/td&gt;
&lt;td&gt;93.10&lt;/td&gt;
&lt;td&gt;96.14&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;400&lt;/td&gt;
&lt;td&gt;99.23&lt;/td&gt;
&lt;td&gt;88.28&lt;/td&gt;
&lt;td&gt;90.63&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Table 5: Percent classification accuracies using various hyperparameter values on the MFCC-RNN&lt;/p&gt;
&lt;h3&gt;B.2 Formant-RNN&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Number of hidden units&lt;/th&gt;
&lt;th&gt;Valid&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;0.58&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;20&lt;/td&gt;
&lt;td&gt;0.56&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;50&lt;/td&gt;
&lt;td&gt;0.62&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;100&lt;/td&gt;
&lt;td&gt;0.63&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;200&lt;/td&gt;
&lt;td&gt;0.74&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Table 6: accuracy using various numbers of hidden units on the Formant-RNN (tuned over 20 epochs)&lt;/p&gt;
&lt;h3&gt;B.3 Spec-CNN&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Acitvation function&lt;/th&gt;
&lt;th&gt;Valid&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Softmax&lt;/td&gt;
&lt;td&gt;0.630513&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Softplus&lt;/td&gt;
&lt;td&gt;0.791683&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Softsign&lt;/td&gt;
&lt;td&gt;0.879486&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Relu&lt;/td&gt;
&lt;td&gt;0.815786&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Tanh&lt;/td&gt;
&lt;td&gt;0.895775&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Sigmoid&lt;/td&gt;
&lt;td&gt;0.699378&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Hard_sigmoid&lt;/td&gt;
&lt;td&gt;0.693815&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Linear&lt;/td&gt;
&lt;td&gt;0.705999&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;learning rate&lt;/th&gt;
&lt;th&gt;Valid&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;0.00001&lt;/td&gt;
&lt;td&gt;0.551715&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0.0001&lt;/td&gt;
&lt;td&gt;0.686002&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0.001&lt;/td&gt;
&lt;td&gt;0.823864&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0.01&lt;/td&gt;
&lt;td&gt;0.544828&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0.1&lt;/td&gt;
&lt;td&gt;0.481128&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Convolution kernel size&lt;/th&gt;
&lt;th&gt;valid&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;0.815786&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;0.796318&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;0.003469&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Initialization mode for kernel&lt;/th&gt;
&lt;th&gt;Valid&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;uniform&lt;/td&gt;
&lt;td&gt;0.888756&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;lecun_uniform&lt;/td&gt;
&lt;td&gt;0.872997&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;normal&lt;/td&gt;
&lt;td&gt;0.890081&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;zero&lt;/td&gt;
&lt;td&gt;0.545226&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;glorot_normal&lt;/td&gt;
&lt;td&gt;0.890081&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;glorot_uniform&lt;/td&gt;
&lt;td&gt;0.883989&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;he_normal&lt;/td&gt;
&lt;td&gt;0.831148&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;he_uniform&lt;/td&gt;
&lt;td&gt;0.843729&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Optimizer&lt;/th&gt;
&lt;th&gt;Valid&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;SGD&lt;/td&gt;
&lt;td&gt;0.679115&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;RMSprop&lt;/td&gt;
&lt;td&gt;0.818037&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Adagrad&lt;/td&gt;
&lt;td&gt;0.624421&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Adadelta&lt;/td&gt;
&lt;td&gt;0.527480&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Adam&lt;/td&gt;
&lt;td&gt;0.808767&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Adamax&lt;/td&gt;
&lt;td&gt;0.723348&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Nadam&lt;/td&gt;
&lt;td&gt;0.790094&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h1&gt;Authors (alphabetical order)&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Jonathan Bhimani-Burrows&lt;/li&gt;
&lt;li&gt;Khalil Bibi&lt;/li&gt;
&lt;li&gt;Arlie Coles&lt;/li&gt;
&lt;li&gt;Akila Jeeson Daniel&lt;/li&gt;
&lt;li&gt;Violet Guo&lt;/li&gt;
&lt;li&gt;Louis-François Préville-Ratelle&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Citations and figures are available upon request.&lt;/p&gt;
&lt;style class=&quot;grvsc-styles&quot;&gt;
  .grvsc-container {
    overflow: auto;
    position: relative;
    -webkit-overflow-scrolling: touch;
    padding-top: 1rem;
    padding-top: var(--grvsc-padding-top, var(--grvsc-padding-v, 1rem));
    padding-bottom: 1rem;
    padding-bottom: var(--grvsc-padding-bottom, var(--grvsc-padding-v, 1rem));
    border-radius: 8px;
    border-radius: var(--grvsc-border-radius, 8px);
    font-feature-settings: normal;
    line-height: 1.4;
  }
  
  .grvsc-code {
    display: table;
  }
  
  .grvsc-line {
    display: table-row;
    box-sizing: border-box;
    width: 100%;
    position: relative;
  }
  
  .grvsc-line &gt; * {
    position: relative;
  }
  
  .grvsc-gutter-pad {
    display: table-cell;
    padding-left: 0.75rem;
    padding-left: calc(var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem)) / 2);
  }
  
  .grvsc-gutter {
    display: table-cell;
    -webkit-user-select: none;
    -moz-user-select: none;
    user-select: none;
  }
  
  .grvsc-gutter::before {
    content: attr(data-content);
  }
  
  .grvsc-source {
    display: table-cell;
    padding-left: 1.5rem;
    padding-left: var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem));
    padding-right: 1.5rem;
    padding-right: var(--grvsc-padding-right, var(--grvsc-padding-h, 1.5rem));
  }
  
  .grvsc-source:empty::after {
    content: &apos; &apos;;
    -webkit-user-select: none;
    -moz-user-select: none;
    user-select: none;
  }
  
  .grvsc-gutter + .grvsc-source {
    padding-left: 0.75rem;
    padding-left: calc(var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem)) / 2);
  }
  
  /* Line transformer styles */
  
  .grvsc-has-line-highlighting &gt; .grvsc-code &gt; .grvsc-line::before {
    content: &apos; &apos;;
    position: absolute;
    width: 100%;
  }
  
  .grvsc-line-diff-add::before {
    background-color: var(--grvsc-line-diff-add-background-color, rgba(0, 255, 60, 0.2));
  }
  
  .grvsc-line-diff-del::before {
    background-color: var(--grvsc-line-diff-del-background-color, rgba(255, 0, 20, 0.2));
  }
  
  .grvsc-line-number {
    padding: 0 2px;
    text-align: right;
    opacity: 0.7;
  }
  
&lt;/style&gt;</content:encoded></item><item><title><![CDATA[Recipe Normalization]]></title><description><![CDATA[In deep learning, we use normalization to solve gradient vanishing or explosion. It kind of inspired me to apply normalization to a recipe…]]></description><link>https://violetguos.github.io/recipe-norm/</link><guid isPermaLink="false">https://violetguos.github.io/recipe-norm/</guid><pubDate>Tue, 07 Sep 2021 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;In deep learning, we use normalization to solve gradient vanishing or explosion. It kind of inspired me to apply normalization to a recipe.&lt;/p&gt;
&lt;p&gt;Let’s dive in.&lt;/p&gt;
&lt;p&gt;This is a gluten free, diary free, processed sugar free cake recipe.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;1/2 cup coconut flour, or 8 tbs&lt;/li&gt;
&lt;li&gt;1/2 cup cacao&lt;/li&gt;
&lt;li&gt;1/2 tsp salt&lt;/li&gt;
&lt;li&gt;1/2 tsp baking soda&lt;/li&gt;
&lt;li&gt;1/4 cup coconut oil&lt;/li&gt;
&lt;li&gt;1 cup raw honey&lt;/li&gt;
&lt;li&gt;6 eggs
(&lt;a href=&quot;https://www.youtube.com/watch?v=Hlh1RJ1Tmig&quot;&gt;Link&lt;/a&gt; to recipe &amp;#x26; tutorial)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you are making this for yourself, it’s a pretty big cake. You could freeze it, but the texture becomes suboptimal.&lt;/p&gt;
&lt;p&gt;I decided to apply a similar normalization idea. I’ll normalize the recipe wrt the eggs.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;1 egg&lt;/strong&gt;, and divide everything by 6.&lt;/li&gt;
&lt;li&gt;1.3 tbs coconut flour (8 tbs/6 = 1.3 tbs approx.)&lt;/li&gt;
&lt;li&gt;1.3 tbs caco&lt;/li&gt;
&lt;li&gt;1/12 tsp salt&lt;/li&gt;
&lt;li&gt;1/12 tsp baking soda&lt;/li&gt;
&lt;li&gt;4 tsp coconut oil&lt;/li&gt;
&lt;li&gt;8 tsp raw honey&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now, it yields one serving. You could alternatively normalize wrt the coconut oil or raw honey, just like how we have &lt;a href=&quot;https://arxiv.org/abs/1502.03167&quot;&gt;batch norm&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1607.06450&quot;&gt;layer norm&lt;/a&gt;, etc in deep learning.&lt;/p&gt;
&lt;p&gt;Happy cooking.&lt;/p&gt;
&lt;style class=&quot;grvsc-styles&quot;&gt;
  .grvsc-container {
    overflow: auto;
    position: relative;
    -webkit-overflow-scrolling: touch;
    padding-top: 1rem;
    padding-top: var(--grvsc-padding-top, var(--grvsc-padding-v, 1rem));
    padding-bottom: 1rem;
    padding-bottom: var(--grvsc-padding-bottom, var(--grvsc-padding-v, 1rem));
    border-radius: 8px;
    border-radius: var(--grvsc-border-radius, 8px);
    font-feature-settings: normal;
    line-height: 1.4;
  }
  
  .grvsc-code {
    display: table;
  }
  
  .grvsc-line {
    display: table-row;
    box-sizing: border-box;
    width: 100%;
    position: relative;
  }
  
  .grvsc-line &gt; * {
    position: relative;
  }
  
  .grvsc-gutter-pad {
    display: table-cell;
    padding-left: 0.75rem;
    padding-left: calc(var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem)) / 2);
  }
  
  .grvsc-gutter {
    display: table-cell;
    -webkit-user-select: none;
    -moz-user-select: none;
    user-select: none;
  }
  
  .grvsc-gutter::before {
    content: attr(data-content);
  }
  
  .grvsc-source {
    display: table-cell;
    padding-left: 1.5rem;
    padding-left: var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem));
    padding-right: 1.5rem;
    padding-right: var(--grvsc-padding-right, var(--grvsc-padding-h, 1.5rem));
  }
  
  .grvsc-source:empty::after {
    content: &apos; &apos;;
    -webkit-user-select: none;
    -moz-user-select: none;
    user-select: none;
  }
  
  .grvsc-gutter + .grvsc-source {
    padding-left: 0.75rem;
    padding-left: calc(var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem)) / 2);
  }
  
  /* Line transformer styles */
  
  .grvsc-has-line-highlighting &gt; .grvsc-code &gt; .grvsc-line::before {
    content: &apos; &apos;;
    position: absolute;
    width: 100%;
  }
  
  .grvsc-line-diff-add::before {
    background-color: var(--grvsc-line-diff-add-background-color, rgba(0, 255, 60, 0.2));
  }
  
  .grvsc-line-diff-del::before {
    background-color: var(--grvsc-line-diff-del-background-color, rgba(255, 0, 20, 0.2));
  }
  
  .grvsc-line-number {
    padding: 0 2px;
    text-align: right;
    opacity: 0.7;
  }
  
&lt;/style&gt;</content:encoded></item></channel></rss>